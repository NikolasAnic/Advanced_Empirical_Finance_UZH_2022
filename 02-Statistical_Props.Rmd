# The Programming Environment

The first week covers two baseline subjects required to work within the field of empirical financial applications. 

The first topic covers the application environment. Therein, we cover the structure and use of the R Markdown file, on which we will write all of the subjects covered during the Lab sessions. We offer a discussion as how to create a markdown file, what makes these files so special in empirical research, how to organise and shuffle them as well as their connectivity to latex. Especially, we will consider how to write functions, how to add text-based outputs, how to construct and use coding cells as well as how to render the files. 

##  Markdown Files

The first part of the programming environment concerns the general programming environment. Markdown files will be used as programming interface for the Exercise and Lab sessions. Within these files, the students will describe the theoretical questions and solve the empirical exercises. Furthermore, the students will write a report and present their results in Markdown files. 

R Markdown files work as combination of R files and Latex files. That is, they enable the use of coding results and writing a report automatically. The handy feature arises in that you don't have to copy your code output, tables or other things into a latex file in which you write your report. However, although we advise the use of a markdown file for writing reports, it is important to stress that we do not recommend writing entire papers or theses within markdown files, as the compiling burden could render mistakes within the program. As such, we are indifferent whether you decide to provide markdown or Latex files

### How to create a Markdown File

To create a markdown file, you simply open your R Studio, in the side bar you click on `Edit`, then you select `New File` and `R Markdown...`. It will pop up a new window where you need to enter the title as well as the authors. After choosing appropriate names, you can select one of the options `HTML`, `PDF` or `Word` as an output window. This is the first handy feature of markdown files: They enable different outputs, meaning you can have a multitude of channels to present it in. Further note that on the left-hand side, there is a sidebar which allows you to choose other file types, such as Shiny (supporting interactive graphs) or presentation files. Although we do not cover these throughout the lecture, they offer exciting new options. 

Although we would advise you to use PDF as output, you can also choose HTML [to see how a HTML output would look like, just click anywhere here](https://rpubs.com/nikanic/798372). 

Once you choose your desired output format, you will see that a new window pops up that looks like this: 

![Figure 1: Markdown First file structure](~/Desktop/Empirical Asset Management/R/Images/Markdown_First_Look.jpg)  

In order to comprehend the structure of a markdown file, it is firstly important to understand that a markdown file consists of white spaces and grey code Chunks The white spaces are used to write text, whereas the code Chunks are used to write code. Importantly, you won't need the part below the first code block, so just delete everything from the "## R Markdown"" title on. This will leave you with only the first code block.

The first code block is important because there you can get your working directory set for the entire markdown session. You set your working directory in the following way

```{r}
# knitr::opts_knit$set(root.dir = "/This/is/your/path/to/your/files/")
```

The working directory is your source file from which R gets all data and to which R sends all outputs, by default. You can later change your working directory or the path from which files should be read / should be written to, but it is handy to define one source that can be used for most of the files. 

Once you enter your preferred working directory, you can continue to start entering the packages you want to use. For that, add a code block by clicking on the green tool box above and select the option "R", just as in the image below.

![Figure 1: Markdown Select R Codeblock](~/Desktop/Empirical Asset Management/R/Images/Markdown_Select_Codeblock.jpg)

Once you have your codenblock, you can enter the packages you need to use. It is handy to define functions which either install the packages or load the packages for that:

```{r, comment=NA}
# packs.inst <- c("readxl","foreign","dplyr","tidyr","ggplot2","stargazer","haven","dummies","Hmisc", "lmtest","sandwich", "doBy", "multiwayvcov", "miceadds",  "car", "purrr", "knitr", "zoo", "readstata13", "tidyverse", "psych", "wesanderson", "lubridate","reporttools", "data.table", "devtools", "rmarkdown","estimatr", "ivpack", "Jmisc", "lfe", "plm", "tinytex", "xts", "psych", "PerformanceAnalytics", "roll", "rollRegres", "glmnet", "hdm", "broom", "RCurl", "learnr", "maps", "fGarch", "remotes", "RPostgreSQL", "wrds", "DBI", "RPostgreSQL", "remotes", "RPostgres")


# packs.inst <- c()

# lapply(packs.load, require, character.only = TRUE)

# lapply(packs.inst, install.packages, character.only = FALSE) 

```

The underlying idea of this code block is that you just create a list with all the packages you need to load first and assign it a name (packs.load). Then, you uncomment the first code line (# lapply(packs.inst, require, character.only = TRUE)) to load the respective packages. 

However, it might be the case that you did not yet install all of the necessary packages. In that case, just take all the packages you did not yet install and add them to the list called packs.inst. Then, uncomment the second code line (lapply(packs.inst, install.packages, character.only = FALSE)) and run that code. 

The idea behind the code block is that you apply the install.packages or require command for a list of packages, instead of manually installing or loading each of them. Although these are more packages than needed for the firs sessions, it still is handy to have an overview and automate a rather cumbersome procedure. 

### The R Markdown Structure

We just have created the markdown file. Now it comes to understanding how to work on this thing. For that, we first need to understand the underlying structure. 

In general, R Markdown files are comprised of two parts: White Spaces and Grey Code Chunks 

White spaces are the spaces you are currently reading this text. They serve as string-based Chunks, meaning that you can write human language based sentences within these cells. As such, these white spaces serve as areas for writing results of reports, explanations, formulas or in general everything that has no code-based output at the end. 

Opposed to this, Code Chunk serve as fields in which you write your actual code. The two boxes we just added before are coding boxes. To use them, just follow th approach I described earlier. Code Chunks incorporate many nice features, which we will cover next. 

#### The use of Code Chunks

Code Chunks incorporate a number of handy features that can be defined quite easily. To set certain code chunk properties, we make use of the packages `knitr`. Maybe you realised this already, we used this package before to define the working directory source. Knitr, in general, is used to handle most of the R syntax and background, or administrative, tasks, R needs to do in order to operate. 

To set code chunk options, we just access the `{r}` field in the top row of each code chunk. 

Markdown offers quite a range of potential options to include, so make sure [to check them out on this link](https://yihui.org/knitr/options/).  

In general, the most used settings involve the following:

- `eval = FALSE`: Do not evaluate (or run) this code chunk when knitting the RMD document. The code in this chunk will still render in our knitted `html` 
- `echo = FALSE`: Hide the code in the output. The code is evaluated when the Rmd file is knit, however only the output is rendered on the output document.
- `results = hide`: The code chunk will be evaluated but the results or the code will not be rendered on the output document. This is useful if you are viewing the structure of a large object (e.g. outputs of a large data.frame which is the equivalent of a spreadsheet in R). 
- `include = FALSE`: runs the code, but doesn’t show the code or results in the final document. Use this for setup code that you don’t want cluttering your report.
- `message = FALSE` or `warnings = FALSE`: prevents messages or warnings from appearing in the finished file
- `error = TRUE`: causes the render to continue even if code returns an error. This is rarely something you’ll want to include in the final version of your report, but can be very useful if you need to debug exactly what is going on inside your Rmd. 
- `tidy = TRUE`: tidy code for display 
- `cache = TRUE`: When the cache is turned on, knitr will skip the execution of this code chunk if it has been executed before and nothing in the code chunk has changed since then. This is useful if you have very time-consuming codes. These would then be skipped if they haven't been changed since the last rendering.
- `comment=""`: By default, R code output will have two hashes ## inserted in front of the text output. We can alter this behavior through the comment chunk option by e.g. stating an empty space. 

Furthermore, we can use specific options for figures to include in the files:

- `fig.width = `: Defines the figure width in the chunk
- `fig.height = `: Defines the figure height in the chunk
- `fig.align = `: Defines the alignment of the figure (left, center, right)
- `fig.cap = `; Defines the caption of the figure 

#### The use of White Spaces

White spaces are commonly used to define the text we write in a markdown file. Markdown white spaces have the handy feature that they follow the same syntax as Latex files. Consequently, it is very easy to write formulas, expressions, definitions or other features commonly used in Latex based tools or applications. However, since this is not a drag-and-drop interface, commonly used ways of working in other programs, such as Microsoft Word, do not work here. But since we are working within an academic setting, we should definitively foster the use of non Latex-based writing. 

We also call the white space writing Pandoc’s Markdown. The most important features of it are the following: 

*Structure of an Rmd File*

To structure different markdown files, we need to define headers that indicate in which part of the file we currently are. We define headers by using the `#` sign. We have, in total, six headers available, that increase in specification with increasing amount of `#`:

- `#`: Header 1
- `##`: Header 2
- `###`: Header 3
- `####`: Header 4
- `#####`: Header 5
- `######`: Header 6

*Different Font Styles*

In general, we use three different styles to design the font: 

- `textbf{}` or `** **`: This is to indicate a bold text (write either in brackets or between the arcteryx) 
- `textit{}` or `* *`: This is to indicate an italic text 
- `underline{}`: This is to indicate an underlined text 
- `verbatim`: This is to indicate the verbatim text - it is a text in this grey box which is more strongly visible 

*Numbering and Lists*

We can define lists with several properties: 

- `*`: This will render an item in an unordered list 
- `+`: This will render a sub-item in an unordered list 
- `-`: This will render a sub-sub-item in an unordered list 

- `1.`: This will render an item in an ordered list 
- `i)`: This will render a sub-item in an ordered list 
- `A.`: This will render a sub-sub-item in an ordered list 

*Jumping and Linking*

- `[link](name)`: This will render a link to a specific URL code 
- `Jump to [Header 1](#anchor)`: This will jump to a certain part in the Rmd. In this case, Header 1 

*Images and how to include them*

`![Caption](File/to/png/file)`: This will include the image from a given file path on your computer and add a caption

$\textbf{Showing output in markdown file}$

- `$$  $$`: This forms a code block in which math mode or other modes can be used (just like with latex). Neatly, the output is shown immediately
- `$ $`: This is the same as the double-dollar-sign, but you can use it in a sentence just to write some mathematical notion in a string-based text

*Mathematical Symbols*

Rmd offers a great range of different mathematical symbols and equations. They follow the same syntax as in latex files. To get a good overview, [just visit the following link](https://www.caam.rice.edu/~heinken/latex/symbols.pdf).  

Importantly, if you want to use lower- or upper subscripted letters, such as in mathematical notations, you need to use `x_t` or `x^t` to do so. Note that, if you have more than one letter or one mathematical symbol, these subscripts must always be in `{}`. That is: `x_{t,p}` or `x^{t,p}`. 

*Equations*

To create equations, we simply use the following syntaxes:

If we only want to use an easy mathematical equation, we can use the following:

$$
p(x) = \sum_{w_i \in E} p_i
$$

If we want to use more complex mathematical symbols, we need to use the `\begin{equation} ... \end{equation}` command:

$$
\begin{equation}
\textbf{C} = 
\left(
\begin{matrix}
1 & \rho_{12} & \dots & \rho_{1n} \\
\rho_{12} & 1 & \dots & \rho_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
\rho_{1n} & \rho_{2n} & \dots & 1
\end{matrix}
\right)
\end{equation}
$$

Lastly, if we have proofs to write or want an order to show many calculation steps within a formula, then we need to use the `\begin{align} ... \end{align}` command:

$$
\begin{align}
var(aX + bY) &= E[(aX + bY - E[aX]E[bY])^2]\\
&= E[((aX - E(aX)) + (bY - E(bY)))^2] \\
&= E[(a(X-E(X)) + b(Y-E(Y)))^2] \\
&= a^2E[X-E(X)]^2 + b^2(Y-E(Y))^2 + 2 ab(X-E(X))(Y-E(Y))\\
&= a^2\cdot var(X) + b^2\cdot var(Y) + 2\cdot a \cdot b \cdot cov(X,Y)
\end{align}
$$

## Coding In R - A concise overview

Now that we understand the most important concepts of creating and maintaining the programming environment, it is now time to actually start programming. While we are doing this in this file, I want to emphasize that much of the content is taken by Derek L. Sonderegger, who [created an awesome bookdown book in which he explains the most important first steps of using R very concisely](https://dereksonderegger.github.io/570L/index.html).

We will cover most of what he's written in this Markdown. But, if you have any further questions, you can also stick to his files. Note that we do not expect you to understand everything that is written here. However, it should serve as a guideline to show you what is all available for R and what you could use R for. Furthermore, we will introduce some of the most important concepts of his work within our own domain.

### Introduction 

In the first chapter, we cover the basics of R. R is a open-source program that is commonly used in Statistics. It runs on almost every platform and is completely free and is available at www.r-project.org. Most of the cutting-edge statistical research is first available on R. 

Finding help about a certain function is very easy. At the prompt, just type `help(function.name)` or `?function.name`. If you don’t know the name of the function, your best bet is to go the the web page \href{www.rseek.org}{www.rseek.org} which will search various R resources for your keyword(s). Another great resource is the coding question and answer site stackoverflow.

### Vectors

#### Creating Vectors

R operates on vectors where we think of a vector as a collection of objects, usually numbers. In R, we can define vectors with **three** functions. 

**Arbitrary Vectors**

The first thing we need to be able to do is define an arbitrary collection using the `c()` function.

```{r}
# Define a vector of numbers: 
a = c(1,2,3,4)
str(a)
```

An important feature of vector creation is that we can repeat certain entries multiple times to get a vector of only those entries (instead of manually typing in the same number n times). We can do so by using the function `rep(x, times)`, which just repeats x a the number times specified by times:

**Vectors with repeating elements**

```{r}
# This repeats 2 and 3 five times
rep(c(2,3), 5)
```

**Vectors with a sequence**

If we want to define a sequence of numbers, we can do so by using the function `seq(from, to, by, length.out)`. 

```{r}
# Define a sequence from 1 to 10 but only take each second element:
seq(from=1, to=10, by=2)
```

The `by = ` argument is especially handy here, since it allows us to draw any arbitrary sequence. 

#### Accessing Vector Elements

```{r}
# We create a vector first:
vec <- c(1,3,8,5,1,9)
```

**Accessing Single Elements**

We can access elements of vectors by calling a squared bracket around the vector item `[]`. 

```{r}
# Get the 3rd element
vec[3]
```

**Accessing Multiple Elements**

As before, we can access multiple elements by using the `c()` function in the squared brackets

```{r}
# Get the second and fourth element
vec[c(2,4)]
```

**Special Accessing**

If we only want to get a sequence, we do so with `:`.

```{r chunk on}
# Get elements 1 to 3
vec[1:3]
```


If we want to get a sequence, we do so by using `:` as well as a comma separating the non-sequential gap. 

```{r}
# Get elements 1 to 3 and 5. 
vec[c(1:3,5)]
```

If we want everything but the first element, we use a `-` sign.

```{r}
# Get everything except the first element
vec[-1]
```

If we want everything except the first i elements, we use `-1*c(1,i)`.

```{r}
# Get everything except the first three elements and the fifth element
vec[-1*c(1:3, 5)]
```

#### Scalar Functions and Vector Algebra 

We can perform operations on all elements of a vector simultaneously. These operations could be additive, multiplicative, exponentially, logarithmic, absolute etc. Pretty much any calculation you know from mathematics can be done using vectors. Although we introduce to you vector operations in Chapter 3.2.1 and 3.2.2 expect you to go over it yourself, let's give you a quick introduction what we mean by working on vectors:

```{r}
# Define a vector
x_vec <- seq(-10, 10, 2)
x_vec
```

```{r}
# Get the absolute values
abs(x_vec) 
```

```{r}
# Get divided by two values
x_vec/2
```

```{r}
# Get exponential values
exp(x_vec)
```

Furthermore, we can use vector algebra such as we can use matrix algebra in R. 

```{r}
# Define another vector 
y_vec <- seq(10,20,1)

# Use vector additivity
x_vec + y_vec
```

**Commonly used Vector functions**

There are some commonly used vector functions:

```{r, echo = FALSE, message = FALSE}

vector_fun <- c("min(x)", "max(x)", "length(x)", "sum(x)", "mean(x)", "median(x)", "var(x)", "sd(x)")
vector_des <- c("Minimum value in vector x", "Maximum value in vector x", "Number of elements in vector x", "Sum of all the elements in vector x", "Mean of the elements in vector x", "Median of the elements in vector x", "Variance of the elements in vector x", "Standard deviation of the elements in x")

vector_functions <- as.data.frame(cbind(vector_fun, vector_des))
colnames(vector_functions) <- c("Vector Function", "Description")

vector_functions

```
### Data Types

R operates with five major data types: Integers, Numerics, Character Strings, Factors and Logicals. 

**Integers**

These are the integer numbers. That is, they can only be **full numbers** without any decimal point. To convert a number to an integer, you use `as.integer()`. 

```{r}
# Transform to integer
as.integer(1.233)
```

**Numerics**

These could be any number (whole number or decimal). To convert another type to numeric you may use the function `as.numeric()`.

```{r}
as.numeric(1.20)
```

**Strings**

These are a collection of characters (example: Storing a student’s last name). To convert another type to a string, use `as.character()`.

```{r}
x <- "Hello World"

as.character(x)
```

**Factors** 

R handles categorical variables in terms of factors. R does this in a two step pattern. First it figures out how many categories there are and remembers which category an observation belongs two and second, it keeps a vector character strings that correspond to the names of each of the categories. This is done with the function `factor()`.

```{r}
z_vec <- rep(c("A", "B", "D"), 4)
factor(z_vec)
```

Often we wish to take a continuous numerical vector and transform it into a factor. The function `cut()` takes a vector of numerical data and creates a factor based on your give cut-points.

```{r}
# Divide x_vec into four groups of equal length:
cut(x_vec, breaks=4)
```

We can also set the break points individually

```{r chunk check 3}
# Set breaks individually
cut(x_vec, breaks = c(-10, 0, 2.5, 5.0, 7.5, 10))
```

**Logicals**

We can test vector levels in R. This is useful if we need to make a comparison and test if something is equal to something else, or if one thing is bigger than another. To test these, we will use the `<`, `<=` `==`, `>=`, `>`, and `!=` operators. 

```{r}
# See which values are larger than or equal to zero
x_vec >= 0
```

```{r}
# which vector elements are > 0
which(x_vec > 0)  
```

```{r}
# Grab these
x_vec[which(x_vec > 0)]
```

In order to make **multiple comparisons** we use the `&` = `and` as well as `|` = `or` signs. 

```{r}
# Let's make an and comparison:
x_vec[which(x_vec > 0 & y_vec > 5)]
```

```{r}
# Let's make an or comparison:
x_vec[which(x_vec > 0 | y_vec > 5)]
```

### Data Frames

Data Frames are the most common tool to display matrices within R. To generalize our concept of a matrix to include these types of data, R incorporates a structure called a `data.frame`. These are like simple Excel spreadsheet where each column represents a different trait or measurement type and each row will represent an individual. For instance, we create a data frame object like this:

```{r}
# create a data frame object 
df <- data.frame(
  Type = c("A", "B", "C"),
  Solution = c("9", "10", "4")
)

df
```

If we already have vectors that we may want to combine to a data frame, we can also create data frames by using the functions `cbind()` and `rbind()`. cbind binds **vectors into columns** and rbind binds **vectors into rows**. 

```{r}
# Let's create a matrix and corresponding data frame with cbind()
column_bind <- cbind(x_vec, y_vec)
df_cb <- as.data.frame(column_bind)

# Let's change the column names: 
colnames(df_cb) <- c("X Vector", "Y Vector")

# Print it: 
df_cb
```

Because a data frame feels like a matrix, R also allows matrix notation for **accessing particular values**.

```{r}
# Let's grab the first row and column element:
df_cb[1,1]
```

```{r}
# Grab all elements from the first column
df_cb[,1]
```

```{r}
# Grab all elements except for the first three and fifth row 
df_cb[-1*c(1:3,5),]
```

```{r}
# Grab all elements of the second column except for the first three and fifth row 
df_cb[-1*c(1:3,5), 2]
```

Note that we need to adjust the access formula by **separating rows and columns with a comma (",")**. In front of the coma, we define options for rows, and behind for columns. 

Although data frames are just generalised matrices, we still cover the underlying principles of matrices in Chapter 3.2 and expect you to understand this on your own. 

### Importing Data

Now that we know more about how to handle data, we need to understand how to actually read in the data into R. It is most common for data to be in a data-frame like storage, such as a MS Excel workbook, so we will concentrate on reading data into a data.frame.

#### Working Directory 

As we have already discussed, the Working Directory is the source in which R will look for files that you need to upload. As we already covered, you can set a global working directory in R Markdown in the first grey code chunk that appears when opening a new file, with the command ` knitr::opts_knit$set(root.dir = "/This/is/your/path/to/your/files/")`. 

To see where your working directory currently is, use the function `getwd()`.

```{r}
getwd()
```

If you need to temporarily adjust your working directory, you can do so by defining it new for any given code chunk. That is, you can define your new working directory when calling an individual path to the files you want to upload instead of taking the given path. 

For instance, if I want to just upload a csv file called first_return from the usual working directory, I do so by calling: `read_csv("first_return.csv")`. 

However, if I need to get it from another source (let's say from the folder "Empirical Codes"), I need to enter: `read_csv("~/Desktop/Empirical_Codes/first_return.csv")`. 

As such, we need to define the source path which otherwise would automatically be assumed by R to be the given working directory. 

#### Reading in Data

In order to read in the data, we need to understand the `data path`, the `data type` as well as the `separation properties` of the data and the general `structure of columns and rows`. In general, this gives us three questions: 

- What is the path of the data (is it on your laptop or external)?
- What is the separation character of the data (comma, semicolon etc.)?
- Is the first row column names or already data to work on?

**Common data formats**

The most common data formats that you will encounter are text, csv and Stata files. You read in text and csv files by calling the function `read_csv()`, and stata files by calling `read_dta13()`.  

Both reading functions have certain attributes you need to specify:

```{r, echo = FALSE}

args <- c("file", "header", "sep", "skip", "na.strings")
desc <- c("A character string denoting the file location", "Is the first line column headers?", "What character separates columns", "The number of lines to skip before reading data. This is useful when there are lines of text that describe the data or aren’t actual data", "What values represent missing data. Can have multiple. E.g. c('NA', -9999)")

arde <- as.data.frame(cbind(args, desc))

colnames(arde) <- c("Argument", "Description")
arde
```

Let's now read in some data:

```{r}
# We read in some dataset with 
read.csv("~/Desktop/Master UZH/Data/A1_dataset_01.txt", header = T, sep = "\t",  dec = '.')

# Here: 

## file = "~/Desktop/Master UZH/Data/A1_dataset_01.txt" --> an own defined path different from the current working directory
## header = T --> the first column is a header
## sep = "\t" --> the separation is a \t sign 
## dec = "." --> decimals should be separated with a dot
```

### Data Manipulation

In order to explore and modify our data, we use data manipulation techniques. Most of the time, our data is in the form of a data frame and we are interested in exploring the relationships. In order to effectively manipulate the data, we can work with the function `dplyr`. Although it requires you to think a little differently about the code and how it is constructed, it is a very handy feature to explore your data. 

#### Classical functions

We first start with more classical functions. 

**Summary**

`summary()` is to calculate some basic summary statistics (minimum, 25th, 50th, 75th percentiles, maximum and mean) of each column. If a column is categorical, the summary function will return the number of observations in each category.

```{r}
# Let's use the dataset from before: 
df_1 <- read.csv("~/Desktop/Master UZH/Data/A1_dataset_01.txt", header = T, sep = "\t",  dec = '.')
# We just cut it quickly such that it is better visible
df_1 <- df_1[,2:7]

# Create a summary
summary(df_1)
```

**Apply**

`apply()` is commonly used if we want the ability to pick another function to apply to each column and possibly to each row. To demonstrate this, let's say we want the **mean value of each column** for the given data frame.

```{r}
# Summarize each column by calculating the mean.
apply(df_1,        # Set the dataframe you want to apply something on
      MARGIN = 2, # Set whether rows or cols should be accessed. 
                  # rows = 1, columns = 2, (same order as [rows, cols]
      FUN = mean, # Set which function should be accessed 
      na.rm = T)  # Set whether NA values should be ignored
```

This gives us the mean value of each company's prices. 

If we want to get the **standard deviation of prices on the first ten dates**, then we just need to change `MARGIN == 1`.

```{r}
# Summarize each column by calculating the mean.
apply(df_1[1:10,],        # Set the dataframe you want to apply something on
      MARGIN = 1, # Set whether rows or cols should be accessed. 
                  # rows = 1, columns = 2, (same order as [rows, cols]
      FUN = sd, # Set which function should be accessed 
      na.rm = T)  # Set whether NA values should be ignored
```

#### Package `dplyr`

The package `dplyr` strives to provide a convenient and consistent set of functions to handle the most common data frame manipulations and a mechanism for chaining these operations together to perform complex tasks. 

The most important operator using the deployer (= dplyr) package is the pipe command `%>%`. It allows for better or easier readable code. The idea is that the %>% operator works by translating the command a %>% f(b) to the expression f(a,b). The handy feature arises once you have multiple functions you need to incorporate into each other. 

For example, if we wanted to start with x, and first apply function f(), then g(), and then h(), the usual R command would be h(g(f(x))) which is hard to read because you have to start reading at the innermost set of parentheses. Using the pipe command %>%, this sequence of operations becomes x %>% f() %>% g() %>% h(). 

In dplyr, all the functions below take a data set as its first argument and outputs an appropriately modified data set. This will allow me to **chain together commands in a readable fashion**. 

**Verbs**

The foundational operations to perform on a data set are:

– `select`: Selecting a *subset of columns* by name or *column number*.

– `filter`: Selecting a *subset of rows* from a data frame based on *logical expressions*.

– `slice`: Selecting a *subset of rows* by *row number*.

- `arrange`: *Re-ordering* the rows of a data frame.

- `mutate`: Add a *new column* that is some *function of other columns*.

- `summarise`: Calculate some *summary statistic* of a column of data. This collapses a set of rows into a single row.

Let's apply these commands now in the sliced Swiss Markets dataset of earlier. 

*`select`*

We already understand how we can simply select columns by using square brackets [, col], where col is (are) the column(s) of interest. Instead, we can also use `select` in order to select the columns we want: 

```{r}
# First, let's select the first three columns. To not exagerate the output, let's only take the first five rows
df_1[1:5,] %>% select(c(1:3))

```

```{r}
# Now, we can also write the names of the columns instead of their indexes
df_1[1:5,] %>% select(c(ABB:Adecco))
```

```{r}
# Also, not selecting works as well. For instance, get all except the second column. 
df_1[1:5,] %>% select(-c(2))
  
```

The `select()` command has a few other tricks. There are functional calls that describe the columns you wish to select that take advantage of pattern matching. The most commonly used are: 

- `starts_with()`: Show columns that start with a certain pattern (e.g. Letter, Name)
- `ends_with()`: Show columns that end with a certain pattern
- `contains()`: Show columns containing a certain pattern
- `matches()`:  Show columns with a regular expression

```{r}
df_1[1:5,] %>% select(starts_with("B"))
```
```{r}
df_1[1:5,] %>% select(ends_with("B"))
```

```{r}
df_1[1:5,] %>% select(contains("B"))
```

*`filter`*

It is common to want to select particular *rows* where we have some logically expression to pick the rows.

```{r}
# Let's only select the rows in which ABB has a larger Share Price than 30
df_1 %>% filter(., ABB > 30)
```

We can also use the `&` and `|` operators to combine filtering. 

```{r}
# Select only rows in which ABB is above 30 and Adecco below 90
df_1 %>% filter("ABB" > 30 & "Adecco" < 90)
```

We can also now start combining the select and filter commands.

```{r}
# Select only ABB, Adecco and Baloise and then filter such that ABB is above 30 and Adecco below 90
df_1 %>% 
  select(ABB, Adecco, Baloise) %>% 
  filter(ABB > 30 & Adecco < 90)
```

*`slice`*

When you want to filter rows based on row number, this is called slicing. This is the same as with columns, just for rows

```{r}
# Take only the first three rows
df_1 %>% slice(1:3)
```

*`arrange`*

We often need to re-order the rows of a data frame. Although this is, in a security setting with a time-series, not the most useful tool (given that anyway anything is sorted according to date), it still is very useful for many other applications. 

```{r}
# Arrange according to the Share Prices of ABB
df_1[1:5,] %>% arrange(ABB)
```

Although this tells us not much in the given context, it stil is useful for other sorting purposes. For that, let's create another small example. How about treatment level and then gender. That would make more sense to have an overview then.

```{r}

# Create a dataframe 

## Note that treat is a factor and thus must be assigned a level to show which values are "lower" than others. 
treatment <- data.frame(treat = factor(c("Low", "Med", "Med", "Low", "High", "High", "Extreme"), levels = c("Low", "Med", "High", "Extreme")),
                        gender = factor(c("M", "F", "X", "M", "M","F", "F"), levels = c("F", "M", "X")), 
                        age = c(41,40,88,14,10,55,31), 
                        y = c(3,4,5,1,2,7,4))

# Now arrange / sort the dataframe according to multiple rows: 
treatment %>% arrange(treat, gender, desc(age))

```

Great, now we have a better overview, as we sorted first according to treatment, then within treatment according gender, and then within gender according to age (descending). This "layered sorting" is quite handy when using the arrange function. 

*`mutate`*

The perhaps most important function is `mutate`. It is used to create a new column according to functions used from other variables. To do so, let's show you the two different ways to create new variables. 

```{r}
# Let's do it the old-fashioned way. We define a high indicator each time ABB is larger its average 
df_1$High_Indicator <- ifelse(df_1$ABB > mean(df_1$ABB, na.rm = T), 1, 0)
df_1$High_Indicator

# Let's delete the column again: 
df_1$High_Indicator <- NULL
```


```{r}
# Now, the dplyr way. 
df_1[1:5,] %>% mutate(High_Indicator = ifelse(ABB > mean(ABB, na.rm = T), 1,0))
```

The nice thing is that you can also use it to create multiple columns simultaneously, and you can *even refer to columns that were created in the same mutate() command*.

```{r}
# Let's do this again. Let's create the High Indicator and create a High times Price element
df_1 %>%  mutate(High_Indicator = ifelse(ABB > mean(ABB, na.rm = T), 1, 0),
                 High_Price = High_Indicator * ABB)
```

Another situation I often run into is the need to select many columns, and calculate a sum or mean across them. This can be used when combining the `mutate` function with the `apply` function we discussed earlier. 

```{r}
# Here, we only take the mean values of ABB, Actelion and Adecco and create another variable called Average_Price_AAAD
df_1[1:5,] %>% mutate(Average_Price_AAAD = select(., ABB:Adecco) %>% apply(1, mean, na.rm = T))
```

*`summarise`*

`summarise` is used to calculate summary statistics using any or all of the data columns. 

```{r}
# Let's get the mean and standard deviation for ABB

df_1 %>% summarise(ABB_Mean = mean(ABB), ABB_SD = sd(ABB))
```

We can also use `summarise_all` to get the summary statistics of each variable we selected

```{r}
# Here, we take the variables of ABB and Actelion and calculate their mean, variances and standard deviation
df_1 %>% 
  select(.,ABB:Actelion) %>% 
  summarise_all(funs(mean, var, sd), na.rm = T)
```

#### Using multiple commands including split, apply and combine functions

The major strength of the dplyr package is the ability to split a data frame into a bunch of sub-data frames, apply a sequence of one or more of the operations we just described, and then combine results back together. Thus, let's now look at how to combine some of the commands.

```{r}
# Let's create a slightly more advanced data frame than we had previously by looking at the treatment data set again and enhancing it
treatment <- data.frame(treat = factor(c("Low", "Med", "Med", "Low", "High", "High", "Extreme", "Med", "High", "Low", "Low", "Med"), levels = c("Low", "Med", "High", "Extreme")),
                        gender = factor(c("M", "F", "X", "M", "M","F", "F", "X", "F", "M", "M", "F"), levels = c("F", "M", "X")), 
                        age = c(41,40,88,14,10,55,31, 55, 23, 46, 97, 12), 
                        glyphs = c(3,4,5,1,2,7,4,5,1,12,8,6), 
                        cells = factor(c("W", "W", "R", "W", "R", "W","W", "W", "R", "W", "R", "W"), levels = c("R", "W")),
                        smoke = c("Y", "N", "N", "N", "Y", "N", "Y", "N", "N", "N", "Y", "N"))

# Let's do the following now:

## 1) Select all except for the "glyphs" column
## 2) Filter according to male and female individuals without extreme treatment status
## 3) Group by cells
## 4) Calculate the mean age for each group 
## 5) Calculate smoking average age levels 

treatment %>% 
  select(-glyphs) %>% 
  filter(treat != "Extreme" & gender != "X") %>% 
  group_by(cells) %>% 
  mutate(mean_age = mean(age)) %>% 
  mutate(smoke_mean_age = ifelse(smoke == "Y", mean_age, "NA")) 

```

### Data Reshaping

Another important feature in R is Data Reshaping. The idea behind data reshaping is the ability to shape the data frame between a `wide` and a `long` format. Long format is a format in which **each row is an observation and each column is a covariate** (such as time-series formats). In practice, the data is often not stored like that and the data comes to us with repeated observations included on a single row. This is often done as a memory saving technique or because there is some structure in the data that makes the ‘wide’ format attractive. 

Building on this, we may want a way to join data frames together. This makes the data easier to modify, and more likely to maintain consistence. 

#### `tidyr`

As described, the perhaps most important tidying action we need is to transform wide to long format tables. Just like with other packages, there are two main arguments to note here. 

- `gather`: Gather multiple columns that are related into two columns that contain the original column name and the value. 
- `spread`: This is the opposite of gather. This takes a key column (or columns) and a results column and forms a new column for each level of the key column(s)

Although this sound somewhat complicated now, it's actually quite easy to get your head around. 

**`gather`**

```{r}
# Let's create another treatment dataframe

treat_tests <- data.frame(Patient = factor(c("A", "B","C", "D"), 
                                           levels = c("A", "B", "C", "D")),
                          Test_1 = c(1,3,5,4), 
                          Test_2 = c(2,4,6,4), 
                          Test_3 = c(5,3,2,1))

treat_tests
```

```{r}
# Now, we want to understand the overall test scores (or average test scores). We do so by using the long format to get the overall scores per patient. 

treat_tests_all <- treat_tests %>% 
  gather(key = Test,
         value = Scores, 
         Test_1:Test_3)

treat_tests_all
```
Accordingly, we now have an observation per patient and test score, instead of three test observations in one row. 

This is the main idea of transforming from wide to long formats. It basically constitutes that you **define columns that you transform into rows, based on given attributes**. In our case, we used the Test Scores columns and transformed them into one column where we indicated with Test gave which score. This is also quite handy to use with time-series transformation, which is what we will look at later in the course. 

**`spread`**

Spread is the opposite of gather. That is, you transform one column to multiple columns, based on some attributes. 

```{r}
# Let's transform it back
treat_test_back <- treat_tests_all %>% 
  spread(key = Test, 
         value = Scores)

treat_test_back
```

As such, we just reversed what we did before. That is, we transformed one column according to the key attributes from another column. 

### Beautiful Graphs with `ggplot2`

Plotting and graphing data, commonly known as visualising, is perhaps one of the most important abilities you can have as data scientist or analyst. Although the usual plot functions work on R, they are quite ugly. Consequently, I would not recommend you to present any of your output in a general baseline plot, unless you just want to assess the data yourself and no one except you can see it. 

However, once you need to present your data, visualisation is key. Even the best analysis may easily fail with an ugly plot, but you can stretch a bad analysis pretty far if you can deliver on the plot. As such, we will introduce you to the plotting technique required for this course. This comes in form of the package `ggplot`. Although it may be a little tricky to understand its inner workings, you will get the hang out of it soon. And, once you get it, the effort-return relation is pretty nice. 

So, let's get started. 

#### Basic Workings and Options

First, it should be noted that `ggplot2` is designed to act on data frames. It is actually hard to just draw three data points and for simple graphs it might be easier to use the base graphing system in R. However for any real data analysis project, most data will already be in the required format.


There are many defined graphing functions within ggplot. This makes it easy to stack multiple graphs on each other and create quite complicated structures with just some lines of code. The [entire list of options can be found here](https://ggplot2.tidyverse.org/reference/), but the main geometries to display data are the following: 
 
![Figure 3: Geometries in ggplot](~/Desktop/Empirical Asset Management/R/Images/GGPlot_Geometries.jpg)

A graph can be built up layer by layer, where:

* Each layer corresponds to a geom, each of which requires a dataset and a mapping between an aesthetic and a column of the data set.
+ If you don’t specify either, then the layer inherits everything defined in the ggplot() command.
+ You can have different datasets for each layer!
* Layers can be added with a +, or you can define two plots and add them together (second one over-writes anything that conflicts).

#### Metling a dataframe

An important notion for using ggplots is that you need to define which columns should be accessed. This is where the **wide to long format** transformation comes into play. Let's say we have multiple columns that we would like to plot. In order for ggplot to understand which columns we want to print, it is often easier to transform the columns of interest into a long format, implying that you summarise the columns into one single column and have an index column telling you which values belong to which former column. 

This can be done by using the `melt()` function. The melt function does exactly what we did before when transofming wide to long formats. As such, I will primarily stick with this function while using ggplots. 

#### Graph Geometry Options

In this section, we quickly show you the most important geometries and how to apply them. We will use the `mpg` dataset for the illustration purposes. This is a dataset summarising car attributes. 

```{r}
# Load the data
data(mpg, package = "ggplot2")
str(mpg)
```

**Bar Charts**

The simplest graphs are Bar Charts and Histograms. In them, we try to understand some statistical properties about the data, without assessing any relationship thoroughly. 

We may be interested in knowing the count of each class of car first. 

```{r}
# General Structure of a ggplot: 

# Lastly 
mpg %>%  ggplot(aes(x=class)) + # you first define the dataset you want to operate on and assing the tidy sign %>% 
  # Then, you call the ggplot() function and define the aesthetics. Those are the values of the x and y variable, if required
  geom_bar() # Lastly, you call the respective geometry you want for the given display
```

The general way how we constructed a data frame here is:

- The data set we wish to use is specified using `mpg %>% ...`

- The column in the data that we wish to investigate is defined in the aes(x=class) part. This means the x-axis will be the car’s class, which is indicated by the column named class

- The way we want to display this information is using a bar chart, which define after the + sign

**Histograms**

We use histograms if we want to find the distribution of a single discrete variable.

```{r}
# Let's analyse the distribution of cty:

mpg %>% ggplot(aes(x=cty)) +
  geom_histogram()

```

`geom_histogram` breaks up the distribution of the variable into distinct bins. The default bin size is 30. However, we can change the bin size manually: 

```{r}
# Let's analyse the distribution of cty and change the bin size:

mpg %>% ggplot(aes(x=cty)) +
  geom_histogram(bins = 10)

```

Often, instead of using counts, we want to display the *density* on the y-axis. In order to calculate the density instead of counts, we simply add the option `y=..density..` to the `aes()` list that specifies that the y-axis should be the density.

```{r}
# Let's analyse the distribution of cty and change the bin size and density on the y axis:

mpg %>% ggplot(aes(x=cty, y = ..density..)) +
  geom_histogram(bins = 10)

```

**Scatter Plots**

Scatter Plots are used to portray a relationship between two variables based on two to three attributes. 

```{r}
mpg %>% ggplot(aes(x=cty, y = hwy)) +
  geom_point()
```

The only difference between histograms / bar charts and scatter plots is that we **need to define a value on the y-axis** as we are *plotting a relationship*. In our case, this relationship is highly positively correlated. 

If we want to add a further attribute to distinguish the data, we do so by calling the `color = ` argument. 

```{r}
# Still add the relationship between hwy and cty, but now we also add a third dimension in form of the colours for easch distinct transmitter object
mpg %>% ggplot(aes(x = cty, y = hwy, color = trans)) + # Here, just add color
  geom_point(size = 1.3)
```

**Line Plots**

Line Plots are quite similar to scatterplots, but they just connect the dots with actual lines. 

```{r}
# Let's quickly create a line plot with new data

new_dat_line <- data.frame(dose = c(1,2,3,4,5,6,7,8,9,10), 
                           treat = c(2,5,2,7,8,10,4,3,7,9))

new_dat_line %>% ggplot(aes(x=dose, y = treat)) +
  geom_line()
```

We can fine-tune the underlying relation then a little

```{r}
# Let's quickly create a line plot and fine tune the relation 

# Here, we create a dashed line plot with red dots indicating the x value observation
new_dat_line %>% ggplot(aes(x=dose, y = treat)) +
  geom_line(linetype = "dashed", col = "blue") + 
  geom_point(size = 1.3, col = "red")
```


**Box Plots**

Box Plots are used to show a categorical variable on the x-axis and continuous on the y-axis. 

```{r}
# Let's create a boxplot for each class and define its cty distributional parameters
mpg %>% ggplot(aes(x=class, y = cty)) +
  geom_boxplot()
```

Note again how a boxplot is constructed: 

- line below box: show the smallest non-outlier observations
- lower line of box: show the 25th percentile observation
- fat middle line of box: show the median, or 50th percentile, observation
- upper line of box: show the 75th percentile observation
- line above box: show the largest non-outlier observations
- dots: show the outliers

**Density Plot**

If we want to plot the distribution of one continuous variable, we need to plot a density distribution plot. This is the continuous conuterpart to a histogram. In order to do so, R requires us to define a y variable that can plot the values on the y-axis, such that we can have dot combinations that show a filed area. In order to do so, we need to modify our plot of a density distribution. That is, we need to make use of both the `melt()` function as well as the `geom_density()` geometry as a plot. 

```{r message = F, warning = F}
# Let's create the function for only one variable distribution plot
hwy <- mpg[,c("hwy")]
melt_hwy<- melt(hwy)

# This creates of the two columns wide format a one column long format which summarises both columns (indicated as value column). Based on this, we have a column that indicates which of the two former columns is assigned to which value, to now be able to plot multiple column density plots.  
melt_hwy %>% ggplot(aes(x = value, fill = variable)) +
  geom_density(alpha = 0.2)
```

#### Added Geometry Options

**Smooth**

If we have a scatterplot, we can easily add a line to show the average relation between both variables. This is done using the `geom_smooth()` geometry option. 

```{r}
mpg %>% ggplot(aes(x = cty, y = hwy)) +
  geom_point(size = 1.3) + 
  geom_smooth(method = "lm") # This is the geom_smmoth option
```

Here, we add a linear model regression line to define the relation between both variables. The grey area around it is the 95\% Confidende Interval. Inerestingly, if we would add a third dimension in form of the colour argument, then we could see that R produces mutliple smooth lines for each of the z variable values, if the z variable is categorical

```{r}
# Let's create a smooth line for each unique element of the z variable called "drv"
mpg %>% ggplot(aes(x = cty, y = hwy, color = drv)) +
  geom_point(size = 1.3) + 
  geom_smooth(method = "lm") # This is the geom_smmoth option
```

**Errorbar**

Error Bars are also a useful feature to determine more about the structure of the underlying data. You can use error bars in the same fashion as you would use a confidence interval band we've seen in the `geom_smooth` example. Just opposed to the smooth example, error bars are not continuous. They basically indicate an upper and lower bar compared to some pre-defined value you set. In order to use error bars, we make use of the `geom_errorbar()` geometry. 

The most common case to define error bars is to set a mean value and then draw the benchmarks that are $\pm$ 1 standard deviation in proximity to the mean. As such, we mainly use it for further statistical analysis of our underlying variable(s) based on different characteristics. 

For instance, we could aim to find out what the upper and lower standard deviation bounds are for the cty characteristic, and distinguish this for the individual classes. 

```{r}

# First, define new variables for each class. Here, we:

mpg <- mpg %>% 
  # Group by classes 
  group_by(class) %>% 
  # Get the mean and sd values for the cty variable based on its class
  mutate(mean_cty = mean(cty)) %>% 
  mutate(sd_cty = sd(cty)) %>% 
  # Create lower and upper bounds per class 
  mutate(lwr_cty = mean_cty - 1.96*sd_cty) %>% 
  mutate(upr_cty = mean_cty + 1.96*sd_cty)

# Define the error bar plot for each class
mpg %>% ggplot(aes(x = class)) + # Still get in the x axis the individual classes
  # Get the error bars per class
  geom_errorbar(aes(ymin=lwr_cty, ymax=upr_cty)) + 
  # Get the mean points per class
  geom_point(aes(y = mean_cty), col = "blue", size=3, shape=21, fill="white")
```

#### Plotting Multiple Variables

Plotting multiple variables can, at times, be tricky in ggplot. However, especially when considering time-series data or distributional characteristics, it may be important to display multiple columns in the same plot. This is the case when considering histograms or density plots. In order to circumvent this issue, one (although not the only) approach is to transform multiple columns into one column. We've seen this already when we introduced the wide to long transformation format. 

This type of transformation is possible by using the function `melt()` that we've introduced while working with density plots. In essence, melt takes the columns of interest and creates two additional columns: A `value` column, which includes all the values of the baseline columns stacked beneath each other, as well as a `variable` column, which assigns to each value cell the respective variable name of the baseline columns. 

By using melt, R understands that multiple values are assigned specific to specific variables. By using a two-column organisation, ggplot is able to draw a x-y combination of data, making it easier to handle the relationship. 

Let's apply this. For instance, take two variables: 

```{r, message = F, warning = F}
# To create a density function, we first need to define a density to melt the respective variables into one column
hwy_cty <- mpg[,c("hwy", "cty")]
melt_hwy_cty <- melt(hwy_cty)

# This creates of the two columns wide format a one column long format which summarises both columns (indicated as value column). Based on this, we have a column that indicates which of the two former columns is assigned to which value, to now be able to plot multiple column density plots.  
melt_hwy_cty %>% ggplot(aes(x = value, fill = variable)) +
  geom_density(alpha = 0.2)
```

```{r}
# We can also create a histogram from it
melt_hwy_cty %>% ggplot(aes(x = value, fill = variable)) +
  geom_histogram(alpha = 0.2)
```

#### Plotting Time-Series

Plotting time-series objects will be especially important throughout this course. In essence we want to see how different variables behave throughout time. Remember that we always use an xts object transformation to work with time-series data. Also note that all data manipulation strategies work exactly in the same way as when working with non time-series based data. 

**Line Plots**

```{r}
# Let's load some of the data on the SMI again
df_2 <- read.csv("~/Desktop/Master UZH/Data/A1_dataset_01.txt", header = T, sep = "\t",  dec = '.')[,c("Date", "ABB", "Adecco", "Nestle_PS", "Roche_Holding")]

# Use a Date column
df_ts_date <- as.Date(df_2$Date)

# Transform the new dataset into an xts object
df_ts <- xts(df_2[,-1], order.by = df_ts_date)

# Calculate the Returns (We show you everything on this next week)
df_2_ret_ts <- Return.calculate(df_ts)
df_2_ret_ts <- df_2_ret_ts["1999-09-30/2010-12-31"]
df_2_ret_ts_cum <- cumprod(1+df_2_ret_ts)

head(df_2_ret_ts_cum)
```

Now, we created a time-series return data frame that we will use for further elaboration. Especially, we are interested in understanding the return behaviour over time of the securities under consideration. Doing so, we need to work with the `tidy()` function that we introduced earlier. 

```{r}
# Here, we create a tidied time-series function on which we then run the ggplot
tidy(df_2_ret_ts_cum) %>% ggplot(aes(x = index, y = value, color = series)) + 
  # The ggplot aesthetics block takes three arguments: 
  ## 1) x = index (Time-Series (Years here))
  ## 2) y = value (The actual cumulative returns data)
  ## 3) color = series (The different securities)
  geom_line()

```

This is a very handy feature to define a ggplot time series. Note the steps that we performed: 

* We created a tidy-time-series (TTS). This is, to some extent, a similar format as what we get when using the `melt()` function. 
+ From this, the program operates as if we merge the multiple columns of the time-series returns for the different securities into one column and assign the respective security names to each time-series return structure. 
* We then created a ggplot aesthetics block with the following arguments:
+ x = index
- This takes the index column of the dataframe and assigns it to the x-axis of the plot
- Since we work with an xts object, the date column is the index column
- We assigned the index status to the date column by setting the `group.by = ` to the respective date column we defined
+ y = value
- This takes the artificially created value column and prints its values on the y-axis 
- Since we used a `tidy()` command for our dataset, we artificially melted the different security columns into one stock column
- This column is now assigned the name "value" and is the long-format from the previous wide format of securities
+ color = series
- This takes the artificially created variable column and assigns the time-series relation of each distinct value component to its respective group (in this case: the respective company)
- Since we used a `tidy()` command for our dataset, we artificially melted the different column names of the wide dataset for each security return into one company column
- This column is now assigned the name "series" and is identical to the variable column we defined in melt

**Density Plots**

Working with time-series, we can use the same format structure to create many plots:

```{r}
# Let's plot the densities:
tidy(df_2_ret_ts_cum) %>% ggplot(aes(x = value, fill = series, color = series)) +
  # We define here: 
  ## 1) x = value: These are the cumulative returns used on the x-axis 
  ## 2) fill & color = series: These are the y-axis attributes that give you 
  ### (1) the color which defines the density attributes for each stock
  ### (2) the filling of the density for each security (if you remove the fill command, you just get lines)
  geom_density(alpha = 0.2)
```

Here, we defined the same set-up as with the line plots, but just had to re-shuffle in order not to use the time-series character of the data. 

**Melt and Tidy**

By using this set-up, we are able to draw a time-series relationship of multiple stocks and distinguish them by color. Note that this is  **identical to the melt set-up** we used to create two differently coloured histograms or density plots for each of the two variables. In effect, looking at the both datasets, we see their identity:

```{r}
tidy(df_2_ret_ts_cum)
```

```{r}
head(melt_hwy)
```

The only, but MAJOR, difference is, that you should use: 

- A **tidy** dataset for **Time-Series**
- A **melt** dataset for **Cross-Sections**

#### Further Aesthetics

We now have seen how to use the basic plotting options, how to add geometry features and how to apply them to multiple variables in both cross-sectional as well as time-series structures. Now, we can start fine-tuning our plots. That is, we can start to make them more visually appealing and add important information surrounding the data. [The entire documentation can be found here](https://ggplot2.tidyverse.org/reference/), but we will go over the major arguments.  

**Colours**

First, let's look at the color scales offered. ggplot offers over 150 distinct colors that you can use. These colours can be used on any plot, to define lines, histogram fillings, points, bars, regions etc. A general overview of all available colours is in the image below 

![Figure 4: Colors in `ggplot`](~/Desktop/Empirical Asset Management/R/Images/ggplot2_colors.jpg). 

Further, we can use colour scales. These are ranges of colours that enable the use of continuous colour formats, which are often used enable a better distinction between certain values based on the color attribute (e.g. blue are low and red high values). 

![Figure 5: Color Ranges in `ggplot`](~/Desktop/Empirical Asset Management/R/Images/ggplot2_color_ranges.jpg)

In general, ggplot offers some color combinations for all geometries. There are numerous main arguments to use:

- `scale_color_manual()`: Manually add colors for lines and points in the plots
- `scale_colour_steps()`: Step-wise add colors for lines and points in the plots
- `scale_colour_gradient()`: Gradient color format for lines and points in the plots (e.g. scale colors)
- `scale_colour_grey()`: Grey-scale colours for lines and points in the plots

- `scale_fill_manual()`: Manually add colors used for the filling of areas or densities
- `scale_fill_steps()`: Step-wise add colors used for the filling of areas or densities
- `scale_fill_gradient()`: Gradient color format for the filling of areas or densities
- `scale_fill_grey()`: Grey-scale colours used for the filling of areas or densities

**Shapes, Radii, Sizes**

Despite distinguishing data by colour, we can also find discrepancies by using different forms:

- `scale_linetype()`: Automatically assign line types to different variables
- `scale_shape()`: Automatically assign point shape types to different variables 
- `scale_size()`: Automatically assign point size or radius to different variables 

**Scales**

- `ggtitle()`: Set the title of a plot
- `xlab()`, `ylab()`: Set the x-axis and y-axis titles
- `xlim()`, `ylim()`: Set the x-axis and y-axis limits
- `labs()`: Set the title of the legend

**Themes**

* `theme()`: Used as an umbrella term to define sizes, margins, colours and grids of the background of a plot. In it you get: 
+ `plot.title`: Used to define the plot title design. Main arguments used: 
- `element_text()`: Used to define the elements of the title design, including: `size`: Title Size, `color` = Title Colour, `hjust` = Horizontal Adjustment (How centered the title should be), `lineheight` = Height between the lines if the title has multiple lines, `margin` = The exact positioning between the upper end of the image and the actual plot
+ `axis.title.x`, `axis.title.y`: Used to define the plot x-axis and y-axis title design. Main arguments used are identical to `plot.title`
+ `panel.background`: Used to define the background of the panel (part that includes the roaster with data). Main argument used: 
- `element_rect`: Used to define the filling (`fill`) and the colour (`colour`) of the panel background
+ `plot.background`: Used to define the background of the plot excluding the panel. Main arguments used are identical to `panel.background`
+ `panel.grid.major.x`, `panel.grid.major.y`: Used to define the design of the major vertical and horizontal lines on the x and y axis (if needed). Main arguments are:
- `size`: Get the size of the line
- `linetype`: Get the type of the line
- `colour`: Get the colour of the line
- `element_blank()`: Do not add any line
+  `panel.grid.minor.x`, `panel.grid.minor.y`: Used to define the design of the minor vertical and horizontal lines on the x and y axis (if needed). Main arguments identical to the major options
`axis.line`: Used to define the design of the x and y axis. Main argument is:
- `element_line()`: Gives the colour of the x and y axis


```{r}
# Create the plot

figure <- tidy(df_2_ret_ts_cum) %>% 
  # Plot design in general
  ggplot(aes(x=index,y=value, color=series)) + 
  # Plot geometry(ies)
  geom_line() + 
  # Line, Point and Fill colours
  scale_color_manual(values=c("goldenrod", "darkorchid4", "darkorange2","dodgerblue1", "springgreen2",  "darkorchid4", "dodgerblue4"))  +
  scale_fill_manual(values=c("goldenrod", "darkorchid4", "darkorange2","dodgerblue1", "springgreen2",  "darkorchid4", "dodgerblue4")) +

  # IMPORTANTLY, IF YOU HAVE MULTIPLE COLOURS, THE COLOUR SORTING (WHICH VARIABLE GETS WHICH COLOUR) IS BASED ON THE NAME OF THE VARIABLE, NOT ITS POSITION IN THE DATAFRAME. E.G.: HERE, THE SEQUENCE OF VARIABLE NAMES IS: 10K-PHYS, 10K-REG, 8K-GEN, MC-GEN, MC-PHYS, MC-REG, MC-OPPTY 
  # --> THIS IS THE WAY HOW GGPLOT ASSIGNS COLOURS ABOVE!

  # X and Y axis string
  ylab("Cumulative Returns") + xlab("Time") + 
  # Title string
  ggtitle("Relationship of Cumulative Returns of some SMI Securities") +
  labs(color='Factor Portfolios') +
  theme(
  # Title Elements
  plot.title= element_text(size=14, color="grey26", hjust=0.3,lineheight=0.4, margin=margin(15,0,15,0)), 
  # Axis Elements
  axis.title.y = element_text(color="grey26", size=12, margin=margin(0,10,0,10)),
  axis.title.x = element_text(color="grey26", size=12, margin=margin(10,0,10,0)),
  # Background colour and fill
  panel.background = element_rect(fill="#f7f7f7"),
  plot.background = element_rect(fill="#f7f7f7", color = "#f7f7f7"),
  # Major Panel Grids
  panel.grid.major.x = element_blank(),
  panel.grid.major.y = element_line(size = 0.5, linetype = "solid", color = "grey"),
  # Minor Panel Grids
  panel.grid.minor.x = element_blank(),
  panel.grid.minor.y = element_blank(),
  # Line colour of the x and y axis
  axis.line = element_line(color = "grey")) 

figure
```

#### Facetting plots

The goal with faceting is to make many panels of graphics where each panel represents the same relationship between variables, but based on attributes of a third variable. This is basically the same as if we would print relationships according to colour, where colour defines the third attribute under consideration. We use this tool with the function `facet_grid()`. 

```{r}
# Let's create a faceted ggplot by drawing the relationship of cty and hwy on a scatterplot
mpg %>% ggplot(aes(x = cty, y = hwy)) + 
  geom_point() + 
  # Here, we define that it should print the same relation just for each year separately
  facet_grid(. ~ year) 
```

The `facet_grid()` tells ggplot to plot the relationship of hwy and cty twice, for each year separately. 

Note that we can easily extend this relation up to multiple dimensions. Let's take another variable into the equation. 

```{r}
mpg %>% ggplot(aes(x = cty, y = hwy)) + 
  geom_point() + 
  # Here, we define that it should print the same relation just for each year separately
  facet_grid(drv ~ year) 
```

Here, we take the relation between hwy and cty in a scatterplot, but we do this depending on the **combination of two variables** now. The first is the year of construction, the second is the driving gear (four-wheel, rear or forward drive). As such, we introduce another dimension on the facet grid. As such, we can see relations for both the driving gear and the year of production. 

#### Adjusting Scales

It is often useful to scale the x and y axis in order to display logarithms, exponentials or other forms of transformations. For this, let's take a data set that incorporates income and age. 

```{r}
# Load the data set
install.packages("Lock5Data")
library(Lock5Data)
data(ACS)
```

First print income to age in a normal relation:

```{r}
# Unadjusted scatterplot
ACS %>% ggplot(aes(x = Age, y = Income)) +
  geom_point() 
```

Here, the data is not well structured, as certain values skew the display of the data considerably. 

Consequently, we need two adjustments: 

1. We need to adjust the y values in order to get a better shape of the data 
2. We need to adjust the y-axis to represent the scaled the y values correctly

If we now want to use a log-transformation, we do so by the following code:

```{r}
ACS %>% ggplot(aes(x = Age, y = Income)) + 
  geom_point() + 
  # Here, we add the log-transformation on the y-axis
   scale_y_log10(breaks=c(1,10,100, 1000),
  # First define the major break lines according to the log-scale
                minor=c(1:10,
                        seq(10, 100,by=10 ),
                        seq(100,1000,by=100))) +
  # Then, set the minor break lines as sequence from to and by a given amount
  # to get the e^10 distributional marks 
  ylab("Income in 1'000 US Dollars")
```

The handy property of the scaling of the y-axis is that it **directly transforms the values on the y axis** such that they are correctly positioned in the graph. As such, we see that the area between **1 and 10 is the same 10 to 100 and 100 to 1000**. Consequently, we can better show the relations, since they are **proportionally transformed**. 

Note that we also added some minor gridlines in the form of either 1000, 10000 or 100000 for the respective horizontal levels. This is to better distinguish and identify the appropriate salary levels. 

**More Transformations**

Apparently, logarithmic transformation is not the only scaling option. [To get an overview, visit the summary page of ggplot here](https://ggplot2.tidyverse.org/reference/). 

### Dates and Times

We will be working often times with time-series data. This implies that dates are key when administering and operating with the data. However, dates come in many forms and shapes and are quite unsimilar to each other sometimes. Although the financial industry tries to have common notions regarding date formatting, there still is a discrepancy between the European and Anglosaxen way of writing dates. 

#### Creating Date and Time Objects

To create a Date object, we need to take a string or number that represents a date and tell the computer how to figure out which bits are the year, which are the month, and which are the day. The `lubridate` package is designed for exactly this purpose. 

![Figure 5: Different Date types in `lubridate`](~/Desktop/Empirical Asset Management/R/Images/Dates_R.jpg)

Usually, in time-series formats for asset pricing, we always use the `ymd()` specification. Accordingly, the `lubridate` package works such that it is able to **transform any of these six orders into the `ymd()`** format. To see this, look at the following: 

```{r}
# Let's transform a date written in month, day and year to a ymd() object
mdy( 'May 31, 1996', 'May 31 96', '5-31-96', '05-31-1996', '5/31/96', '5-31/96')

```

```{r}
# Let's transform a date written in day, month and year to a ymd() object
dmy("31st May 1996", "31.05.1996", "31 May 96", "31 May 1996", "31-5-96", "31-5-1996")
```

As we can see, you can tell lubridate which format your date cell is in and it will automatically transform the date into a `dmy()` format. Especially, we see that the all of the other common date formats can be transformed to this. Especially, we see that this all is possible: 

- "31.05.1996": German / European Date format
- "31st May 1996": Anglosaxen format with String in dmy 
- "May 31, 1996": Anglosaxen format with String in mdy
- '5-31-96', '05-31-1996': Anglosaxen format without String in mdy

In our case, we usually need to create a `date()` variable which we then use to define an order in an `xts` object. We can easily do so by combining a `lubridate` command with the `as.Date()` command. 

```{r}
# Let's say our data is in a European format. This implies the entire date column of the data frame is given as: dd.mm.yyyy (e.g. 31.05.1996)
# We can now easily transform this to a ymd() and then to a date column
date_test <- as.Date(dmy("31.05.1996"))
date_test
```

And here, we see that we were able to transform the data into the correct format which we can use for `xts` objects. 

#### Adding time to date

If we want to add a time to a date, we will use a function with the suffix `_hm()` or `hms()`, (= hour-minute-second). 

```{r}
# Let's transform a date written in day, month and year to a ymd_hms() object
dmy_hms("31st May 1996 16:05:11", "31.05.1996 16:05:11", "31 May 96 16:05:11", "31 May 1996 16:05:11", "31-5-96 16:05:11", "31-5-1996 16:05:11")
```

```{r}
# Let's transform a date written in day, month and year including HMS to a ymd_hms() object
dmy_hms("31st May 1996 16:05:11", "31.05.1996 16:05:11", "31 May 96 16:05:11", "31 May 1996 16:05:11", "31-5-96 16:05:11", "31-5-1996 16:05:11")
```

As we see, the exact same strategy works as before. Consequently, the `lubridate` package can transform both only dates as well as date and time objects into a classical date and time structure. 

This is also interesting for us when transforming date and time objects of a different structure into date objects of structure `yyyy-mm-dd`. For instance, some dates we get online incorrectly include a time attribute. This is the same time attribute for each date and it contains no information about the data. We can get rid of this time infromation accordingly:

```{r}
# Let's say again our data is in a European format. This implies the entire date column of the data frame is given as: dd.mm.yyyy (e.g. 31.05.1996). Further we get the same time object, e.g. 16:05:11 (maybe b/c the data was gathered then).
# We can now easily transform this to a ymd_hms() and then to a pure date column
date_time_test <- as.Date(dmy_hms("31.05.1996 16:05:11"))
date_time_test
```

As we can see, we excluded the time part of the object and can thus only work with dates. 

#### Extracting information from a date

We can get the specific information of a date column in R. For instance, we can get the weekday, the month, the second, or the quarter in which the date is. This may be handy for transformation and grouping purposes of time-series data. A full list is available below: 

![Figure 6: Extracting information from a date-time object in `lubridate`](~/Desktop/Empirical Asset Management/R/Images/Date_Info_Extraction.jpg)

#### The time-series of Returns - Transformations in R

Note that security analysis is always given within a time-series format. We now saw how we can transform date objects to get a common fomrat or how we can extract information from date objects. 

Next, we will show you how to **create a time-series object**. We will often work with data which is not pre-defined as a time-series object. However, in order to use many of the analytics functions needed in empirical asset management, we first need to modify the data set into a time-series format. We do so by using the package `xts`. In the next code block, we will show you how to create such an object and what the structure of this object is. This is the common procedure of transformation in order to get the data in a format with which we can use all the required packages for our analysis. 

```{r}

# Load in the dataset with your path
A1 <- read.csv("~/Desktop/Master UZH/Data/A1_dataset_01.txt", header = T, sep = "\t",  dec = '.')

# Look the data set to get a better overview
head(A1)
```

Here, we see how the data is structured. Although it already is in a long format, we still need to transform it to a time-series. Especially, we need to ensure that the Date column is located in the index of the data frame. We do so by following this code:

```{r}

# Define the date column in the dataset as as.Date()
date = as.Date(A1[,1]) 

# Here, we first assign a date format to the date variable, otherwise the xts package cannot read it. 
# Other forms of transformation (as.POSIXct etc.) would certainly also work. 
A1ts <- xts(x = A1[,-1], order.by = date)

str(A1ts) # Print the structure of Returns

```

As you can see, the date column is now an index. We need this to be an index to be better able to calculate through it. 

**Small Primer into using lag()**

`lag()` is one of the most commonly used functions when working with time-series. You can use lag(1) for instance to define a variable that is lagged by 1 period. Like that, you can calculate returns manually. However, there is an *important caveat when using lag()*. That is: **if the package `dplyr` is loaded, lag won't work when trying to render or knit the document**. This is b/c the lag function is available in both the `stats` and `dplyr` package. Thus, when we do ont specify from which package we should take the lag function, it will take the function automatically from a "higher-order" package, in this case dplyr. However, since the dplyr lag function cannot be rendered, we need to specify that we want to take the lag function explicitly from the stats package. That is, we need to write: `stats::lag()` to use the function. 

#### Checking the time-series dimensions of different dataframes 

Usually, we may work with different dataframes. It is not given that they share the same format of dates. Luckily, we found a way to circumvent this issue and to put all respective dates to the same format. 

However, another issue which commonly occurs is related to the correct **timing of the dates**. This implies that different datasets have a slightly different date for certain observations. It might be that some dates are just incorrectly defined. Other times, it can be that different dataframes have different rules for which date they want to use. For instance, when looking at monthly data, some datasets may regard the **last weekday date** of a given month as final date. However, others may regard the **actual last date** of a given month as final date. 

To put this clearly, let's say data frame A regards the last *Friday* of a month as final date. Let's say that this is the *29th of January in 2018*. However, other datasets regard the last actual day of the month as final date. In our case, this would be the *Sunday, 31st of January in 2018*. 

This is, unfortunately, a common issue when considering periodic datasets. Unfortunately, many financial algorithms we use require an **exact match of dates** when we feed them with data of different sources. Otherwise, they will just return an error. And often, this error is not automatically traced back to the date issue, and you may spend hours looking for the bug before realizing that it was a simple date mismatch. 

Consequently, it is essential to understand how we can notice and flag non-matching date observations and to comprehend how we can match the dates such that we have the same date structure and format for all our distinct data. 

Let's show you how to do this: 

```{r}

# Load two datasets with different dates
A1 <- read.csv("~/Desktop/Master UZH/Data/A2_dataset_01.txt", header = T, sep = "\t")
A2 <- read.csv("~/Desktop/Master UZH/Data/A2_dataset_02.txt", header = T, sep = "\t")

# Create a time-series as usual and calculate the bond return as usual
A1_date <- as.Date(A1$Date)
A1_ts <- xts(x = A1[,-1], order.by = A1_date)
A1_ts_returns <- Return.calculate(A1_ts, method = "discrete")[-1,]

A2_date <- as.Date(A2$Date)
A2_ts <- xts(x = A2[,-1], order.by = A2_date)
A2_ts_1_year <- A2_ts$SWISS.CONFEDERATION.BOND.1.YEAR...RED..YIELD / 100
A2_ts_1_year_monthly <- ((1 + A2_ts_1_year)^(1/12) - 1)[-1,]

# Combine the two dataframes
df_dates_comp <- as.data.frame(cbind(index(A1_ts_returns), index(A2_ts_1_year_monthly)))

# Create the dates again (transform them from numeric arguments to dates again)
df_dates_comp_real <- as.data.frame(lapply(df_dates_comp, function(x) as.Date(x)))

# Add an index for later sourcing 
df_dates_comp_real$ID <- seq.int(nrow(df_dates_comp_real))

# Column names
colnames(df_dates_comp_real) <- c("Stocks_TS", "Risk_Free_TS", "Index")

# Match the dates
df_dates_comp_real$compared <- ifelse(df_dates_comp_real$Stocks_TS == df_dates_comp_real$Risk_Free_TS, 1, 0)

# Filter which dates are not aligned
df_dates_comp_real %>% filter(compared == 0)

```

Here, we see the small discrepancy that may be very annoying in our time. Of nearly 400 observations, there is exactly one that does not match due to an issue we don't know. However, in order to use the funcitonalities of time-series management in R, we need to align these dates. Luckily, we also defined an index column. Based on this index column, we can then change the respective observation in the risk-free dataframe. 

```{r}
# Change the respective index
index(A2_ts_1_year_monthly)[355] <- "2018-01-29"
A2_ts_1_year_monthly[355]
```

And here we are. We now were able to change the date accordingly. 

### String Manipulations in R: The `stringr()` package

Strings are a major component of data in R. They are needed to perform a wide range of data manipulation tasks. However, in order to use strings, we cannot use the exactly same syntax and approaches as we did for numeric data. Using strings will enable us to vastly widen our programming abilities, since we need strings in large data classes. For instance, the correct use of string manipulation may be extremely handy if you need to perform large, identical operations for dozens or hundreds of individual data frames. Things like loading multiple frames with different, but similar names, assigning names to columns / rows, extracting features of specific data frames and creating new data frames with individual names are very common manipulation techniques as data scientist. These operations are often used in terms of for loops, where you take multiple distinct data sources, run the exact same experiment on each, and obtain individual outputs with distinct names again. You can repeat all steps manually for each data source, but if you have more than 10 sources, then this will get quite messy and exhausting. So let's see how we can lever these techniques. 

`stringr()` is the main package when operating and manipulating strings in R. It adds more functionality to the *base functions* for handling strings in R, and is also compatible with *regular expressions* (we'll cover what they are in the second part, but they are immensely powerful!). In general, stringr offers a wide range of string manipulation techniques and is using consistent function arguments to match structural patterns with other R packages. 

#### Basic string operations

The main basic functions are the following: 

![Figure 7: Main basic manipulation operations in `stringr`](~/Desktop/Empirical Asset Management/R/Images/stringr_basic_ops.jpg)


**Concatenate: `str_c()`**

The function `str_c()` is used to concatenate different strings with one another. This is equivalent to the `paste()` function we saw earlier when printing some of the results. 

```{r}
# Let's create a small example:
str_c("The", "SMI", "gained", "five", "percent", "annually", sep = " ")
```

**Character length: str_length()**

We use str_length() to count the character length of each string under consideration. That is:

```{r}
# Let's create a small example to count the length of each word
str_length(c("The", "SMI", "gained", "five", "percent", "annually"))
```

**Substring characters: str_sub()**

To extract substrings from a character vector stringr provides `str_sub()`. The main function of it is the following. 

```{r}
# Let's only get part of the string
text <- c("The", "SMI", "gained", "five", "percent", "annually")
str_sub(text, start = 1, end = 3)
```
v
This returns the substring that contains the first until the third letter of the word "hello". 

We can basically perform any manipulation operation as we did it when using select and other methods:

```{r}
# Let's only get the last three strings:
str_sub(text, start = -3, end = -1)
```

We can use the command to replace things

```{r}
# Let's increase all strings:
str_sub(text, 1, 3) <- "$$"
text
```

**Duplicate: str_dup()**

We can also duplicate words: 

```{r}
# Let's duplicate: 
str_dup(text, 2)
```

**Padding with str_pad()**

Here, the idea is to take a string and pad it with leading or trailing characters to a specified total width. That is, you get a character and you define its overall length, then you can pad it to the specific length with some filling character(s) of your choice. The general form of it is: `str_pad(text, width = , side = , pad = )`. For instance, we can add two connecting lines on either side of the word "hashtag", by defining the following function: 

```{r}
str_pad("hashtag", width = 11, side = "both", pad = "-")
```

**Trimming with str_trim()**

One of the typical tasks of string processing is that of parsing a text into individual words. Usually, you end up with words that have blank spaces, called whitespaces, on either end of the word. In this situation, you can use the str_trim() function to remove any number of whitespaces.

For instance:

```{r}
text_white <- c("This", " example ", "has several   ", "   whitespaces ")
str_trim(text_white, side = "right")
```

Like this, we create a text with only one white space and make it readable for text. 

**Replacing with str_replace()**

`str_replace()` is quite handy if we need to replace some sub-string within our data. For instance, let's assume we have dates. 

```{r}
# Create a date vector:
dates <- c("2008:Feb", "2010-Mar", "2005-Jun", "2003-Nov")

# Replace the : with - 
str_replace(dates, ":", "-")
```

**Splitting with str_split()**

Also, we can use `str_split()` to split strings at given intervals. 

```{r}
# Split at the - signs
str_split_fixed(str_replace(dates, ":", "-"), pattern=fixed('-'), n=2)

```

These are the most common string manipulation operations. 

#### Using for loops with `paste0()` and `assign()` to create new variables dynamically

Let's now get to an exciting use case of string transformation: Repeating string manipulations for multiple data frames using for loops. Imagine that you have multiple data frames and you need to perform the same set of operations for each of them. Either, you can do this manually by writing the function for each frame itself, but this may be extremely cumbersome once you need to work with potentially hundreds or thousands of individual data frames. Consequently, we need to find a way load, store and output each file automatically to get the desired output. 

We can facilitate this operation by using the functions `paste0()` and `assign()`. 

`assign()` basically assigns a name to a variable. This is nothing special in general, but it is handy when considering that we can use the function to assign a variable name to a newly generated variable, which then is unique based on the for-loop characteristic. 

`paste0()` is similar to the paste function we know from earlier. In a for-loop characteristic, we can use the function ot define a unique string argument that the for loop should create. This is handy, as it can also automatically paste strings into a function, to indicate, for instance, which variable should be created or selected. 

In essence, we can use the **assign function in combination with the paste0 function to create new variable names within a for-loop dynamically**. For that, let's assume that we have multiple portfolios and we need to load them, transform them to a time-series object and, ultimately, plot their cumulative returns. The exact operation will make more sense once we covered risk and return, but we focus more on the technical aspects of creating variables and data frames dynamically. 

```{r}
# We have a folder consisting of seven distinct but identical datasets. It is our aim to perform the same operations on each of the dataset. For that, we need to define a way to make R understand which datasets it should read in, how to differentiate these datasets from each other, and how to use the same functions on all of the datasets to get the outputs of interest. 

# As we have multiple data frames, we want to do this all within a for loop structure. 

# First, we want to read in the data. To do so, we need to tell R what the name of the different data files is, and where they are located. 
# We know that the usual command for reading in a csv file is: read_csv("way/to/path/file.csv", header = , sep = "", dec = ""). 
# Further, we know that the path to our files is the same for each of them, namely: "~/Desktop/EMMA/Excel_Files/Cumulative_Returns/filenames.csv"
# As a consequence, we need to change the "filenames.csv" to the respective name of the individual csv files such that read_csv() can read them in. 
# To do this in a for loop, we need to define the respective names of each csv file in a vector. This is done below. 
data_names <- c("Cum_Ret_MC_Gen", "Cum_Ret_MC_Phys", "Cum_Ret_MC_Reg", "Cum_Ret_MC_Oppty", 
                "Cum_Ret_10K_Phys", "Cum_Ret_10K_Reg", "Cum_Ret_8K_Gen")

# Now, we have the names. We now need to tell R to:

## loop through each element of these names
## paste each element together with the path structure to get the file path of each csv file 
## run the csv_read() command for the respective filepath to read in each file 
## assign a name to each uploaded csv file (otherwise we would just overwrite each file over and again while running the loop, resulting in only the last file being read in and saved)

# This is one of the ways to proceed 
for (i in 1:length(data_names)){
  # First, we tell R to loop through each element in the vector "data_names" which contains the names of the csv files
  paste0(assign(paste0("data", i),
                # Here, we create a name for the data frame we read in (in our case data1 - data7). These are the names of the read-in csv files
         # Here, we assign the created names to the individual read-in csv files (this is identical to e.g.: data1 = read.csv(...)) which is how we            would assign a name if we would read in the data manually 
         read.csv(paste0("~/Desktop/MA_Fundamentals_Non_Coding/Excel_Files/Cumulative_Returns/", data_names[i], ".csv"), sep = ",")))
         # Here, we read-in the data. We use another paste0 command to paste together the:
          ## 1) path to the files 
          ## 2) respective data name 
          ## 3) csv indicator
        # E.g. for the first element of the files, paste0 creates the following string:     
          ##"~/Desktop/EMMA/Excel_Files/Cumulative_Returns/Cum_Ret_MC_Gen.csv"
        # This looks identical to when we usually type in the data path and name manually. 
  
  # We repeat this for all the individual csv files at the given location. As such, we read-in seven csv files and named them data1 - data7
  
}

# Great, we now know how to read in data from multiple sources into R. If we look at them, we realize they are cumulative products of portfolios for a given time period. However, they are not yet in the desired xts object format. In order to change that, let's use another for loop to create xts objects for each of the data frames we have now. 

# In order to create time-series objects from the data frames defined, we need to re-frame the data frames. That is, we need to define a vector with all current data frames and define a list with names to define how their time-series objects should be called. This is done below:
list_df <- list(data1, data2, data3, data4, data5, data6, data7)
list_names <- c("Cum_Ret_MC_Gen_ts", "Cum_Ret_MC_Phys_ts", "Cum_Ret_MC_Reg_ts", "Cum_Ret_MC_Oppty_ts", "Cum_Ret_10K_Phys_ts", "Cum_Ret_10K_Reg_ts", "Cum_Ret_8K_Gen_ts")

# Now, we need to again loop through each element of the list with all the data frames
for (i in 1:length(list_df)){
 # First, we tell R to loop through each element in the vector "list_df" which contains the read-in csv files

 # Now, we just use the same steps as before to create an xts object, just within a loop format  
 list_df[[i]] <- list_df[[i]][,-1]
 # First, we exclude the first column from the current data frames since this is just an empty column. Note that we need a double bracket [[]] as 
 # we first need to access the individual file from the file list and then the variable from that file
 list_df[[i]]$Date <- format(as.Date(list_df[[i]]$Date), "%Y-%m-%d")
 # Next, we need to transform the dates into an actual date column of ymd format (note this is equivalent to: as.Date(ymd(list_df[[i]]$Date)))
 list_df_Date <- as.Date(list_df[[i]]$Date)
 # Then, we create another variable which is just the date variable (this is identical to the step 2 of creating an xts object)
 assign(paste(list_names[i],sep=''), 
        xts(x = list_df[[i]][,-1], order.by = list_df_Date))
        # Here, we again create the xts objects for each data frame, simply by defining getting all except for the date column and then ordering 
        # them by the respective ymd date. 
 # Lastly, we again need to assign a name to the newly created xts objects. We do so by pasting the respective element from the list_names vector
 # and then assigning this name to the created time-series file
}

# Like that, we performed the time-series transformation for each of the variables and now have seven distinct xts objects named with the elements of list_names. 

# Lastly, we want to create ggplots for each of the files. 

# For that, we define a list again with the xts objects.
list_df_ts <- list(Cum_Ret_MC_Gen_ts, Cum_Ret_MC_Phys_ts, Cum_Ret_MC_Reg_ts, Cum_Ret_MC_Oppty_ts, Cum_Ret_10K_Phys_ts, Cum_Ret_10K_Reg_ts, Cum_Ret_8K_Gen_ts)


# Running the loop to create multiple plots is actually quite straight forward, the only thing we need to adjust the name of each plot
for (i in 1:length(list_df_ts)){
  # Here, R loops through each xts object again
  assign(paste("figure_",list_names[i],sep = ""),
         # Here, we define the name again for each created ggplot
         tidy(list_df_ts[[i]]) %>% 
  # Here, we then assign the name to the ggplot that we created, based on each tidy element of the list_df_ts (our xts objects)
    
  # Below, we just run our usual code to define some good-looking plots!
    
          ggplot(aes(x=index,y=value, color=series)) + 
          geom_line() + 
          # Line, Point and Fill colours
          scale_color_manual(values=c("goldenrod", "darkorchid4", "darkorange2","dodgerblue1", "springgreen2",  "darkorchid4", "dodgerblue4"))  +
          scale_fill_manual(values=c("goldenrod", "darkorchid4", "darkorange2","dodgerblue1", "springgreen2",  "darkorchid4", "dodgerblue4")) +  
          # X and Y axis string
          ylab("Cumulative Returns") + xlab("Time") + 
          # Title string
          ggtitle("Relationship of Cumulative Returns of some SMI Securities") +
          labs(color='Factor Portfolios') +
          theme(
          # Title Elements
          plot.title= element_text(size=14, color="grey26", hjust=0.3,lineheight=0.4, margin=margin(15,0,15,0)), 
          # Axis Elements
          axis.title.y = element_text(color="grey26", size=12, margin=margin(0,10,0,10)),
          axis.title.x = element_text(color="grey26", size=12, margin=margin(10,0,10,0)),
          # Background colour and fill
          panel.background = element_rect(fill="#f7f7f7"),
          plot.background = element_rect(fill="#f7f7f7", color = "#f7f7f7"),
          # Major Panel Grids
          panel.grid.major.x = element_blank(),
          panel.grid.major.y = element_line(size = 0.5, linetype = "solid", color = "grey"),
          # Minor Panel Grids
          panel.grid.minor.x = element_blank(),
          panel.grid.minor.y = element_blank(),
          # Line colour of the x and y axis
          axis.line = element_line(color = "grey")))
}

# Note that we can even further aggregate these things by, for instance, changing the ggtitle accordingly. Try this out yourself :-) 
```

And there we are! This may all be a little much if you have never really worked with such formats before. We do not expect you to draw these things on your own, but we still find it very important to have because exactly these structures can save you potentially a lifetime during your projects. This is especially because these functions are massively scalable. If you want to do this for 10000 data frames, no problem. All you have to do is execute the code and wait until R has done the job (and the best part, if you're getting paid for it you can write the hours it takes R to compute :-) ).  

Now, let's look at one of the outputs:

```{r}
figure_Cum_Ret_MC_Phys_ts
```

Great, we now have seen how to use string adjustments to enforce highly scalable data manipulation techniques! 

#### Matching Patterns with Regular Expressions (RegEx)

Regexps are a very terse language that allow you to describe patterns in strings. They take a little while to get your head around, but once you understand them, you’ll find them extremely useful. To learn regular expressions, we’ll use `str_view()`and `str_view_all()`. These functions take a character vector and a regular expression, and show you how they match. 

### Database Queries

We understand how to retrieve data from local sources. However, most larger datasets are not usually stored somewhere locally, but rather they exist within databases. Databases are remote hubs that allow the user to access data remotely. The main advantage, at least of relational databases, is that you can retrieve much larger datasets from a multitude of sources. 

Operating with databases in R is a great method for working efficiently. This is because you can largely automate any data retrieval process, instead of having to drop-and-drag all the desired variables on the company web pages or Desktop sources. We will show you that we can retrieve thousands of parameters in a short period of time with these algorithms. 

In order to access databases, we need to work with queries. These are methods to access the server where the data is stored through a range of functions. We do so by retrieving a host and, if necessary, using certain credentials to open up a connection. 

The perhaps most known data source that can operate on queries in both Python and R is the WRDS database. As most relational databases, it works with a SQL based syntax, so it's syntax is comparable to most other databases. It serves as a consortium for multiple data providers, such as Compustat, SECM, Reuters or Bloomberg. The handy thing about the WRDS connection is that you can retrieve data from multiple sources simultaneously, thereby enabling an even more efficient use of database queries. 

#### Open the connection to WRDS

We first need to import two packages in order to be able to open a connection. Then, we define a `dbConnect()` function which accesses the WRDS host. We need to define the host, port, database name, as well as the user and password. 

```{r}
# Import the important packages: 
library(remotes)
library(RPostgres)

# Open the connection 
wrds <- dbConnect(Postgres(),
                  host='wrds-pgdata.wharton.upenn.edu',
                  port=9737,
                  dbname='wrds',
                  sslmode='require',
                  user='gostlow',
                  password = "climaterisk8K")
```

#### Data Sources on WRDS

The most important data sources that we can retrieve with the WRDS host are the following: 

* CompuStat:
+ Comp.funda:         Annual Financial Data only for Domestic Firms
+ Comp.fundq:         Quarterly Financial Data only for Domestic Firms
+ Comp.g_funda:       Annual Financial Data only for International Firms
+ Comp.g_fundq:       Quarterly Financial Data only for International Firms
+ Comp.secm:          Monthly stock file

* For CRSP:
+ Crsp.dsf:           Daily stock file
+ Crsp.msf:           Monthly stock file 
+ Crsp.dsi:           Daily market returns
+ Crsp.msi:           Monthly market returns  

* For I/B/E/S:
+ Ibes.actpsum_epsus: Summary history EPS actual and pricing - adjusted
+ Ibes.actpsumu_epsus:Summary history EPS actual and pricing - unadjusted

* For Fama-French Factors:
+ FF.factors_daily:   Daily portfolio four factors
+ FF.factors_monthly: Monthly portfolio four factors

[For a full list, visit this page](https://studyres.com/doc/21204961/common-wrds-datasets-compustat-comp.funda-annual-financial). 

From these four databases, we are likely to retrieve all the data that we need on company financials as well as security data for portfolio management purposes. From each of the respective data sources, we can retrieve specific variables, such as the stock price, total assets or dividends. However, we need to know the names of the respective variables. The easiest way to do this is to access the WRDS Data retrieval homepage and simply source manually for the variables of interest. For instance, when you [open this link and type in valid credentials](https://wrds-www.wharton.upenn.edu/pages/get-data/compustat-capital-iq-standard-poors/compustat/north-america-daily/security-monthly/), you will be redirected to the US Security Monthly database source from Compustat. Therein, you can choose the selections on query variables and then take the shortname of the required variables. These shortnames are used when you access variables either from R or Python (e.g. if you want the Ticker Symbol, the shortname is: TIC). Later, we will show you a full list of potential variables. 

#### Syntax of SQL in WRDS

Note that we won't thoroughly teach you how to write in the SQL way. But note that it's actually quite quick to understand the logic behind it. SQL is a mostly "text-written" language, so, compared to R or Python, the input you have to write to get what you need is more intuitive. 

In general, in order to retrieve data from multiple sources on WRDS, we need the following logic: 

```{r}

# Run the Query manually
res <- dbSendQuery(wrds, "select a.gvkey, a.datadate, a.tic,
                   a.conm, a.at, a.lt, b.prccm, b.cshoq
                   from comp.funda a join comp.secm b
                   on a.gvkey = b.gvkey
                   and a.datadate = b.datadate
                   where a.tic = 'IBM'
                   and a.datafmt = 'STD'
                   and a.consol = 'C'
                   and a.indfmt = 'INDL'")

# Fetch data
data <- dbFetch(res)

# Close database
dbClearResult(res)

# See data
head(data)
```

Therein, we make use of the function `dbSendQuery()`, which effectively accesses and retrieves the data we specified from the WRDS host. The syntax is as follows: 

* `select`
** This command is needed to select all the identifiers and quantitative variables we want to gather (such as Ticker, Company Name, Price etc.)
** We can retrieve different variables from multiple data sources easily
** To symbol SQL which variables belong to which data source, we need to add a specific letter and a dot in front of each variable
*** e.g.: a.gvkey selects one of the identifier variables for companies from the first data source 
*** e.g.: b.prccm selects the share price variable for companies from the second data source
* `from`
** This command tells SQL from which primary data source (if we have multiple) we retrieve the variables
** If we have multiple, we also need to add the same letter as for the variables
*** e.g.: comp.funda a tells that we retrieve all the variables indicated by a. from this data sources
* `join`
** This command is needed if we have MULTIPLE data sources 
** In this case we have comp.funda and comp.secm and retrieve different variables from either source
** We define for both data sources the respective letter such that SQL knows which variables belong to which source 
*** e.g. comp.funda a join comp.secm b then joins the variables from both sources
* `on`
** This command is needed to tell SQL on which variables we join both data sources 
** e.g. here we join them based on identity of the GVKEY company identifier and the Date
* `where`
** This command is needed to tell SQL for which companies to retrieve the data and in what format the data should be displayed 
** Usually, you can leave `datafmt`, `consol` and `indfmt` as they are defined above 
** Then, to define which companies you want, you can select based on four identifiers
*** tic: Ticker of companies
*** gvkey: GVKEY number of companies
*** usip: CUSIP number of companies
*** cik: CIK / SIC number of companies

In essence, the command above means, literally translated, the following: 

* (I) Select the GVKEY identifier, Date, Ticker, Company Name, Total Assets and Total Liabilities from the source Compustat - Annual Fundamentals
* (II) Select the Share Price and the Shares Outstanding from the source Compustat - Securities Monthly 
* (III) Merge both data sources on GVKEY identifier and Date
* (IV) For the company whose ticker is "IBM"
* (V) And display it with the specified data format, consol and keyset format

This is quite handy and allows us to retrieve data with a simple command line. 

#### Automating SQL with a pre-defined function

We now have seen how to retrieve some variables from one company. However, what if we want to fetch hundreds of variables from thousand of companies? In this case, it may be very cumbersome to manually write all the required commands yourself. Rather, it may be smart to have a function in which you only need to enter the respective variables and then let the function do the writing of the query and compiling of the database access. This is what we provide you with below. 

Don't worry, the function looks more complicated than it is. But the main idea behind it is that it creates a query fetch syntax, just as we have seen above, from variables we specify. Like that, instead of manually copy-pasting hundreds of company tickers, we can just define the respective column of a dataframe in which all the tickers are saved and enter this into the function. 

```{r}
# Create a function to automate the data retrieval jobs on SQL
dataset_a = NULL
dataset_b = NULL
dataset_sql = NULL

datafmt = NULL
consol = NULL
indfmt = NULL
sic = NULL
gvkey = NULL
tic= NULL
cusip= NULL
isin = NULL

filters_list= NULL
filters_list_final= NULL
filters_list_tweaked= NULL
filters_list_tweaked_final= NULL

query_sql <-
  function(dataset_a, dataset_b, column_a, column_b, column_sql, start, end, datafmt, consol, indfmt, sic, gvkey, tic, cusip, isin, multi_function = TRUE, reuters_ds = FALSE){
    
    if (reuters_ds == FALSE){
    
      if (!is.null(column_a)) {
      for (i in 1:length(column_a)) {
      column_a[i] = paste("a.", column_a[i], sep = "")
      column_filter_a = paste(column_a, collapse = ',')
      }
    }
    else {
      columns_filter_a = ""
    }
    
    if (!is.null(column_b)) {
      for (i in 1:length(column_b)) {
      column_b[i] = paste("b.", column_b[i], sep = "")
      column_filter_b = paste(column_b, collapse = ',')
      }
    }
    else {
      columns_filter_b = ""
    }
    
    if (!is.null(column_sql)) {
      for (i in 1:length(column_sql)) {
      column_sql[i] = paste("b.", column_sql[i], sep = "")
      column_filter_sql = paste(column_sql, collapse = ',')
      }
    }
    else {
      columns_filter_sql = ""
    }
    
    if (!is.null(start) & !is.null(end)){
      date_filter = paste("a.datadate BETWEEN '", start, "' AND '", end, "'")
    }
    
    sic_filter = NULL
    if (!is.null(sic)) {
      for (i in 1:length(sic)) {
      sic[i] = paste("'", sic[i], "'", sep = "")
      sic_filter = paste("a.sic IN (", paste(sic, collapse = ','), ")")
      }
    }
    
    gvkey_filter = NULL
    if (!is.null(gvkey)) {
      for (i in 1:length(gvkey)) {
      gvkey[i] = paste("'", gvkey[i], "'", sep = "")
      gvkey_filter = paste("a.gvkey IN (", paste(gvkey, collapse = ','), ")")
      }
    }
    
    tic_filter = NULL
    if (!is.null(tic)) {
      for (i in 1:length(tic)) {
      tic[i] = paste("'", tic[i], "'", sep = "")
      tic_filter = paste("a.tic IN (", paste(tic, collapse = ','), ")")
      }
    }
    
    cusip_filter = NULL
    if (!is.null(cusip)) {
      for (i in 1:length(cusip)) {
      cusip[i] = paste("'", cusip[i], "'", sep = "")
      cusip_filter = paste("a.cusip IN (", paste(cusip, collapse = ','), ")")
      }
    }
    
    if (!is.null(datafmt)) {
      for (i in 1:length(datafmt)) {
      datafmt[i] = paste("a.datafmt =  '", datafmt[i], "'", sep = "")
      datafmt_filter = paste(datafmt, collapse = ',')
      
      }
    }
    
    if (!is.null(consol)) {
      for (i in 1:length(consol)) {
      consol[i] = paste("a.consol =  '", consol[i], "'", sep = "")
      consol_filter = paste(consol, collapse = ',')
      
      }
    }
    
    if (!is.null(indfmt)) {
      for (i in 1:length(indfmt)) {
      indfmt[i] = paste("a.indfmt =  '", indfmt[i], "'", sep = "")
      indfmt_filter = paste(indfmt, collapse = ',')
      
      }
    }
    
    filters = c(date_filter, cusip_filter, tic_filter, gvkey_filter, sic_filter, datafmt_filter, consol_filter, indfmt_filter)
    
    for (i in 1:length(filters)){
      if (!is.null(filters[i])){
        filters_list[i] = paste(filters[i], sep = "")
        filters_list_final = paste(" WHERE ", paste(filters_list, collapse = " AND "))
      }
    }
  
    filters_tweaked = c(date_filter, cusip_filter, tic_filter, gvkey_filter, sic_filter)
    
    if (!is.null(filters_tweaked[i])){
       for (i in 1:length(filters_tweaked)){
        filters_list_tweaked[i] = paste(filters_tweaked[i], sep = "")
        filters_list_tweaked_final = paste(" WHERE ", paste(filters_list_tweaked, collapse = " AND "))
      }
    }
    
    if (multi_function == TRUE){
      sql = (paste("SELECT ", 
                   column_filter_a, 
                   ", ", column_filter_b,
                   " FROM  ", dataset_a, " a",
                   " inner join ", dataset_b, " b", 
                   " on ", column_a[1], " = ", column_sql[1], 
                   " and ", column_a[2], " = ", column_sql[2], 
                   " and ", column_a[3], " = ", column_sql[3],
                   filters_list_final))
    } 
    
    else {
      sql = (paste("SELECT ", 
                   column_filter_a, 
                   " FROM  ", dataset_a, " a",
                   filters_list_tweaked_final))
    }
      
  }
    
  else {
      if (!is.null(column_a)) {
        for (i in 1:length(column_a)) {
        column_a[i] = paste("a.", column_a[i], sep = "")
        column_filter_a = paste(column_a, collapse = ',')
        }
      }
      else {
        columns_filter_a = ""
      }
      
      if (!is.null(column_b)) {
        for (i in 1:length(column_b)) {
        column_b[i] = paste("b.", column_b[i], sep = "")
        column_filter_b = paste(column_b, collapse = ',')
        }
      }
      else {
        columns_filter_b = ""
      }
      
      if (!is.null(column_sql)) {
        for (i in 1:length(column_sql)) {
        column_sql[i] = paste("b.", column_sql[i], sep = "")
        column_filter_sql = paste(column_sql, collapse = ',')
        }
      }
      else {
        columns_filter_sql = ""
      }
      
      if (!is.null(start) & !is.null(end)){
        date_filter = paste("a.year_ BETWEEN '", start, "' AND '", end, "'")
      }
      
      sic_filter = NULL
      if (!is.null(sic)) {
        for (i in 1:length(sic)) {
        sic[i] = paste("'", sic[i], "'", sep = "")
        sic_filter = paste("a.sic IN (", paste(sic, collapse = ','), ")")
        }
      }
      
      gvkey_filter = NULL
      if (!is.null(gvkey)) {
        for (i in 1:length(gvkey)) {
        gvkey[i] = paste("'", gvkey[i], "'", sep = "")
        gvkey_filter = paste("a.gvkey IN (", paste(gvkey, collapse = ','), ")")
        }
      }
      
      tic_filter = NULL
      if (!is.null(tic)) {
        for (i in 1:length(tic)) {
        tic[i] = paste("'", tic[i], "'", sep = "")
        tic_filter = paste("a.item5601 IN (", paste(tic, collapse = ','), ")")
        }
      }
      
      cusip_filter = NULL
      if (!is.null(cusip)) {
        for (i in 1:length(cusip)) {
        cusip[i] = paste("'", cusip[i], "'", sep = "")
        cusip_filter = paste("a.item6004 IN (", paste(cusip, collapse = ','), ")")
        }
      }
      
      isin_filter = NULL
      if (!is.null(isin)) {
        for (i in 1:length(isin)) {
        isin[i] = paste("'", isin[i], "'", sep = "")
        isin_filter = paste("a.item6008 IN (", paste(isin, collapse = ','), ")")
        }
      }
      
      
      filters = c(date_filter, cusip_filter, tic_filter, gvkey_filter, sic_filter, isin_filter)
      
      for (i in 1:length(filters)){
        if (!is.null(filters[i])){
          filters_list[i] = paste(filters[i], sep = "")
          filters_list_final = paste(" WHERE ", paste(filters_list, collapse = " AND "))
        }
      }
    
      filters_tweaked = c(date_filter, cusip_filter, tic_filter, gvkey_filter, sic_filter, isin_filter)
      
      if (!is.null(filters_tweaked[i])){
         for (i in 1:length(filters_tweaked)){
          filters_list_tweaked[i] = paste(filters_tweaked[i], sep = "")
          filters_list_tweaked_final = paste(" WHERE ", paste(filters_list_tweaked, collapse = " AND "))
        }
      }
      
      if (multi_function == TRUE){
        sql = (paste("SELECT ", 
                     column_filter_a, 
                     ", ", column_filter_b,
                     " FROM  ", dataset_a, " a",
                     " inner join ", dataset_b, " b", 
                     " on ", column_a[1], " = ", column_sql[1], 
                     " and ", column_a[2], " = ", column_sql[2], 
                     " and ", column_a[3], " = ", column_sql[3],
                     filters_list_final))
      } 
      
      else {
        sql = (paste("SELECT ", 
                     column_filter_a, 
                     " FROM  ", dataset_a, " a",
                     filters_list_tweaked_final))
      }
    }
      
  }
```


Let's see how this works. In the next text block, we define the function inputs. As we see, the function requires the following arguments (or variables):

* dataset_a: Defines the first data source (e.g. comp.funda)
* dataset_b Defines the second data source (e.g. comp.secm)
* column_a: Defines the list of variables we want to retrieve from the first data source
* column_b: Defines the list of variables we want to retrieve from the second data source
* column_sql: Defines on which variables we want to merge both data sources
* start: Start date of the retrieval
* end: End data of the retrieval
* datafmt: Data Format (usually "STD") 
* consol: Consol (usually "C")
* indfmt: Key Format (usually "INDL") 
* sic: Potential Company Identifier
* gvkey: Potential Company Identifier
* tic: Potential Company Identifier
* cusip: Potential Company Identifier
* multi_function: Logical argument idicating if we draw data from multiple source or only one source. Default: TRUE
* reuters_ds: Logical argument indicating if we take data from Reuters Datastream or Compustat/CRSP. Default: FALSE

Note that if we want to take data from Reuters, we need different input parameters as when we take data from CRSP / Compustat. As such, we need to change both input parameters for date and company variables. This will be shown in a later chapter. 

Great, now we can just define lists for the variables within column_a, column_b and column_sql, enter which sources we want to retrieve the data from in dataset_a and dataset_b and which identifier to use (here I used a list of unique gvkeys for over 5'000 companies). Note that, by default, all arguments are NULL and thus, if you don't want to change some of the variables you can just leave them with the identity command (e.g. sic = sic, etc.)

What this does now is it creates a string-based query input identical to the one above. We can directly take this query input and send it to the database through `dbSendQuery(wrds, query)` (whereas query represents the fetching command we wrote manually before). 

```{r}

# This is the first database, used for the keys from the fundamentals annually list
column_a = list('gvkey', 'iid', 'datadate', 'cik', 'conm', 'tic', 'cusip', 'exchg',
             'atq', 'actq', 'ltq', 'lctq', 'ceqq', 'teqq', 'revtq', 'cogsq', 'xintq', 'xsgaq', 'ugiq', 'niq', 'epspxq', 'dvpspq')

# This is to connect the both databases without duplicating their output in the end
column_sql = list('gvkey', 'iid', 'datadate')

# This is the second database, used for the keys from the securities monthly list
column_b = list('secstat', 'cshtrm')

# This is the query output. It is a string that replicates what one would write in the SQL server to obtain data from a (multiple) source(s)
gvkey <- read.csv("~/Desktop/Empirical Asset Management/Data/SQL_Query_Check/calls_us_climate_final_df.csv", header = T)[,-1]
gvkey = unique(gvkey)

# Get the quarterly data on company financials
query = query_sql(dataset_a = "comp.fundq",
                  dataset_b = "comp.secm",
                  multi_function = TRUE,
                  column_a = column_a,
                  column_b = column_b,
                  column_sql = column_sql,
                  gvkey = gvkey,
                  sic = sic,
                  tic = tic,
                  cusip = cusip, 
                  datafmt = 'STD', 
                  consol = 'C', 
                  indfmt = 'INDL',
                  start = '2000-01-01',
                  end = '2010-01-01') 

res <- dbSendQuery(wrds, query)

# Fetch data
data <- dbFetch(res)
# See data
colnames(data) <- c('GVKEY', 'IID', 'Date_t', 'CIK', 'Comp_name', 'Ticker', 'CUSIP', 'Exchange_Code',
                         'Total_Assets_t', 'Current_Assets_t', 'Total_Liab_t', 'Current_Liab_t', 'Common_Equity_t',
                         'Shareholder_Equity_t', 'Revenue_Tot_t', 'COGS_t', 'Interest_Exp_t', 'SGA_t', 'Gross_Income_t', 'Net_Income_t',
                         'EPS_t', 'DPS_t', 'Security_Status', "Traded_Shares")
head(data)
```

This is great. In under a minute we were able to retrieve over 36'000 rows and 24 columns of data. This is definitively quicker than doing this via drag-and-drop (btw: now you know why GS Investment Banking Interns work 100+ hours - b/c they don't know these commands :-) ). 

#### Upsample Data

We now retrieved much of the data we need. But, unfortunately, most of company fundamentals data is only available in quarterly or annual format (this makes sense as they only report in this periodicity). However, most of the security data (returns, dividends, share amounts, OHLC) is given in much higher intervals. Now, if we want to combine both data sources, we need to sample them both onto the same periodicity. 

Throughout the lecture, we mostly work with monthly data. As such, in order to be able to compare fundamentals with security data, we should upsample the periodicity of the fundamentals. In principle, this means that we need to duplicate the rows within a given period. For instance, if we have quarterly data and transform it to monthly data, we need to replicate each quarter row twice to get three identical rows for quarterly financials of a given company per quarter. Although this does not necessarily add any new information, this will make it extremely handy to use the data for later analysis (especially for factor creation). 

```{r}
# Create the upsampled time-series to obtain monthly intervals (from quarterly)

## First, delete rows that SQL incorrectly retrieved multiple times
data_unique <- data %>% distinct(GVKEY, Date_t, .keep_all = T) 
## Define, for each company and quarter, the last date of the quarter (e.g. if your date is 2000-03-31, then Last_Date_Q_t = 2000-06-30)
data_unique$Last_Date_Q_t <- data_unique$Date_t %m+% months(2)
## Define the interval, in months, between both the first and last date of each quarter and add 1 (to get the absolute period interval)
data_unique$Interval_Date_Q <- (interval(data_unique$Date_t, data_unique$Last_Date_Q_t) %/% months(1)) + 1


# Tidy command to create duplicated observations for each month (going from quarterly to monthly observations)
data_monthly <- expandRows(data_unique, "Interval_Date_Q", drop = FALSE) %>% arrange(GVKEY, Date_t) %>% 
group_by(GVKEY, Date_t, Last_Date_Q_t)  %>% 
  mutate(Date_Monthly = seq(as.Date(ceiling_date(first(Date_t), unit = "month")),
                  as.Date(ceiling_date(first(Last_Date_Q_t) , "month")),
                  by = "months") - days(1)) %>% ungroup()

# This command is a little nasty. So nasty, that it even looks complicated with a tidy command. But fear not, it can be defined accordingly: 

## expandRows: This expands or duplicates each row based on the Interval variable. That is, if the interval = 3, then we get three duplicated rows per observation. 

## arrange: This is needed such that tidy understands how to sort each expansion 

## group_by: Here, we then group by GVKEY First Date and Last Date of the Quarter to tell R that it should execute the following commands for each group individually (as such, no duplication of the commands can be ensured)

## mutate: Here, we create the new date variables per group. This basically then creates for each group defined a variable that the last day of each month for the specific quarter as a Date. We do this in the following fashion:

### seq(): We first enter for each quarter-company group a sequence, starting with the first day of the first month of each quarter and ending with the first day of the last month of the next quarter. By defines the interval fromt the first to the last observation  

### unit = "month" defines that we want to have the specific date in a month format (e.g. 2000-03-31)

### ceiling_date(): This command ensure that we get the first day of the respective month

### days(1) We then subtract from the respective dates one day, by doing so, we get the last days of each month for each quarter (we want the last days as, in monthly intervals of financial analysis, the dates are always w.r.t the last day of each month)

# Like this, we create a column which includes, for each quarter-company combination, the three end dates of the respective quarter. For instance, for the quarter between 2000-03-31 and 2000-06-30, we get. 2000-03-31, 2000-04-30 and 2000-05-31. Like that, we successfully duplicated each company-quarter analysis three times and assigned the correct end-of-month dates to each of the rows. 

# That is, we finally created a monthly upsampled dataset from quarterly observations. Halleluja (this actually took an entire day to code so don't worry if you don't get it from the start). 

# Lastly, we can create a Time-Series Object :
data_monthly <- data_monthly %>% select(-c(Date_t, Interval_Date_Q, Last_Date_Q_t)) 
data_monthly_ts <- xts(data_monthly[, -which(names(data_monthly) == "Date_Monthly")] , order.by = as.Date(as.POSIXct(data_monthly$Date_Monthly)))
```

Now, we have upsampled the quarterly fundamentals data set. We may now take data on securities and combine it with the fundamentals data. 

```{r}

# Get the monthly data on share prices 
column_a = list('gvkey', 'iid', 'datadate', 'cik', 'conm', 'tic', 'cusip', 'prccm', 'cshom', 'cshtrm', 'trt1m', 'dvrate')

# Note, here we say multi_function = FALSE as we only retrieve data from one source. Note that if we use it from only one source, we nevertheless need to type in an entry for dataset_b and column_b. 

query = query_sql(dataset_a = "comp.secm",
                  dataset_b = "comp.secm",
                  multi_function = FALSE,
                  column_a = column_a,
                  column_b = column_b,
                  column_sql = column_sql,
                  gvkey = gvkey,
                  sic = sic,
                  tic = tic,
                  cusip = cusip, 
                  datafmt = 'STD', 
                  consol = 'C', 
                  indfmt = 'INDL',
                  start = '2000-01-01',
                  end = '2010-01-01')

res_shares <- dbSendQuery(wrds, query)

data_shares <- dbFetch(res_shares)


# Here, we define again the names for each variable
colnames(data_shares) <- c('GVKEY', 'IID_check', 'Date_t', 'CIK_check', 'Comp_name', 'Ticker', 'CUSIP', 'Closing_Price_t', 'Shares_Outst_t', 'Trading_Volume_t', 'HRT_t', 'Div_Rate_t')

# Transform to Time-Series Object
data_shares_unique <- data_shares %>% distinct(GVKEY, Date_t, .keep_all = T) 
data_shares_monthly_ts <- xts(data_shares_unique[, -which(names(data_shares_unique) == "Date_t")] , order.by = as.Date(as.POSIXct(data_shares_unique$Date_t)))

# Close database
dbClearResult(res)
```

Now, we can merge both dataframes

```{r}
# Merge both dataframes and convert to one Time-Series Object

## Change variable name of data_monthly Date
data_monthly$Date_t <- data_monthly$Date_Monthly
data_monthly$Date_Monthly <- NULL

## Create a merged object
data_final_monthly <- merge(data_monthly, data_shares_unique, by=c("GVKEY", "Date_t", "CUSIP", "Comp_name", "Ticker"))

## Create a Time-Series Object
data_final_monthly_ts <- xts(data_final_monthly[, -which(names(data_final_monthly) == "Date_t")] , order.by = as.Date(as.POSIXct(data_final_monthly$Date_t)))

```

This is just great! Now, we are able to retrieve tons of data quickly from multiple sources, change periodicity to the same levels and combine the data to get one large dataframe in which we can conduct all the operations for factor and variable creation we need. 

# Statistical Properties 

The first topic covers mathematical and statistical properties that most modern finance is built upon. These properties serve as cornerstones required to comprehend the foundations of financial economics, risk management as well as asset management. ALthough widely used in many areas, their comprehension is key in identifying and retracing financial concepts in any of the afore-mentioned areas. 

In this chapter, we will cover the mathematical foundations and statistical properties that are often used within empirical finance contexts. Based upon the topics taught in Statistics and Mathematics courses as well as Empirical Methods, we will cover properties related to $\textbf{Probability Theory}$ and $\textbf{Matrix Algebra}$. 

This section should serve as a repetition to the topics already discusses in Empirical Methods and as such students should at least be familiar with the subjects at hand. However, as the course will rely substantially on these properties, a coherent discussion of them is a necessary prerequisite to be able to expect the baselines of empirical finance. 

## Random Variables and Probability Distributions: Introdcution

In this chapter, we will repeat the fundamentals of probability as well as probability distributions. We dive into what random variables are, how they are used in financial applications, how they can be related to probability measures, such as distributions, what the distributions tell us, how they can be related to financial concepts and how they can be calculated using R. 

The chapter is outlined as follows: Section 1 introduces the concept of random variables within a countable space. Next, in Section 2, we look at discrete probability distributions, such as Bernoulli, Poisson or Mutlinomials. In Section 3 we look at continuous probability distributions, before we dive into continuous distributions with appealing properties, such as normal or log-normal distributions. Lastly, we will look at continuous functions that can deal with extreme events. 

### The concept of Probability: Important Notions and Definitions

To understand the concept of random variables more thoroughly, we need to define some concepts first. The concepts we discuss are \textit{outcomes, spaces, events, $\sigma$-algebra and probability measures}.  

For that, let's use a dice throwing example. A dice can take up six values when being rolled, ranging from 1 to 6, with, theoretical, probability of 1/6 for each outcome. 

#### Outcomes, Spaces and Events, Measurable and Immeasurable Spaces

$\textbf{Definition 2.1: Outcome}$

Outcomes are just all possible, or feasible, values that a certain experiment can render. It is denoted by $\omega$. In the case of throwing a dice, this is just all numbers that the dice can show, e.g. 1 to 6. We write this accordingly as:

$$
\omega_1 = 1, \omega_2 = 2, \omega_3 = 3, \omega_4 = 4, \omega_5 = 5, \omega_6 = 6
$$

$\textbf{Definition 2.2: Space}$

The set of all feasible outcomes is called space. It is denoted by $\Omega$. In a dice experiment, this is just all values of $\omega$ defined previously. We write this as: 

$$
\Omega = [\omega_1, \omega_2, \omega_3, \omega_4, \omega_5, \omega_6]
$$

\textbf{Definition 2.3: Subsets and Power Sets}

Each Space $\Omega$ can be distributed into certain parts. For instance, in the dice example we can be interested in whether the rolled number is odd or even, defining a set of either all odd or all even numbers. 

In general, the \textbf{Power Set} $2^\Omega$ comprises of all possible subsets of a space $\Omega$, including the \textit{empty space } 
$\emptyset$ and the space set $\Omega$. With the aid of this power set, we are able to describe \textbf{all possible events}.

Another neat property of the power set is that it includes each union of arbitrarily many events as well as any intersection of arbitrarily many events. The power set also contains the complements to all events.

\textbf{Definition 2.4: }$ $\sigma$-$\textbf{algebra}

The $\sigma$-algebra, denoted as $\mathbb{A}$, is the collection of events that are subsets of $\Omega$ with the following properties: 

$$
\text{(I) } \Omega \in \mathbb{A} \text{ and } \emptyset \in \mathbb{A}\\
\text{(II) }\text{if event } E \in \mathbb{A} \text{ then }\hat{E} \in \mathbb{A} \\
\text{(III) }\text{If the countable sequence of events } E_1, \dots, E_n \in \mathbb{A}, \text{ then } \cup^\infty_{i=1}E_i \in \mathbb{A} \text{ and } \cap^\infty_{i=1}E_i \in \mathbb{A}
$$

Which defines that (I) both the space and the empty set, (II) the complements of any event and (III) both the intersection as well as the union of any event(s) are included. 

In the case of the dice rolling experiment, this would include all potential values as well as their intersections, combinations and complements. 

\textbf{Definition 2.5: Borel }$\sigma$-\textbf{algebra}

The Borel $\sigma$-algebra is mostly used in uncountable spaces. That is, where $\Omega$ is no longer finite, or countable, implying we have uncountably many potential outcomes. Suppose that we are analyzing the daily logarithmic returns for a common stock or common stock index. Theoretically, any real number is a feasible outcome for a particular day’s return, although we might expect some capping above and below certain values. So, events are characterized by singular values as well as closed or open intervals, such as being interested if the return is at least 10 percent, and each potential outcome in the real space. 

To design our set of events of the uncountable space $\Omega$, we take the following approach. 

We first (I) include “any real number,” which is the space itself, $\Omega$, as well as the empty space $\emptyset$. Next, one includes (II) all events of the form “less than or equal to a”, for any real number a. Accordingly, we consider all possible half-open intervals given by $(-\infty, a]$ for any a $\in \mathbb{R}$. For each of these half-open intervals, we then add (III) its complement $(a, \infty)$  which expresses the event “greater than a.” Lastly, we include (IV) all possible unions and intersections of everything already in the set of events as well as (V) of the resulting unions and intersections themselves.

IN total, the Borel $\sigma$-algebra consists of all these sets, intersections, unions and complements for an immeasurable space. It is denoted by $\mathbb{B}$. 

#### Probability Measure

There are some formal definitions a probability measure needs to satisfy: 

\textit{Property 1}: A probability measure should assign each event E from our $\sigma$-algebra a nonnegative value corresponding to the chance of this event occurring.\\
\textit{Property 2}: The chance that the empty set occurs should be zero since, by definition, it is the improbable event of “no value.”\\
\textit{Property 3}: The event that “any value” might occur (i.e., 1) should be 1 or, equivalently, 100% since some outcome has to be observable.\\
\textit{Property 4}: If we have two or more events that have nothing to do with one another that are pairwise disjoint or \textbf{mutually exclusive}, and create a new event by \textbf{uniting them}, the \textbf{probability of the resulting union} should equal the \textbf{sum of the probabilities of the individual events}.

More formally, this means: 

$$
\text{(I) } P(\emptyset) = 0\\
\text{(II) } P(\Omega) = 1\\
\text{(III) } \text{ For a countable sequence of events } E_1, \dots, E_n \in \mathbb{A} \\\text{ that are mutually exclusive  we have that } P(\cup_{i=1}^\infty E_i = \scriptstyle\sum_{i=1}^\infty \textstyle P(E_i))
$$

#### Modelling Randomness and Chance: The Probability Space

Now, we defined all individual constituents needed to model randomness and chance. By understanding what the space, $\Omega$, the subsets of events with certain properties, $\sigma$-algebra, and the probability measure, P, we defined the triplet {$\Omega$, $\sigma$-algebra, P} that forms the so called \textbf{probability space}.

#### Modelling Randomness and Chance: Probability Measure in Countable and Uncountable Spaces

Understanding the differences of P in cases of countability vs. uncountability is key in understanding the important implications for the \textbf{determination of the probability of certain events}. 

Suppose first a countable space $\Omega$. Here, the probability of any event E in the $\sigma$-algebra $\mathbb{A}$ can be computed by adding the probabilities of all outcomes associated with E. That is: 

$$
P(E) = \scriptstyle\sum_{\omega_i \in E}\textstyle p(i)
$$

where $P(\Omega) = 1$. 

In case of the dice rolling experiment, each outcome is associated with a probability of 1/6, formally: 

$$
p(\omega_i) = 1/6
$$

Now, suppose we are in an uncountably large space, given by $\Omega = \mathbb{R}$. Here, the $\sigma$-algebra is given by the Borel $\sigma$-algebra $\mathbb{B}$ and we can no longer pin the probability of the events E in the space down by simply following the same approach as before. 

Doing so, we require the use of the \textbf{distribution function of the probability measure P}: 

$\textbf{Definition 2.6: Distribution function of P}$

A function F is a distribution function of the probability measure P if it satisfies the following properties:

\textit{Property 1}: F is right-continuous\\
\textit{Property 2}: F is nondecreasing\\
\textit{Property 3}: $\lim_{x\rightarrow -\infty} = 0$ and  $\lim_{x\rightarrow \infty} = 1$\\
\textit{Property 4}: For any $x \in \mathbb{R}$, we have $F(x) = P((-\infty, x])$

This is exactly the foundation of the probability distributions we use in statistics and how to calculate each probability therein. 

Because, it follows that, for any interval (x,y], we compute the \textbf{associated probability} according to: 

$$
F(y) - F(x) = P((x,y])
$$

So, in this case we have a function F uniquely related to P from which we derive the probability of any event in $\mathbb{B}$. 

Now, if we understand or define which distribution a variable follows, we can pin down the area under the distribution to understand the probability of certain events. 

To illustrate, the probability of the S&P 500 log return being between -1% and 1% is: 

$$
F(0.01) - F(-0.01) = P((–0.01,0.01])
$$

Whereas F is the function given by the probability distribution related to the probability measure, which consists of the space, all sub-spaces (Borel) as well as the probability measure, P. 

### Random Variables

When we refer to some quantity as being a random variable, we want to express that its value is subject to uncertainty, or randomness. Strictly speaking, any random variable of interest is called stochatic. This is in contrast to a deterministic quantity whose value is determined with absolute certainty. As opposed to this, the random variable value is unknown until an outcome of an experiment is observable. 

A straight-forward way to think about a random variable is the following. Suppose we have a random experiment where some outcome $\omega$ from the space $\Omega$ occurs. Depending on this value, the random variables takes some value $X(\omega) = x$ where $\omega$ is an input to X. What we observe, finally, is the value x, which is only a consequence of the outcome $\omega$ of the underlying random experiment. 

Consequently, a random variable is a function that is completely deterministic and depends on the outcome $\omega$ of some experiment. As such, we understand random variables as \textbf{functions from some space into an image or state space}. 

Mostly, we define random variables as measurable function. 

\textbf{Definition 2.7: Measurable functions}

Let {$\Omega$, $\mathbb{A}$} and {$\Omega'$, $\mathbb{A}'$} be two measurable spaces and their corresponding $\sigma$-algebrae, respectively. Then, a function $X: \Omega \rightarrow \Omega'$ is $\mathbb{A}-\mathbb{A}'$-measurable if, for any set $E' \in \mathbb{A}'$, we have:

$$
X^{-1}(E') \in \mathbb{A}
$$

In words, a function from one space to another is measurable if 

$$
\text{(I)  you can map outcomes } \omega \text{ from } \Omega \text{ with values } X(\omega) = x \text{ in } \Omega'\\
\text{(II)  you can map events } E^{-1} \text{ in the state space back with } \sigma-\text{algebra }, \\\mathbb{A'}, \text{ to the corresponding origin of } E^{-1} \text{ in } \sigma-\text{algebra } \mathbb{A} \text{ of the original probability space}   
$$

In essence, for each for each event in the state space $\sigma$-algebra, $\mathbb{A'}$, we have a corresponding event in the $\sigma$-algebra of the domain space, $\mathbb{A}$. 

#### Discrete and Continuous Random Variables 

Discrete Random Variables are variables that can take up a limited, or countably large, number of outcomes, $\omega$, such that $\omega \in {\omega_1, \dots, \omega_n}$. As such, with discrete random variables, we are in a countable origin space. 

As opposed to this, a Continuous Random Variable is a variable that can take on any real number value. That is, we understand that $\omega \in \mathbb{R}$. Based on the definitions from earlier, we are in an infinite, or uncountable, origin space. 

### Discrete Random Variables and Distributions

We now consider the random variables in countably finite spaces and their distributions. The random variables on the countable space will be referred to as discrete random variables.

#### Random Variables in the countable space

In cases of discrete random variables, the corresponding probability distribution function (PDF) is denoted as p(x)

$$
p(x) = \sum_{w_i \in E} p_i
$$

whereas $p_i$ is the probability of the individual outcome $\omega_i$ in E. 

Let's quickly assume and recreate a potential discrete distribution. For that, let's assume return based probabilities we generate:

```{r check chunk}
ret = c(-0.2, 0, 0.15, 0.35, 0.7)
probabs = c(0.05, 0.14, 0.46, 0.25, 0.1)
plot(ret, probabs, lwd=4, xlab="Return", 
     ylab="Probability", xaxt="n")
axis(1, at=ret)
```

#### Bernoulli Distribution

Suppose, we have a random variable X with two possible outcomes. As such, the state space is $\Omega' \in {x_1, x_2}$. 

In general, the Bernoulli distribution is associated with random variables that assume the values $x_1 = 1$ and $x_2 = 0$. The distribution of X is given by the probability for the two outcomes, that is:

$$
p(X = 1) = p_1 = \pi\\
p(X=0) = p_2 = (1-\pi)
$$
 
Having both the probability and values, we can describe the model as:

$$
p(x) = \pi^x(1-\pi)^{1-x} 
$$

Consequently, the mean of the Bernoulli Distribution is: 

$$
0*(1-\pi) + 1*\pi = \pi
$$

And its variance is gien by:

$$
(1-p)^2p+(0-p)^2(1-p) = p(1-p)
$$

#### Binomial Distribution

A binomial distribution is basically n linked single Bernoulli trials. In other words, we perform a random experiment with n “independent” and identically distributed Bernoulli random variables, which we denote by B(p). 

\textbf{Definition 2.8: Independently and Identically Distributed}

We just assumed Independence and Identical Distribution. This is also known as "IID" assumption. Although we do not cover this in detail, it's important to understand that \textbf{Independence} means that the outcome of a certain item does not influence the outcome of any others. By \textbf{identical distribution} we mean that the two random variables’ distributions are the same.

This experiment is as if one draws an item from a bin and replaces it into the bin before drawing the next item. As such, we speak of \textit{drawing with replacement}. 

In general,  a binomial random variable X counts the number of "successes" in n repeated Bernoulli trials, denoted as $X \sim B(n,\pi)$. To define the probability of X being equal to k, we need to define two concepts. 

The first determines many different samples of size n are there to yield a i realizations of the outcome. It is called as the \textbf{Binomial coefficient} and is given by:

$$
\begin{pmatrix}
n \\
k
\end{pmatrix} = 
\frac{n!}{(n-k)!k!}
$$

The second defines the probability measure. Since in each sample the n individual B(p) distributed items are drawn independently, the probability of the sum over these n items is the product of the probabilities of the outcomes of the individual items, given by:

$$
\pi^k(1-\pi)^{n-k}
$$

Combined, we obtain the probability under a Binomial Distribution, as product of both terms:

$$
P(x = k) = \begin{pmatrix}
n \\
k
\end{pmatrix}
\pi^k(1-\pi)^{n-k}
$$

The mean of a Binomial random variable is:

$$
E(x) = np
$$

and its variance is:

$$
var(x) = np(1− p)
$$

\textbf{Application into Financial Markets perspectives}

We can easily extend this idea to financial applications. Let's assume that in each period the stock price can either increase or decrease by i = 10%. Here, probability of increase is given by 0.6 and probability of decline by 0.4. 

We start with an initial price of 20. According to this outcome, the stock price in t+1 will either be 20(1+0.1) = 22 or 20(1-0.1) = 18. In the third period, the stock price will further deviate according to the same principle and thus we will obtain: 

$$
22*1.1 = 24.20\\
22*0.9 = 19.80\\
18*1.1 = 19.80\\
18*0.9 = 16.20
$$

At t=2, we obtain a new state space, $\Omega'$, consisting of {16.2, 19.8, 24.2}. In that case, the probability distribution of $S_2$ is given as follows:

$$
P(S_2 = 24.20) = \begin{pmatrix} 2 \\ 2 \end{pmatrix}\pi^2(1-\pi)^0 = 0.6^2 = 0.36 \\
P(S_2 = 19.80) = \begin{pmatrix} 2 \\ 1 \end{pmatrix}\pi^1(1-\pi)^{2-1} = 0.48 \\
P(S_2 = 16.20) = \begin{pmatrix} 2 \\ 0 \end{pmatrix}\pi^0(1-\pi)^{2-0} = 0.4^2 = 0.16
$$

To get the respective stock returns in t=2, we can use the formula:

$$
S_2 = S_0*1.1^n*0.9^{n-k}
$$

$$
S_t = S_0*(1+i)^k*(1-i)^{n-k}
$$

#### Multinomial Distribution

A multinomial distribution follows the same concept as a binomial distribution, with the difference that the outcomes are more than 2. In general cases, we follow n outcomes. Formally, we have that $x = {x_1, \dots, x_n}$. Whereas the respective probabilities are denoted as $p(x) = {p(x_1), \dots, p(x_n)}$. 

As with the Binomial Distribution, we have two distinct components. The first is the \textbf{Multinomial Coefficient} and it is given by: 

$$
\begin{pmatrix}
& & n\\
n_1 & n_2 & n_3 & \dots & n_k
\end{pmatrix}
$$

The second term is again the probability of each event occurring. However, we can no longer find the complement(s), as only one probability of events can be expressed by the others. Thus, we just work with occurrences:

$$
\pi_1^{n_1} * \pi_2^{n_2} * \pi_3^{n_3} * \dots * \pi_k^{n_k}
$$

Together, we obtain the Multinomial probability for a given event:

$$
P(x_1 = n_1, x_2 = n_2, x_3 = n_3, \dots, x_k = n_k) = \begin{pmatrix}
& & n\\
n_1 & n_2 & n_3 & \dots & n_k
\end{pmatrix}
\pi_1^{n_1} * \pi_2^{n_2} * \pi_3^{n_3} * \dots * \pi_k^{n_k}
$$

Here, the respective Expected Value is:

$$
E(x_k) = p_k*n
$$

and the correspoding Variance:

$$
var(x_k) = p_k*(1-p_k)*n
$$

\textbf{Application into Financial Markets perspectives}

We can easily replicate the ideas formed in the stock price movements to multinomial perspectives. For that, let's assume that we have now three distinct outcomes. That is, the stock can either increase by 10%, stay the same or decline by 10%. As such, we define the respective movements as $Y_u = 1.1, Y_s = 1.0, Y_d = 0.9$. The respective probabilities are said to be $p_u = 0.25, p_s = 0.5, p_d = 0.25$. 

Our new state space consists of six possible outcomes: 

$$
\Omega ' = [(u,s,d)] = [(2,0,0), (0,2,0), (0,0,2), (1,1,0), (1,0,1), (0,1,1)]
$$

And the corresponding prices are: 

$$
S_2 = S_0*p_u^{n_u}*p_s^{n_s}*p_d^{n_d} \in [16.2, 18, 19.8, 20, 22, 24.2]
$$

These are the multinomial coefficients we use for calculation of the probability for x being equal to some value. Consequently, we get the following probabilities:

$$
P(S = 24.4) = \begin{pmatrix} & 2 \\ 2 & 0 & 0\end{pmatrix}p_up_u = 0.0625 \\
P(S = 22) = \begin{pmatrix} & 2 \\ 1 & 1 & 0\end{pmatrix}p_up_s = 0.25 \\
P(S = 20) = \begin{pmatrix} & 2 \\ 0 & 2 & 0\end{pmatrix}p_sp_s = 0.25 \\
P(S = 19.8) = \begin{pmatrix} & 2 \\ 1 & 0 & 1\end{pmatrix}p_up_d = 0.125 \\
P(S = 18) = \begin{pmatrix} & 2 \\  0 & 1 & 1 \end{pmatrix}p_sp_d = 0.25 \\
P(S = 16.2) = \begin{pmatrix} & 2 \\ 0 & 0 & 2 \end{pmatrix}p_dp_d = 0.0625 \\
$$

### Continuous Random Variables and Distributions

As previously mentioned, within the scope of continuous distributions, we no longer have a countable space $\Omega$ we can rely on. That is, the different outcomes, $\omega$ are uncountable. Technically, without limitations caused by rounding to a certain number of digits, we could imagine that any real number could provide a feasible outcome, thereby the subsets is given by the Borel $\sigma$-algebra, $\mathbb{B}$, which is based on all half-open intervals from $(-\infty, a]$ for any $a \in \mathbb{R}$. 

As the space set is uncountable, we need a unique way to assign a probability to a certain event. Recall that, as just described, the subsets in an uncountable space are given by all half-open intervals from $(-\infty, a]$. We can make use of this property by introducing a \textbf{Continuous Distribution Function, F(a)} which expresses the \textbf{probability that some event of the sort} $(-\infty, a]$ occurs. That is, the probability that a \textbf{number is realised that is at most a}. In said case, F(a) states the \textbf{probability that some outcome is at most a}.

To be a little more concise, we assume that the Continuous Distribution Function, F(a), has the following properties: 

\textit{Property 1:} $\lim_{x \rightarrow -\infty} \rightarrow 0$ \\
\textit{Property 2:} $\lim_{x \rightarrow \infty} \rightarrow 1$ \\
\textit{Property 3:} $F(b) - F(a) \geq 0 for b \geq a$ \\
\textit{Property 4:} $\lim_{x \downarrow a} F(x) = F(a)$

These Properties state (I) Behaviour in Extremes (II) Monotonically Increasing behaviour (III) Right-Continuity 

As the set of events in real numbers are uncountably many, pinning down an exact number is zero. As such, we generally assign probabilities in the following way: 

$$
P((a,b)) = F(b) - F(a)
$$

Whereas $F(b) = P((-\infty, b]))$ and $F(a) = P((-\infty, a]))$. That is, the entire probability that an outcome of at most a occurs is subtracted from the greater event that an outcome of at most b occurs, implying: 

$$
(a,b] = (-\infty, b) / (-\infty, a]
$$
To assign probabilities in the continuous way, however, we need to define certain knowledge of the distribution function F. 

#### Density Function: General Case 

The continuous distribution function F of a probability measure P on $\mathbb{R}, \mathbb{B}$ is defined as follows:

$$
F(x) = \int^x_{-\infty}f(t) dt
$$

where f(t) is the \textbf{density function} of the probability measure P. 

We interpret the density function equation accordingly: Since, at any real value x the distribution function uniquely equals the probability that an outcome of at most x is realized ($F(x) = P((-\infty, x])$), the density function states that this probability is obtained by \textbf{integrating some function f over the interval from} $-\infty$ \textbf{to x}. 

We interpret this function as the \textbf{marginal rate of growth of the distribution function F at some point x}. This follows the subsequent logic. We know that with continuous distribution functions, the probability of exactly a value of x occurring is zero. However, the probability of observing a value \textbf{inside of the interval} between x and some very small step to the right, denoted as $\triangle$ x (i.e. [x, x+$\triangle$ x]), is \textbf{not} necessarily zero.

As such, between this increment of x and $\triangle$ x, the distribution function F increases by exactly this probability. That is, the increment is:

$$
F(x + \triangle x) - F(x) = P(X \in [x, x + \triangle x))
$$

Now, dividing this equation by the width of the interval, denoted as $\trianlge$ x, we obtain the \textbf{average probability or average increment of F} per unit step on this interval. If we reduce the step size $\triangle$ x to an infinitesimally small step, $\delta x$, this average approaches the \textbf{marginal rate of growth of F at x}, which we denote f. This is the \textbf{Probability Density Function (PDF)}.

$$
\lim_{\triangle \rightarrow 0} \frac{F(x+\triangle x) - F(x)}{\triangle x} = \frac{\delta F(x)}{\delta(x)} = f(x)
$$

This equation is quite fundamental for continuous probability. Here, ee divide the probability that some realization should be inside of the small interval by that interval step. And, by letting that interval shrink to width zero, we obtain the marginal rate of growth or, equivalently, the derivative of F. Hence, we call f the probability density function or simply the density function. Commonly, it is abbreviated as pdf.

From the equation above, we understand that the probability of some occurrence of at most x is given by integration of the density function f over the interval $(-\infty, x]$. This follows the respective steps: 

1. For a given outcome, calculate the increment of x and $\triangle$ x, and divide this equation by the width of the interval to get the marginal rate of growth
2. At each value t, we multiply the corresponding density f(t) by the infinitesimally small interval width dt. 
3. Finally, we integrate all values of f (weighted by dt) up to x to obtain the probability for $(-\infty, x]$

In the end, the integral of this marginal rate of growth of F in the interval at x is exactly how the probability $P((-\infty, x])$ is derived through integrating the marginal rate f over the interval $(-\infty, x]$ with respect to the values. The resulting total probability is then given by the area under the curve in the below figure. 

```{r second heck chunk}
mean=80; sd=10
lb=60; ub=100

x <- seq(-4,4,length=100)*sd + mean
hx <- dnorm(x,mean,sd)

plot(x, hx, type="n", xlab="x", ylab="pdf")

i <- x >= lb & x <= ub
lines(x, hx)
polygon(c(lb,x[i],ub), c(0,hx[i],0), col="red")

area <- pnorm(mean, sd) - pnorm(lb, mean)
result <- paste("P(",lb,"< IQ <",ub,") =",
   signif(area, digits=3))
```

The area representing the value of the interval is indicated by the red block. So, the probability of some occurrence of at least a and at most b is given by the area inside red.

Based on the notions above, the probability of $X \in {a,b}$ is given by:

$$
P(X \in (a,b]) = \int^b_{a}f(t) dt
$$


### The cumulative Distribution

Before we dig into distributions with appealing properties for our statistical analysis, we first define some important concepts of distribution functions. 

The first is related ot the cumulative distribution. In general, the cumulative distribution function (CDF) of a random variable assigns the probability of a random variable X to be smaller than or equal to a given threshold. It can be also interpreted as a half-closed interval consisting of the entire space left to a certain threshold. Formally:

$$
F_X(x) = P(X\leq x)
$$

The most important properties are:

$$
\text{Property 1: } \text{If } x_1 < x_2, \text{then } F(x_1) < F(x_2) \\
\text{Property 2: } F_X(-\infty) = 0 \\
\text{Property 3: } F_X(\infty) = 1 \\
\text{Property 4: } P(X > x) = 1 - F_X(x) \\
\text{Property 5: } P(x_1 < X \leq x_2) = F_X(x_2) - F_X(x_1) 
$$

We can easily show an example for both discrete as well as continuous distributions: 

```{r}
d=data.frame(x=c(0,1,2,4,5,7,8,9, 10), cdf=c(0,0.1,0.2,0.3,0.5,0.6,0.7,1, 1))
ggplot() +
geom_step(data=d, mapping=aes(x=x, y=cdf), direction="vh", linetype=3) +
geom_point(data=d, mapping=aes(x=x, y=cdf), color="red") +
ylab("CDF") + xlab("x") + ggtitle("CDF for discrete distribution") +
theme(plot.title= element_text(size=14, color="grey26",
hjust=0.5,lineheight=2.4), 
panel.background = element_rect(fill="#f7f7f7"),
panel.grid.major.y = element_line(size = 0.5, linetype = "solid", color = "grey"),
panel.grid.minor = element_blank(),
panel.grid.major.x = element_blank(),
plot.background = element_rect(fill="#f7f7f7", color = "#f7f7f7"), 
axis.line = element_line(color = "grey")) 

```

```{r}
ggplot(data.frame(x = c(-5, 5)), aes(x = x)) +
  stat_function(fun = pnorm) + 
ylab("CDF") + xlab("x") + ggtitle("CDF for continuous distribution") +
theme(plot.title= element_text(size=14, color="grey26",
hjust=0.5,lineheight=2.4), 
panel.background = element_rect(fill="#f7f7f7"),
panel.grid.major.y = element_line(size = 0.5, linetype = "solid", color = "grey"),
panel.grid.minor = element_blank(),
panel.grid.major.x = element_blank(),
plot.background = element_rect(fill="#f7f7f7", color = "#f7f7f7"), 
axis.line = element_line(color = "grey")) 

```

#### Quantile values of Distributions 

\textbf{Definition 2.9}:

Given a random variable X with a continuous CDF F_X(x), for any $\alpha$, where 0 $\leq \alpha \leq 1$, the $100*\alpha$ % quantile of the distribution for X is given as the value $q_\alpha$ that satisfies:

$$
F_X(q_a) = P(X \leq q_\alpha) = \alpha
$$

In essence, the definition implies that the quantile distribution incorporates all values of a distribution up to a specific threshold such that exactly $\alpha$ % of the entire distribution are included within that range. 

Important examples that are often used in statistics include the 25% quantile, the median (50% quantile), the 75% quantile as well as minimum and maximum values. For instance, the median of the distribution, $q_{0.5}$ satisfies the following:

$$
F_X(q_{0.5}) = P(X \leq q_{0.5}) = 0.5
$$

In the case that $F_X$ is invertible, then $q_{\alpha}$ can be determined as: 

$$
q_\alpha = F_X^{-1}(\alpha)
$$

That is, by using the inverse cdf $F_X^{-1}$, one can determine the quantile value for a given threshold of the underlying distribution. Looking again at the median example, the 50% quantile value can be determined as: 

$$
q_{0.5} = F_X^{1}(0.5)
$$

This inverse is also called \textbf{quantile distribution}. 

\textbf{Example: Finding Quantile functions from a standard normal distribution}

Applying this in R is relatively straight-forward. Given the standard normal distribution, the quantile value can be determined by solving: 

$$
q_\alpha = \Phi^{-1}(\alpha)
$$

Where $\Phi^{-1}$ denotes the inverse of the cdf of the standard normal distribution with the function `qnorm()`. Let's use it to print the critical values of our normal distribution that we usually use for significance tests.

```{r}

# Define the functions 

critical_10 <- qnorm(0.95,mean=0,sd=1)
critical_5 <- qnorm(0.975,mean=0,sd=1)
critical_1 <- qnorm(0.995,mean=0,sd=1)

criticals <- round(cbind(critical_10, critical_5, critical_1),2)

colnames(criticals) <- c("10% Significance (2 Tailed)", "5% Significance (2 Tailed)", "1% Significance 21 Tailed)")

criticals
```

Accordingly, with the quantile function, we can obtain critical values of given distributions.  

### Continuous Distributions with Appealing Properties

Next, we discuss the more commonly used distributions with appealing statistical properties that are used in finance. These are the normal distribution, the student's t distribution, chi-2 distribution, Fisher F distribution and log-normal distribution. 

#### The Normal Distribution

The normal distribution, or Gaussian, is the most common distribution used in finance. It is defined by two parameters: its mean $\mu$ as well as its standard deviation $\sigma$. It is denoted by $N(\mu, \sigma)$. 

The PDF of the normal distribution is given by:

$$
f(x) = \frac{1}{\sqrt{2\pi}\sigma}e^{-(x-\mu)^2/2\sigma^2}
$$

We can easily print the pdf of the normal distribution using the function `rnorm()`

```{r}

dat <- read.table(text = "info mean sd
info1 0 1
info2 1 0.5
info3 2 1
", header = TRUE)

densities <- apply(dat[, -1], 1, function(x) rnorm(n = 100000, mean = x[1], sd = x[2]))
colnames(densities) <- dat$info

densities.m <- melt(densities)
#Plot
densities.m %>%
  ggplot(aes(x = value, fill = Var2, color = Var2)) +
    geom_density(alpha = 0.2) +
  theme_bw() + xlim(-5,5)

```

A problem is that the distribution function cannot be solved for analytically and therefore has to be approximated numerically. That is:

$$
P(a \leq X \leq b) = \int^b_a \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}x^2}dx
$$
does not have a closed form solution. 

In the particular case of the standard normal distribution, the values are tabulated. Standard statistical software provides the values for the standard normal distribution. Some useful approximations are:

$$
\begin{align*}
P(X \in [\mu \pm \sigma]) \approx 0.68\\
P(X \in [\mu \pm 2\sigma]) \approx 0.95\\
P(X \in [\mu \pm 3\sigma]) \approx 0.99
\end{align*}
$$

The above states that approximately 68% of the probability is given to values that lie in an interval of one standard deviation around the mean of the distribution. 

#### Chi-2 Distribution

In this distribution, let Z be a standard normal random variable, in brief $Z \sim N(0,1)$, and $X = Z^2$. Then X is distributed chi-square with one degree of freedom, denoted as $X \sim \chi^2(1)$. 

The PDF of the Chi-2 Distribution is given as:

$$
f(X) = \frac{1}{2^{n/2}\Gamma(\frac{n}{2})}e^{-\frac{x}{2}}x^{(\frac{n}{2}-1)}
$$
for any x $\geq$ 0. Here $\Gamma(z) = \int^\infty_0 t^{z-1}e^{-t}dt$ denotes the gamma function.

In general, Degrees of Freedom (DOF) indicate how many independently behaving standard normal random variables the resulting variable is composed of. Here, X is only composed of one, called Z. 

In general, this distribution is characterised by its DOF. If we have n distributed random variables that are all independent of each other, then their sum is written as: 

$$
S = \sum_{i=1}^n \textstyle X \sim \chi^2(n)
$$

The corresponding properties are: mean of E(x) = n and variance of Var(x) = 2n. So, the mean and variance are directly related to the degrees of freedom.

An important feature of the $chi^2$ distribution is the degrees of freedom equal the number of independent $\chi^2 (1)$ distributed $X_i$ in the sum. Consequently, the summation of any two chi-squared distributed random variables is itself chi-square distributed.

The $chi^2$ distribution is drawn with the function `rchisq()`` as follows:

```{r, warning = FALSE, message = FALSE}
a <- rchisq(100000, df = 2)
b <- rchisq(100000, df = 3)
c <- rchisq(100000, df = 7)
d <- rchisq(100000, df = 10)

df_chi <- as.data.frame(cbind(a,b,c,d))
colnames(df_chi) = c("DOF=2", "DOF=3", "DOF=7", "DOF=10")

df_chi_melt <- melt(df_chi)
#Plot
df_chi_melt %>%
  ggplot(aes(x = value, fill = variable, color = variable)) +
    geom_density(alpha = 0.2) +
  theme_bw() + xlim(0,20)
```

#### Student's t-Distribution

Basically, the Student’s t-distribution has a similar shape to the normal distribution, but thicker tails. For large degrees of freedom n, the Student’s t-distribution does not significantly differ from the standard normal distribution. If X has a Student’s t distribution with degrees of freedom parameter v, then the PDF has the form:

$$
f(x) = \frac{\Gamma(\frac{v+1}{2})}{\sqrt{v\pi}\Gamma(\frac{v}{2})}\left(1 + \frac{x^2}{v}\right)^{-\frac{(v+1)}{2}}
$$

where $\Gamma(z) = \int^\infty_0 t^{z-1}e^{-t}dt$ denotes the gamma function.

Especially, it has the following properties: 

$$
E[X] = 0\\
Var(x) = \frac{v}{v-2}, v > 2 \\
Skew(x) = 0, v > 3 \\
Kurt(x) = \frac{6}{v-4}, v > 4
$$

The parameter v controls the scale and tail thickness of the distribution. If v is close to four, then the kurtosis is large and the tails are thick. In general, the lower the degrees of freedom, the heavier the tails of the distribution, making extreme outcomes much more likely than for greater degrees of freedom or, in the limit, the normal distribution.

The t-distribution is displayed with the function `r`t()` below. Note that we also add the normal distribution to show that the tails for both t distributions are fatter, but the fattest are for the t distribution with the lowest DOF. 

```{r}

a <- rnorm(100000, mean = 0, sd = 1)
b <- rt(100000,  df = 1)
c <- rt(100000,  df = 5)
d <- rt(100000,  df = 10)

df_norm_t <- as.data.frame(cbind(a,b,c,d))
colnames(df_norm_t) = c("N(0,1)", "t, DOF=1", "t, DOF=5", "t, DOF=10")

df_norm_t_melt <- melt(df_norm_t)
#Plot
df_norm_t_melt %>%
  ggplot(aes(x = value, fill = variable, color = variable)) +
    geom_density(alpha = 0.2) +
  theme_bw() + xlim(-5,5)

```

#### F-Distribution

The F-Distribution is similar to the $\chi^2$ distribution, but with two variables. 

The PDF of the F-Distribution is defined as:

$$
f(X) = \frac{F(\frac{n_1 + n_2}{2})}{F(\frac{n_1}{2}) + F(\frac{n_2}{2})}\cdot\left(\frac{n_1}{n_2}\right)^{n_1/2} \cdot \frac{x^{n_1/2 - 1}}{\left[1+x\cdot\frac{n_1}{2}\right]^{\frac{n_1+n_2}{2}}}
$$
for any x $\geq$ 0. 

Here we let both $X \sim \chi^2(n_1)$ and $Y \sim \chi^2(n_2)$ and then $F(n_1, n_2)$ is defined as:

$$
F(n_1, n_2) = \frac{Y/n_1}{X/n_2}
$$

This ratio has an F-distribution with $n_1$ and $n_2$ DOF from the underlying $\chi^2$ distribution for X and Y, respectively. Also like the chi-square distribution, the F-distribution is skewed to the right.

The first two moments of the F-distribution are the following: The mean is given by E(X) = $\frac{n_2}{n_2 - 2}$ for $n_2 > 2$, and the variance is given by Var(X) = $\frac{2n_2^2(n_1 + n_2 -2)}{n_1(n_2 - 2)^2(n_2 - 4)}$ for $n_2 > 4$.

The F-Distribution values can be determined with the function `r(f)` and looks like this:

```{r, warning = FALSE, message = FALSE}
a <- rf(100000, df1 = 4, df2 = 4)
b <- rf(100000, df1 = 4, df2 = 10)
c <- rf(100000, df1 = 10, df2 = 4)
d <- rf(100000, df1 = 10, df2 = 100)

df_F <- as.data.frame(cbind(a,b,c,d))
colnames(df_F) = c("n1=4, n2=4", "n1=4, n2=10", "n1=10, n2=4","n1=10, n2=100")

df_F_melt <- melt(df_F)
#Plot
df_F_melt %>%
  ggplot(aes(x = value, fill = variable, color = variable)) +
    geom_density(alpha = 0.2) +
  theme_bw() + xlim(0,7)
```

#### Log-Normal Distribution

The last important function we look at is the log-normal distribution. It is directly linked to the standard normal distribution. To see this, let X be a normally distributed random variable with mean $\mu$ and variance $\sigma^2$. Then the random variable

$$
X = e^Y
$$

is log-normally distributed also with mean $\mu$ and variance $\sigma^2$. 

This distribution is denoted as $X \sim Ln(\mu, \sigma^2)$. The support of the log-normal distribution is on the positive half of the real line, as the exponential function can only take up positive values. 

Accordingly, the PDF of the log-normal distribution is given by:

$$
f(X) = \frac{1}{x\sigma\sqrt{2\pi}}e^{\frac{(\ln x - \mu)^2}{2\sigma^2}}
$$

for any x > 0. 

The density function is also similar to the normal distribution and results in the log-normal distribution function:

$$
F(x) = \Phi\left(\frac{\ln x - \mu}{\sigma}\right)
$$

The log-normal distribution values can be calculate using the `rlnorm()` function:

```{r, warning = FALSE, message = FALSE}
a <- rlnorm(100000, meanlog = 0, sdlog = 1)
b <- rlnorm(100000, meanlog = 0, sdlog = 0.5)
c <- rlnorm(100000, meanlog = 0, sdlog = 2)
d <- rlnorm(100000, meanlog = 1, sdlog = 1)

df_lnorm <- as.data.frame(cbind(a,b,c,d))
colnames(df_lnorm) = c("mean=0, sd=1", "mean=0, sd=0.5", "mean=0, sd=2","mean=1,sd=1")

df_lnorm_melt <- melt(df_lnorm)
#Plot
df_lnorm_melt %>%
  ggplot(aes(x = value, fill = variable, color = variable)) +
    geom_density(alpha = 0.2) +
  theme_bw() + xlim(-1,7)
```

#### Functions for distribution calculations (d, p, q, r)

To create distributions of the forms above, we use distribution packages. In these distribution packages, we can make use of four distinct functions that can apply all the theory we just discussed. 

For that, we will look at the normal distribution. As previously, each statistical property of the distribution can be calculated using the functions `dnorm()`, `pnorm()`, `qnorm()` and `rnorm()`. We will now cover what each of these functions does. 

**dnorm**

The `dnorm()` returns the value of of the probability density function for the distribution of interest, given the parameters x, $\mu$ and $\sigma$. That is, for a given input x with distributional moments of the mean and variance, we obtain the corresponding value on the y-axis, which indicates the density function of that value. 

```{r}
# This is the largest density of the PDF given that we have the normal distribution with mean = 0 & sd = 1
dnorm(0, mean = 0, sd = 1) 
```

**pnorm**

The `pnorm()` returns the integral from $-\infty$ to q of the pdf of a certain distribution, whereas q is a z-score. That is, for a given x, `pnorm()` returns the value of the y axis on the cdf, also known as probability density. Consequently, with this function you obtain the cdf functional value. In general, it is the function that replaces the table of probabilities and Z-scores at the back of the statistics textbook. 

In general, pnorm is used to get the probability that $-\infty < X \leq x$, and, as such, it gives the **p-value for a distirbution**.

```{r}
# This is the median value of the respective distribution. As such, it comprises of exactly half the overall density of the underlying distribution. This is intuitive, given that the value -1.96 in a normal distribution with mean = 0 and sd = 1 is exactly the middle value, thereby incorporating half of the entire area under the cdf. 
pnorm(-1.96, mean = 0, sd = 1) 
```

We deliberately chose -1.96 b/c, in a two-sided test when assuming normal distribution, then $-\infty < -1.96 \leq x$ and $x \leq -1.96 < \infty$ constitute approximately 5\% of the probability mass under the curve, or, in other words, 5\% of the total probability. 

**qnorm**

The `qnorm()` is the inverse of the `pnorm()` function. Looking at the quantiles part, it is the function that returns the inverse values of the cdf. You can use this function to determine the p'th quantile value of the underlying distribution (e.g. which value incorporates exactly half of the overall area under the curve). 

```{r heck chunk}
# Intuitively, pnorm() inverse for a value of 0.5 indicates what value the 50% quantile must have under the given distribution characteristics. As such, we see that the 5% quantile here must have value -1.96! 
qnorm(0.025, mean = 0, sd = 1) 
```

Consequently, it gives us the value for a given probability. That is, here a one-sided probability mass of 2.5\% would require that the corresponding value is approximately -1.96, thereby stating that -1.96 is at the 2.5'th percentile of the distribution. 

**rnorm**

Lastly, the `rnorm()` is used to generate vector of numbers that follow a certain distribution and its characteristics. We used this function to generate an order of numbers to plot subsequently and show the plotted distributions. 

```{r}
rnorm(10, mean = 0, sd = 1)
```

Although we just showed the functions for the normal distribution, R offers a great amount of functions for other distributions that can be used identically as the normal case. To give you an overview:



Neatly, nearly all of these functions can be used with the prefixes "d,p,q,r". Thus, for instance, just write rlnorm to get the values of a log-normal distribution. 

### Moments and Properties of bivariate distributions

Bivariate and Multivariate distributions are important concepts in asset management settings. This is because each asset can be regarded as a random variable. In order to be able to form portfolios, we thus need to understand how different assets relate with each other and what their common covarying structure is. 

To do so, let's look at the fundamental concepts and moments first. 

#### Bivariate Distributions for continuous random variables 

The joint probability for two random variables is characterised using their \textbf{joint probability distribution} (PDF), called f(x,y), such that:

$$
\int^\infty_{-\infty}\int^\infty_{-\infty}f(x,y)dxdy = 1
$$

Note that the joint probability distribution is plotted in a three-dimensional space. To find the joint probabilities of $x_1 \leq X \leq x_2$ and $y_1 \leq Y \leq y_2$, we must find the volume nder the probability surface over the grid where the intervals $[x_1, x_2]$ and $[y_1, y_2]$ are overlapping. That is:

$$
P(x_1 \leq X \leq x_2, y_1 \leq Y \leq y_2) = \int^{x_2}_{x_1}\int^{y_2}_{y_1}f(x,y)dxdy
$$

#### Standard bivariate normal distribution

An important bivariate distribution constitutes the standard bivariate normal distribution. It has the fofm:

$$
f(x,y) = \frac{1}{2\pi}e^{-\frac{1}{2}(x^2+y^2)} dx dy
$$

#### Marginal Distributions 

The marginal distribution treats the data as if only the one component was observed while a detailed joint distribution in connection with the other component is of no interest. In other words, the joint frequencies are projected into the frequency dimension of that particular component.

The frequency of certain values of the component of interest is measured by the **marginal frequency**. The marginal frequency of X is calculated as sum of all frequencies of X given that Y takes on a particular value. Thus, we obtain the row sum as the marginal frequency of this component X. That is, for each value $X_i$, we sum the joint frequencies over all pairs ($X_i$, $Y_j$) where $Y_j$ is held fix.

Formally, this is:

$$
f_x(X_i) = \sum_jf(X_i, Y_j)
$$

where the sum is over all values $W_j$ of the component Y.

Lastly, the marginal pdf of X is found by integrating y out of the joint PDF f(X,Y): 

$$
f(x) = \int^\infty_{-\infty} f(x,y)dy
$$

#### Conditional Distributions 

The conditional probability that X = x given that Y = y is defined as:

$$
f(x|y) = f(X=x|Y=y) = \frac{f(x,y)}{f(y)}
$$

Whereas an analogous principle holds for the conditional probability of y on x. 

The use of conditional distributions reduces the original space to a subset determined by the value of the conditioning variable.

In general, we need conditional distributions, or probability, to define what value x takes given that y takes a certain value. Consequently, we no longer are within a field of independence between two variables when we include conditional probability. 

Conditional moments are different to unconditional ones. As such, the conditional expectation and variance are defined as follows. 

\textbf{Definitionl 2.14: Conditional Expectation}

For discrete random variables, X and Y, the conditional expectation is given as:

$$
E[X|Y=y] = \sum_{x\in S_X}x\cdot P(X=x|Y=y)
$$

For discrete random variables, X and Y, the conditional variance is given as:

$$
var(X|Y=y) = \sum_{x\in S_X}(x-E[X|Y=y])^2\cdot P(X=x|Y=y)
$$


To do so, we first look at the properties of covariance and correlation

#### Independence

The previous discussion raised the issue that a component may have influence on the occurrence of values of the other component. This can be analyzed by comparison of the joint frequencies of x and y with the value in one component fixed, say x = X. If these frequencies vary for different values of y, then the occurrence of values x is not independent of the value of y. 

This is equivalent to check whether a certain value of x occurs more frequently given a certain value of y. That is, check the conditional frequency of x conditional on y, and compare this conditional frequency with the marginal frequency at this particular value of x. If the conditional frequency is not equal to the marginal frequency, then there is no independence. 

Formally, two random variables are **independent** if:

$$
f_{x|y}(x,y) = f(x)\cdot f(y)
$$

That is, the joint frequency is the mathematical product of their respective marginals. Independence is a handy feature as it allows us to compare marginal and conditional distribution properties of random variables. 

#### Correlation and Covariance 

Covariance and correlation describe properties of the combined variation of two or more assets. As the term describes, they measure to what extent assets covary. Thereby, they quantify the level of similarity of of movements over time for different variables. 

\textbf{Definition: Covariance}

The covariance between two random variables, X and Y, is given as:

$$
\sigma_{XY} = cov(X,Y) =E[(X - E(X))(Y - E(Y))] 
$$

\textbf{Definition: Correlation}

The correlation between two random variables, X and Y, is given as:

$$
\rho_{XY} = cor(X,Y) = \frac{\sigma_{XY}}{\sigma_X\sigma_Y}
$$

Covariance and Correlations have certain nice properties we can use. 

Important properties of the \textbf{Covariance} are:

1. cov(X,X) = var(X)
2. cov(X,Y) = cov(Y,X)
3. cov(X,Y) = E[XY] -E[X]E[Y]
4. cov(aX,bY) = abcov(X,Y)
5. cov(X,Y) = 0 if X and Y independent

Let's quickly show the third and fourth property: 

$$
\begin{align}
cov(X,Y) &= E[(X-E(X))(Y-E(Y))] \\
&= E[XY -E(X)Y -E(Y)X +E(X)E(Y)] \\
&= E[XY] - E(X)E(Y) - E(X)E(Y) + E(X)E(Y)\\
&= E[XY] - E(X)E(Y) 
\end{align}
$$
$$
\begin{align}
cov(aX,bY) &= E[(aX - aE(X))(bY - bE(Y))] \\
&= a\cdot b\cdot E[(X-E(X))(Y-E(Y))] \\
&= a\cdot b\cdot cov(X,Y)
\end{align}
$$
Important properties of the \textbf{Correlation} are:

1. $-1\leq \rho_{xy} \leq 1$
2. $\rho_{xy} = 1 $: Perfect positive linear relation
3. $\rho_{xy} = -1 $: Perfect negative linear relation

#### Expectation and variance of the sum of two random variables

Joint distributions are important when considering asset prices. They define how to compute important properties when considering multiple assets. When considering joint distributions, two important properties can be shown. 

The first relates to the expected value of a linear combination. Especially, it holds that, for two random variables with defined means and covariance matrices:

$$
\begin{align}
E[aX + bY] &= \sum_{x\in S_X}\sum_{y \in S_Y} (ax + by)P(X=x, Y=y) \\
&= \sum_{x\in S_X}\sum_{y \in S_Y} (ax)P(X=x, Y=y) + \sum_{y\in S_Y}\sum_{x \in S_X} (by)P(X=x, Y=y) \\
&= a\sum_{x\in S_X}x\sum_{y \in S_Y}P(X=x, Y=y) + b\sum_{y\in S_Y}y\sum_{x \in S_X}P(X=x, Y=y)\\
&= a\sum_{x\in S_X}xP(X=x) + b\sum_{y\in S_y}yP(Y=y) && \text{sum of all y options renders condition = 1}\\
&= aE[X] + bE[Y] \\
&= a\mu_X + b\mu_Y
\end{align}
$$
This means that expectation is additive.


The second result relates to the variance of a linear combination. Especially, it holds that, for two random variables with defined means and covariance matrices:

$$
\begin{align}
var(aX + bY) &= E[(aX + bY - E[aX]E[bY])^2]\\
&= E[((aX - E(aX)) + (bY - E(bY)))^2] \\
&= E[(a(X-E(X)) + b(Y-E(Y)))^2] \\
&= a^2E[X-E(X)]^2 + b^2(Y-E(Y))^2 + 2 ab(X-E(X))(Y-E(Y))\\
&= a^2\cdot var(X) + b^2\cdot var(Y) + 2\cdot a \cdot b \cdot cov(X,Y)
\end{align}
$$

That is, the variance of a linear combination of random variables is itself not linear. This is due to the covariance term when computing the variance of the sum of two random variables that are not independent. 

This means that the variance is not additive. 

Both properties are inherently important when considering both portfolio return as well as risk characteristics. 

### Moments of Probability Distributions

Moments of a distribution generally tell us things about the center, spread, the distribution as well as the shape behaviour of the underlying distribution. As such, they are important to understand the baseline configuration of a distribution. In our case, we will look at four moments: 

1. Expected Value (mean)
2. Variance 
3. Skewness
4. Kurtosis

#### Expected Value (Mean)

The expected value of a random variable X measures the center of mass for the underlying PDF. 

\textbf{Definition: Mean}

The expected value of a random variable X is given by:

$$
\mu_x = E[X] = \sum x\cdot P(X=x) 
$$

#### Variance and Standard Deviation

The variance of a random variable X measures the spread around the mean. As such, it measures the spread of the distribution. 

\textbf{Definition: Variance and Standard Deviation}

The variance and standard deviation of a random variable X are given by:

$$
\sigma_X^2 = E[(X-\mu_x)^2]\\
\sigma_X = \sqrt{\sigma_X^2}
$$

#### Skewness

\textbf{Definition: Skewness}

The skewness of a random variable X measures the symmetry of a distribution around its mean. It is given by:

$$
skew(X) = \frac{E[(X-\mu_x)^3]}{\sigma_X^3}
$$
If X has a symmetric distribution, then  skew(X) = 0, as values above and below the mean cancel each other out. There are two special cases: 

1. For skew(X) > 0, the distribution has a long right tail. 
2. For skew(X) < 0, the distribution has a long left tail. 

```{r}


a <- rsnorm(100000, mean = 0, sd = 2, xi = 2)
b <- rsnorm(100000, mean = 0, sd = 2, xi = -2)

skew <- as.data.frame(cbind(a,b))
colnames(skew) = c("Right Skewed", "Left Skewed")

skew_melt <- melt(skew)
#Plot
skew_melt %>%
  ggplot(aes(x = value, fill = variable, color = variable)) +
    geom_density(alpha = 0.2) +
  theme_bw() + xlim(-15,15)

```

#### Kurtosis

\textbf{Definition: Kurtosis}

The Kurtosis of X measures the thickness in the tails of a distribution. It is given as:

$$
kurt(X) = \frac{E[(X-\mu_x)^4]}{\sigma_x^4}
$$


Kurtosis is the average of the standardized data raised to the fourth power. Any standardized values that are less than 1 (i.e., data within one standard deviation of the mean) contributes very little to the overall Kurtosis. This is due to the fact that raising a value less than 1 to the fourth power shrinks the value itself (e.g. 0.5^4 = 0.0625). However, since kurtosis is based on deviations from the mean raised to the fourth power, \textbf{large deviations get lots of weight}. Consequently, large Kurtosis values indicate that extreme values are likely to be present in the data. 

Consequently, there are three types of Kurtosis we need to be familiar with:

1. Mesokurtic: This is the normal distribution
2. Leptokurtic: This distribution has fatter tails and a sharper peak. The kurtosis is “positive” with a value greater than 3
3. Platykurtic: The distribution has a lower and wider peak and thinner tails. The kurtosis is “negative” with a value greater than 3

To visualise this, we need to 

```{r}
a <- rnorm(100000, mean = 0, sd = 1)
b <- rnorm(100000, mean = 0, sd = 1.45)
c <- rnorm(100000, mean = 0, sd = 0.55)


df_kurt <- as.data.frame(cbind(a,b,c))
colnames(df_kurt) = c("Mesokurtic", "Platykurtic", "Leptokurtic")

df_kurt_melt <- melt(df_kurt)
#Plot
df_kurt_melt %>%
  ggplot(aes(x = value, fill = variable, color = variable)) +
    geom_density(alpha = 0.2) +
  theme_bw() + xlim(-4,4)
```

## Matrix Algebra: Introduction

In this chapter, we will repeat the basic matrix algebra concepts used throughout the lecture. Matrices are the simplest and most useful way to organise data sets. Using matrix algebra makes manipulation and transformation of multidimensional data sets easier, as it can summarise many steps that would be needed if we worked with each constituent individually. Especially, understanding the functioning of matrix algebra especially helps us in comprehending general concepts of the lecture. For instance, portfolio construction with either two or more assets as well as risk and return calculations can be simplified and generalised using matrix algebra. Further, systems of linear equations to define the first order conditions of mean-variance optimised portfolios can be created using matrix algebra. In general, this form of data manipulation is the most straight-forward when being applied to programming languages such as R, as the syntax of the program largely follows the syntax of textbook linear algebra discussions. Thus, copying theory into programming language is not that hard when using matrix manipulation techniques. Lastly, many R calculations can be efficiently evaluated if they are vectorized - that is, if they operate on vectors of elements instead of looping over individual elements.

The chapter will be organised as follows: Section 1 introduces the basic definitions and concepts of matrix algebra, much like you have already seen in Empirical Methods. Section 2 reviews basic operations and manipulation techniques. In Section 3, we look at how to represent summation notation with matrix algebra. Section 4 presents systems of linear equations that constitute the cornerstones of portfolio math. Then, Section 5 introduces the concept of Positive Semi-Definite (PSD) matrices, before we dive into multivariate probability distribution representations using matrix algebra. We conclude the chapter by having a discusssion on portfolio mathematics using matrix algebra as well as how to use derivatives of simple matrix functions. 

### Matrices and Vectors 

A vector is a one-dimensional array of numbers. For instance, 

$$
\underset{n \times 1}{\textbf{x}} = 
\begin{bmatrix}
x_{1}\\
x_{2} \\
\vdots\\
x_{n} 
\end{bmatrix}
$$

is an $n \times 1$ vector of entries x. This is also known as $\textbf{Column Vector}$

If we transpose a vector, it becomes a $\textbf{row vector}$. For instance: 

$$
\underset{n \times 1}{\textbf{x}'} = 
\begin{bmatrix}
x_{1} & x_2 & \dots & x_n
\end{bmatrix}
$$

A $\textbf{matrix}$ is a two-dimensional array of numbers. Each matrix consists of rows and columns, whereas both make up the dimension of a matrix. A general form of a matrix is the following: 

$$
\underset{n \times k}{\textbf{A}} = 
\begin{bmatrix}
a_{11} & a_{12} & \dots & a_{1k}\\
a_{21} & a_{22} & \dots & a_{2k} \\
\vdots & \vdots & \ddots & \vdots\\
a_{n1} & a_{n2} & \dots & a_{nk}
\end{bmatrix}
$$

where $a_{ij}$ denotes the element in the $i^{th}$ row and $j^{th}$ column of the matrix $\textbf{A}$. Just as with vectors, we can also transpose the matrix:

$$
\underset{n \times k}{\textbf{A}'} = 
\begin{bmatrix}
a_{11} & a_{21} & \dots & a_{n1}\\
a_{12} & a_{22} & \dots & a_{n2} \\
\vdots & \vdots & \ddots & \vdots\\
a_{1k} & a_{2k} & \dots & a_{nk}
\end{bmatrix}
$$

An important concept in matrices is $\textit{symmetric}$ and $\textit{square}$. A $\textit{symmetric}$ matrix $\textbf{A}$ is defined such that $\textbf{A} = \textbf{A'}$. This can only be the case if the matrix is already $\textit{square}$, implying that the number of rows equals the number of columns. 

$\textbf{Example: Creating Vectors and Matrices in R}$

In R, to construct $\textbf{vectors}$, the easiest way is to use the combine function `c()`:

```{r}
xvec = c(1,2,3)
xvec
```

Vectors of numbers in R are of class `numeric` and do not have a dimension attribute:

```{r}
class(xvec)
```

```{r}
dim(xvec)
```

The elements of a vector can be assigned names using the `names()` function:

```{r}
names(xvec) = c("x1", "x2", "x3")

xvec
```

Lastly, to create a $\textbf{matrix}$ from a vector, we use the `as.matrix()` function

```{r}
as.matrix(xvec)
```


In R, matrix objects are created using the matrix() function. For example, we can create a $2 \times 3$ matrix using:

```{r}
matA = matrix(data=c(1,2,3,4,5,6),nrow=2,ncol=3,byrow=FALSE)
matA
```

Looking for the "class" object, we can see if it's really a matrix:

```{r}
class(matA)
```

If we want to transpose the matrix, we can use the `byrow=TRUE` command:

```{r}
matA = matrix(data=c(1,2,3,4,5,6),nrow=2,ncol=3,byrow=TRUE)
matA
```

In order to get the dimension of a matrix, we use the `dim()` command:

```{r}
dim(matA)
```

This indicates the $n \times k$ structure, which is $2 \times 3$. 

Further, we can define names of the rows and columns using the `colnames()` and `rownames()` arguments:

```{r}
rownames(matA) = c("row1", "row2")
colnames(matA) = c("col1", "col2", "col3")
matA
```

Lastly, matrix manipulation starts with $\textbf{slicing operations}$. Thus, the elements of a matrix can be extracted or subsetted as follows:

```{r}
matA[1, 2]
```

This defines the elements which should be extracted from the matrix. In our case, we told the program to only take the `first row and second column` element of the matrix. 

If we only want to select according to one dimension, we do so accordingly:

```{r}
matA[1,]
```

This takes \textbf{all elements from the first row}

We can take \textbf{all elements from the first column} accordingly:

```{r}
matA[,1]
```

Lastly, we can $\textbf{transpose}$ a matrix by using the `t()` function:

```{r}
t(matA)
```

### Basic Matrix Operations

#### Addition and Subtraction

Matrices are additive. That means, given the same dimensions of two matrices, you can add and subtract the respective row-column elements from each other. 

For instance, if we have: 

$$
\textbf{A} = 
\begin{bmatrix}
3 & 4\\
8 & 5
\end{bmatrix}, 
\textbf{B} = 
\begin{bmatrix}
9 & 1\\
5 & 2
\end{bmatrix}
$$

Then:

$$
\textbf{A} + \textbf{B} = 
\begin{bmatrix}
3 & 4\\
8 & 5
\end{bmatrix} + 
\begin{bmatrix}
9 & 1\\
5 & 2
\end{bmatrix} = 
\begin{bmatrix}
3+9 & 4+1\\
8+5 & 5+2
\end{bmatrix} = 
\begin{bmatrix}
12 & 5\\
13 & 7
\end{bmatrix}\\
\textbf{A} - \textbf{B} = 
\begin{bmatrix}
3 & 4\\
8 & 5
\end{bmatrix} -
\begin{bmatrix}
9 & 1\\
5 & 2
\end{bmatrix} = 
\begin{bmatrix}
3-9 & 4-1\\
8-5 & 5-2
\end{bmatrix} = 
\begin{bmatrix}
-6 & 3\\
3 & 3
\end{bmatrix}
$$
In R, this is quite easily done:

```{r}
matA = matrix(c(3,4,8,5),2,2,byrow=TRUE) # Note that we do not indicate nrow = ... & ncol = ... but just write 2 at both places. As long as the 
                                         # order of the commands is correct, R automatically interprets the second entry as nrow and the third as 
                                         # ncol. 
matB = matrix(c(9,1,5,2),2,2,byrow=TRUE)

matA + matB
```

```{r}
matA - matB
```

#### Scalar and Vector Multiplication

Matrices are also $\textbf{multiplicative}$. That is, they can be multiplied by a scalar or by another matrix that is $\textbf{conformable}$. 

For instance, if we take a scalar `c = 2` and use the same matrices again, we get: 

$$
c * \textbf{A} = 2*
\begin{bmatrix}
3 & 4\\
8 & 5
\end{bmatrix} = 
\begin{bmatrix}
6 & 8\\
16 & 10
\end{bmatrix}
$$

Matrix multiplication only applies to conformable matrices. $\textbf{A}$ and $\textbf{B}$ are said to be conformable if the $\textbf{number of columns in A equals the number of rows in B}$. If Matrix $\textbf{A}$ has the dimension of $n \times k$ and $\textbf{B}$ the dimension of $k \times p$, then they are conformable with dimension of $n \times p$.

$$
\underset{2\times 2}{\textbf{A}} \cdot\underset{2\times 3}{\textbf{B}} = 
\begin{bmatrix}
3 & 4\\
8 & 5
\end{bmatrix} \cdot
\begin{bmatrix}
9 & 1 & 7\\
5 & 2 & 3
\end{bmatrix} = 
\begin{bmatrix}
3*9 + 4*5 & 3*1 + 4*2 & 3*7+4*3 \\
8*9+5*5 & 8*1+5*2 & 8*7+5*3
\end{bmatrix}=
\begin{bmatrix}
47 & 11 & 33\\
97 & 18 & 71
\end{bmatrix} = 
\underset{2\times 3}{\textbf{C}}
$$

Here, each element of the matrix $\textbf{C}$ is the `dot product` of the resulting from the $i^{th}$ row of $\textbf{A}$ and the $j^{th}$ column of $\textbf{B}$.

$\textbf{Example: Matrix Multiplication in R}$

To do this in R, we can simply use the `%*%` operator:

```{r}
matA = matrix(c(3,4,8,5),2,2,byrow=TRUE)
matB = matrix(c(9,1,7,5,2,3),2,3,byrow=TRUE)

matA %*% matB
```

```{r}
dim(matA %*% matB)
```

As we can see, the dimensions are now $2 \times 3$. 

#### Miscellaneous Matrix Properties

Some important properties of matrices that we will use for financial applications are the $\textbf{associative property}$ as well as $\textbf{transpose product property}$. 

That is, if three matrices are conformable (number of columns of the first is number of rows of the latter), then: 

$$
\textbf{A}(\textbf{B} + \textbf{C}) = \textbf{A}\textbf{B} + \textbf{A}\textbf{C}
$$

Further, the transpose of the product of two matrices is the product of the transposes in opposite order

$$
(\textbf{A}\textbf{B})' = \textbf{B}'\textbf{A}'
$$

#### Identity, Diagonal as well as Lower and Upper Triangle Matrices 

Some pre-defined matrices are quite common in financial applications. 

The first is called $\textbf{Identity Matrix}$. An identity matrix is a matrix with all zero elements and only diagonal elements consisting of 1's. 

In matrix algebra, pre-multiplying or post-multiplying a matrix by a conformable identity matrix gives back the matrix. Consequently, the matrix must consist of only non-zero diagonal entries. To illustrate, assume that: 

$$
\textbf{I} =
\begin{bmatrix}
1&0\\
0&1
\end{bmatrix}
$$

is the identity matrix and 

$$
\textbf{A} =
\begin{bmatrix}
a_{11}&a_{12}\\
a_{21} & a_{22}
\end{bmatrix}
$$

Then, multiplying $\textbf{A}$ and $\textbf{I}$ equals:

$$
\textbf{A} \cdot \textbf{I} =
\begin{bmatrix}
a_{11}&a_{12}\\
a_{21} & a_{22}
\end{bmatrix} \cdot
\begin{bmatrix}
1&0\\
0&1
\end{bmatrix} =
\begin{bmatrix}
a_{11}*1 + a_{12} * 0 & 0*a_{11}+ a_{12}*1\\
a_{21}*1 + a_{22} * 0 & 0*a_{21}+ a_{22}*1
\end{bmatrix} = 
\begin{bmatrix}
a_{11}&a_{12}\\
a_{21} & a_{22}
\end{bmatrix} = \textbf{A}
$$

In R, an identity matrix is constructed using the `diag()` function:

```{r}
matI = diag(2)
matI
```

Further, we can have a $\textbf{Diagonal matrix}$, $\textbf{Upper-Triangle matrix}$ as well as a $\textbf{Lower-Triangle matrix}$. For that, consider the following matrix:

$$
\textbf{A} = 
\begin{bmatrix}
d_1 & u_{12} & u_{13} & \dots & u_{1n} \\
l_{21} & d_2 & u_{23} & \dots & u_{2n} \\
l_{31} & l_{32} & d_3 & \dots & l_{3n} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
l_{n1} & l_{n2} & l_{n3} & \dots & d_{n}
\end{bmatrix}
$$

Here, d defines the diagonal, u the upper-triangle and l the lower-triangle entries. The diagonal consists of n elements, whereas both the lower- and upper-triangle consist of n(n-1)/2 entries. Then, we have the following matrices: 

A Diagonal Matrix $\textbf{D}$ is a $n \times n$ square matrix with $n \times 1$ vector of diagonal entries and zero else:

$$
\textbf{D} = 
\begin{bmatrix}
d_1 & 0 & 0 & \dots & 0 \\
0 & d_2 & 0 & \dots &0\\
0 & 0 & d_3 & \dots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \dots & d_{n}
\end{bmatrix}
$$

An upper-triangle matrix $\textbf{U}$ has all values below the main diagonal equal to zero:

$$
\textbf{U} = 
\begin{bmatrix}
d_1 & u_{12} & u_{13} & \dots & u_{1n} \\
0 & d_2 & u_{23} & \dots & u_{2n} \\
0 & l_{32} & d_3 & \dots & l_{3n} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \dots & d_{n}
\end{bmatrix}
$$
A lower-triangle matrix $\textbf{L}$ has all values above the main diagonal equal to zero: 

$$
\textbf{L} = 
\begin{bmatrix}
d_1 & 0 & 0 & \dots & 0 \\
l_{21} & d_2 & 0 & \dots & 0 \\
l_{31} & l_{32} & d_3 & \dots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
l_{n1} & l_{n2} & l_{n3} & \dots & d_{n}
\end{bmatrix}
$$

We can apply these matrices easily in R:

```{r}
matA = matrix(c(1,2,3,4,5,6,7,8,9), 3, 3)
matA
```

```{r}
# Extract the lower elements from the matrix:

matA[lower.tri(matA)]

```

```{r}
# Extract the upper elements from the matrix:

matA[upper.tri(matA)]

```

### Summation Notation in Matrix Form

Imagine we have the sum: 

$$
\sum^{n}_{k=1} x_k = x_1 + ... + x_n
$$

Then, this sum can be represented by a matrix multiplication of a vector $\textbf{x} = [x_1,..,x_n]$ and a $n \times 1 $ vector of ones: 

$$
\textbf{x'}\textbf{1} = [x_1,..,x_n] \cdot 
\begin{bmatrix}
1 \\
\vdots \\
1
\end{bmatrix} = 
x_1 + \dots + x_n = \sum^{n}_{k=1} x_k
$$

Next, we can have a squared sum:

$$
\sum^{n}_{k=1} x_k^2 = x_1^2 + ... + x_n^2
$$

Then, this sum can be represented by a matrix multiplication:

$$
\textbf{x'}\textbf{x} = [x_1,..,x_n] \cdot 
\begin{bmatrix}
x_1 \\
\vdots \\
x_n
\end{bmatrix} = 
x_1^2 + \dots + x_n^2 = \sum^{n}_{k=1} x_k^2
$$

Last, we can have cross-products:

$$
\sum^{n}_{k=1} x_ky_k = x_1y_1 + ... + x_ny_n
$$

Then, this sum can be represented by a matrix multiplication:

$$
\textbf{x'}\textbf{y} = [x_1,..,x_n] \cdot 
\begin{bmatrix}
y_1 \\
\vdots \\
y_n
\end{bmatrix} = 
x_1y_1 + \dots + x_ny_n = \sum^{n}_{k=1} x_ky_k
$$

In R, this can easily be facilitated:

```{r}
xvec = c(1,2,3)
onevec = rep(1,3)
t(xvec)%*%onevec
```

```{r}
yvec = c(4,3,5)
crossprod(xvec, yvec)
```

### Systems of Linear Equations

#### Inverse of a Matrix

Systems of linear equations and matrix algebra are indisputably linked. One of such links comes in the form of the $\textbf{inverse of a matrix}$ properties. 

To see how, let's consider the two linear equations: 

$$
x + y = 1\\
2x - y = 1
$$

Considering both functions, we can find their intersection points as $x = 2/3$ and $y = 1/3$. 

Note that these two linear equations can also be written in terms of matrix notation:

$$
\begin{bmatrix}
1 & 1\\
2 & -1
\end{bmatrix}
\begin{bmatrix}
x\\
y
\end{bmatrix} = 
\begin{bmatrix}
1\\
1
\end{bmatrix}
$$

Check yourself that with matrix multiplication you would obtain the same two equations as above. 

In general, this implies $\textbf{A}\cdot\textbf{z} = \textbf{b}$, where

$$
\textbf{A} = 
\begin{bmatrix}
1 & 1\\
2 & -1
\end{bmatrix}, \textbf{z} = 
\begin{bmatrix}
x\\
y
\end{bmatrix}, \textbf{b} = 
\begin{bmatrix}
1\\
1
\end{bmatrix}
$$

If, in this equation, we had a $2 \times 2$ matrix $\textbf{B}$ with elements such that $\textbf{B}\cdot\textbf{A} = \textbf{I_2}$ ($\textbf{I_2}$ being the identity matrix), then we can $\textbf{solve for elements in z}$ as follows:

$$
\begin{align*}
\textbf{B}\cdot\textbf{A}\cdot\textbf{z} &= \textbf{B}\cdot\textbf{b} \\
\textbf{I}\cdot\textbf{z} &= \textbf{B}\cdot\textbf{b} \\
\textbf{z} &= \textbf{B}\cdot\textbf{b}
\end{align*}
$$

or, in matrix notation:

$$
\underset{z}{
\begin{bmatrix}
x \\
y
\end{bmatrix}} = 
\underset{B}{
\begin{bmatrix}
b_{11} & b_{12} \\
b_{21} & b_{22}

\end{bmatrix}} 
\underset{b}{
\begin{bmatrix}
1 \\
1
\end{bmatrix}}
$$

If such a matrix $\textbf{B}$ exists, it is called $\textbf{inverse of A}$ and is denoted as $\textbf{A}^{-1}$. 

This is the same as when we want to solve the system of linear equation $\textbf{A}\textbf{x} = \textbf{b}$, we just take the $\textbf{A}$ to the other side and get: $\textbf{x} = \textbf{A}^{-1}\textbf{b}$. 

As long as we can determine the elements in  $\textbf{A}^{-1}$, then we can solve for the values of x and y in the linear equations system of the vector $\textbf{z}$. The system of linear equations has a solution as long as the $\textbf{two lines intersect}$. If the two lines are parallel, then one of the equations is a multiple of the other. In this case, we say that  $\textbf{A}$ is NOT INVERTIBLE. 

There are general rules in R how to solve for such a system of linear equations. One is in the form of the `solve()` function: 

```{r}
matA = matrix(c(1,1,2,-1), 2, 2, byrow=TRUE)
vecB = c(1,1)

# First we solve for the inverse of A: A^-1
matA.inv = solve(matA) 

# Then, we can easily solve for the vector z:

z = matA.inv%*%vecB
z
```

#### Linear Independence and Rank of a Matrix

Consider again the $n \times k$:

$$
\underset{n \times k}{\textbf{A}} = 
\begin{bmatrix}
a_{11} & a_{12} & \dots & a_{1k}\\
a_{21} & a_{22} & \dots & a_{2k} \\
\vdots & \vdots & \ddots & \vdots\\
a_{n1} & a_{n2} & \dots & a_{nk}
\end{bmatrix} = [\textbf{a}_1, \textbf{a}_2, \dots, \textbf{a}_k]
$$

Where each vector $\textbf{a}$ is a $n \times 1$ column vector. 

In that case, k vectors $\textbf{a}_1,  \textbf{a}_2, \dots  \textbf{a}_k$ are $\textbf{linearily independent}$ if $\textbf{a}_1c_1 + \textbf{a}_2c_2 + \dots + \textbf{a}_kc_k = 0$. 

That is, no vector can be expressed as a $\textbf{non-trivial linear combination}$ of the other vectors.

For us of importance is the $\textbf{Rank of a Matrix}$. The column rank of matrix, denoted $\textbf{rank()}$, is equal to the $\textbf{maximum number of linearly independent columns}$. If rank(A) = m, then we say the matrix has $\textbf{full rank}$. 

In R, we figure the rank of a matrix accordingly with the `rankMatrix()` function:

```{r}
library(Matrix)
Amat = matrix(c(1,3,5,2,4,6), 2, 3, byrow=TRUE)
as.numeric(rankMatrix(Amat))
```

### Positive Definite (PD) Matrix


Another important concept in matrix algebra is positive definiteness. We consider a matrix \textbf{A} to be \textbf{positive definite} if for any $n \times 1$ x $\neq$ 0: 

$$
\begin{equation}
\textbf{x}'\textbf{A}\textbf{x} > 0
\end{equation}
$$

Therein, we consider a matrix $\textbf{A}$ to be $\textbf{positive semi-definite}$ if for any $n \times 1$ x $\neq$ 0: 

$$
\begin{equation}
\textbf{x}'\textbf{A}\textbf{x} \geq 0
\end{equation}
$$
Hence, if a matrix is positive semi-definite then there exists some vector x such that $\textbf{A}\textbf{x} = 0$, which implies that \textbf{A does not have full rank}. 

### Multivariate Probability Distributions

#### Covariance and Correlation Matrix 

Covariance and Correlation Matrices are fundamental concepts in financial applications. 

The covariance matrix is denoted as a sum sign, $\scriptstyle\sum$. It summarizes the variances and covariances of the elements of the random vector \textbf{X}. 

Generally, the covariance matrix of a random vector $\textbf{X}$ with mean vector $\mu$ is defined as:

$$
\begin{equation}
cov(\textbf{X}) = E[(\textbf{X}- \mu)(\textbf{X}- \mu)'] = \scriptstyle\sum
\end{equation}
$$

If $\textbf{X}$ has n elements, then $\scriptstyle\sum$ will be the symmetric and positive semi-definite  n $\times$ n matrix: 

$$
\scriptstyle\sum_{n \times n} = 
\begin{bmatrix}
\sigma_1^2 & \sigma_{12} & \dots & \sigma_{1n}\\
\sigma_{12} & \sigma_2^2 & \dots & \sigma_{2n}\\
\vdots & \vdots & \ddots & \vdots \\
\sigma_{1n} & \sigma_{2n} & \dots & \sigma_n^2
\end{bmatrix}
$$

To understand the mathematics behind a variance-covariance matrix, let's quickly look at the composition of a 2 $\times$ 2 case: 

$$
\begin{align*}
E[(\textbf{X}- \mu)(\textbf{X}- \mu)'] &= E\left[(\begin{matrix} X_1 - \mu_1 \\ X_2 - \mu_2\end{matrix})(X_1 - \mu_1, X_2 - \mu_2)\right]\\
&= E\left[(\begin{matrix}(X_1 - \mu_1)(X_1 - \mu_1) & (X_1 - \mu_1)(X_2 - \mu_2) \\ (X_1 - \mu_1)(X_2 - \mu_2) & (X_2 - \mu_2)(X_2 - \mu_2)\end{matrix})\right] \\
&= \left(\begin{matrix}E[(X_1 - \mu_1)^2] & E[(X_1 - \mu_1)(X_2 - \mu_2)] \\ E[(X_1 - \mu_1)(X_2 - \mu_2)] & E[(X_2 - \mu_2)^2\end{matrix}\right) \\
&= \left(\begin{matrix} var(X_1) & cov(X_1,X_2) \\ cov(X_2, X_1) & var(X_2) \end{matrix}\right) \\
&= \left(\begin{matrix} \sigma_1^2 & \sigma_{12} \\ \sigma_{12} & \sigma_2^2 \end{matrix}\right)
\end{align*}
$$


In contrast to the covariance matrix, the correlation matrix $\textbf{C}$ summarizes all pairwise correlations between the elements of the n $\times$ 1 random vector $\textbf{X}$ is given by: 

$$
\begin{equation}
\textbf{C} = 
\left(
\begin{matrix}
1 & \rho_{12} & \dots & \rho_{1n} \\
\rho_{12} & 1 & \dots & \rho{2n} \\
\vdots & \vdots & \ddots & \vdots \\
\rho_{1n} & \rho_{2n} & \dots & 1
\end{matrix}
\right)
\end{equation}
$$

In general, the correlation matrix can be computed from the covariance matrix. For that, we understand that: 

$$
\begin{equation}
\textbf{C} = \textbf{D}^{-1}\scriptstyle\sum\textstyle\textbf{D}^{-1}
\end{equation}
$$

Whereas $\textbf{D}$ is an n $\times$ n diagonal matrix with the standard deviations of the elements of $\textbf{X}$ along the main diagonal:

$$
\textbf{D} = \left(
\begin{matrix}
\sigma_1 & 0 & \dots & 0 \\
0 & \sigma_2 & \dots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \dots & \sigma_n
\end{matrix} \right)
$$

In R, a covariance matrix is calculated using usual matrix formulation: 

```{r}
sigma1 = 2.3
sigma2 = 1.8
rho12 = 0.35
sigma12 = rho12*sigma1*sigma2
cov_m = matrix(c(sigma1^2, sigma12, sigma12, sigma2^2), 
               2, 2, byrow=TRUE)
cov_m
```

Then, we can transform this into a correlation matrix using the `cov2cor()` function:

```{r}
cov2cor(cov_m)
```

Note that this makes absolutely sense, as the rho12 = 0.35, which is nothing else than the correlation between sigma1 and sigma2 as we defined it above. 

#### Variance of a linear combination of random vectors 

Another important property are transformations with linear combinations. That is, we add an n $\times$ 1 vector called $\textbf{a}$ = $(a_1, \dots, a_n)$ to the random vector $\textbf{X}$. 

If we assume that a random variables $\textbf{Y}$ exists which is a \textbf{linear combination of X and a} of the form $Y = \textbf{a}'\textbf{X} = a_1X_1 + \dots + a_nX_n$, then the expected values is: 

$$
\mu_y = E[Y] = E[\textbf{a}'\textbf{X}] = \textbf{a}'E[\textbf{X}] = \textbf{a}'\mu
$$

and the corresponding variance is:

$$
var(Y) = var(\textbf{a}'\textbf{X}) = E[(\textbf{a}'\textbf{X} - \textbf{a}'\mu)^2] = E[(\textbf{a}'(\textbf{X} - \mu))^2]
$$

We can now use a simple definition from matrix algebra. If z is a scalar, then we know that $z'z = zz' = z^2$. We know that $\textbf{a}'(\textbf{X} - \mu)$ is a scalar, as such we can compute the variance as:

$$
\begin{align*}
var(Y) &= E[z^2] = E[z \cdot z'] \\
&= E[\textbf{a}'(\textbf{X} - \mu)\textbf{a}(\textbf{X} - \mu)'] \\
&= \textbf{a}'E[(\textbf{X} - \mu)(\textbf{X} - \mu)']\textbf{a} \\
&= \textbf{a}'cov(\textbf{X})\textbf{a} \\
&= \textbf{a}'\scriptstyle\sum\textstyle\textbf{a}
\end{align*}
$$

Consequently, we know that the variance of a linear combination of a random variable and a constant is just the inverse of the constant multiplied with the covariance of the random variable multiplied with the constant. 

#### Covariance between linear combination of two random vectors

If we consider two different constants, $\textbf{a}$ = $(a_1, \dots, a_n)$ as well as $\textbf{b}$ = $(b_1, \dots, b_n)$ to the random vector $\textbf{X}$, and $Y = \textbf{a}'\textbf{X} = a_1X_1 + \dots + a_nX_n$ as well as $Z = \textbf{b}'\textbf{X} = b_1X_1 + \dots + b_nX_n$, we can write the covariance in matrix notation as: 

$$
\begin{align*}
cov(Y,Z) &= E[(Y - E[Y])(Z-E[Z])] \\
cov(\textbf{a}'\textbf{X}, \textbf{b}'\textbf{X}) &= E[(\textbf{a}'\textbf{X} - E[\textbf{a}'\textbf{X}])(\textbf{b}'\textbf{X}-E[\textbf{b}'\textbf{X}])] \\
&= E[(\textbf{a}'\textbf{X} - \textbf{a}'\mu])(\textbf{b}'\textbf{X}-\textbf{b}'\mu])] \\
&= E[\textbf{a}'(\textbf{X} - \mu)\textbf{b}'(\textbf{X} - \mu)] \\
&= \textbf{a}'E[(\textbf{X} - \mu)(\textbf{X} - \mu)']\textbf{b}'\\
&= \textbf{a}'\scriptstyle\sum\textstyle\textbf{b}'
\end{align*}
$$

#### Multivariate Normal Distribution

Now, let's start to combine the usual assumptions of distribution functions we use in daily statistic life with the properties of the matrix algebra notations we derived in this chapter. 

For that, we define: n random variables $\textbf{X_1}, \dots, \textbf{X_n}$ that are $\textbf{jointly normally distributed}$. Then, we have n $\times$ 1 vectors $\textbf{X}$ = $(X_1,\dots,X_n)'$, $\textbf{x}$ = $(x_1,\dots,x_n)'$ as well as  $\mu = (\mu_1,\dots,\mu_n)'$ and

$$
\scriptstyle\sum_{n \times n} = 
\begin{bmatrix}
\sigma_1^2 & \sigma_{12} & \dots & \sigma_{1n}\\
\sigma_{12} & \sigma_2^2 & \dots & \sigma_{2n}\\
\vdots & \vdots & \ddots & \vdots \\
\sigma_{1n} & \sigma_{2n} & \dots & \sigma_n^2
\end{bmatrix}
$$

Under these properties, we understand that $\textbf{X} \sim N(\textstyle\mu, \scriptstyle\sum)$ means that the random vector $\textbf{X}$ has a \textbf{multivariate normal distribution with mean vector} $\mu$ and covariance matrix $\scriptstyle \sum$. 

We can show that under the assumptions of iid (independently and identically distributed) standard normal random variables, we can create a random vector with the properties of $\textbf{X} \sim N(\textstyle\mu, \scriptstyle\sum)$. 

For that, let's assume that $\textbf{Z} = (Z_1, \dots, Z_n)'$. In this case, $\textbf{Z} \sim N(0, I_n)$ where $I_n$ denotes the identity matrix. Given \textbf{Z} we can create \textbf{X} with the desired properties, if we define

$$
\textbf{X} = \mu + \scriptstyle\sum^{1/2}\textstyle\textbf{Z}
$$

where $\scriptstyle\sum^{1/2}$ is the upper-triangle matrix defined previously where $\scriptstyle\sum = \sum^{1/2}'\sum^{1/2}$ . In that case: 

$$
E[\textbf{X}] = E[\mu + \scriptstyle\sum^{1/2}\textstyle\textbf{Z}] = \mu + \scriptstyle\sum^{1/2}\textstyle E[\textbf{Z}] \mu + \scriptstyle\sum^{1/2}\textstyle E[\textbf{0}] = \mu
$$

and: 

$$
\begin{align*}
var(\textbf{X}) &= E[\textbf{X} - E[\textbf{X}]]\\
&= E[\mu + \scriptstyle\sum^{1/2}\textstyle\textbf{Z} - \mu] \\
&= E[\scriptstyle\sum^{1/2}\textstyle\textbf{Z}] \\
&= var(\scriptstyle\sum^{1/2}\textstyle\textbf{Z}) \\
&= \scriptstyle\sum^{1/2}'var(\textstyle\textbf{Z})\scriptstyle\sum^{1/2}\\
&= \scriptstyle\sum^{1/2}'I_n\scriptstyle\sum^{1/2} \\
&= \scriptstyle\sum
\end{align*} 
$$

Thus, $\textbf{X} \sim N(\mu, \scriptstyle\sum)$

$\textbf{Example: Simulation of a multivariate normal random vectors in R}$

We can easily simulate the that $\textbf{X} \sim N(\mu, \scriptstyle\sum)$ where $\mu = (1,1)'$ and 

$$
\scriptstyle\sum = \textstyle
\begin{bmatrix}
1 & 1 \\
1 & 3
\end{bmatrix}
$$
```{r}
mu = c(1,1) # Specify the mu vector 

sigma1 = 1
sigma2 = 2 # Specify both standard deviations 
rho12 = 0.35 # Specify the correlation coefficient

sigma12 = sigma1*sigma2*rho12 # Define the covariance  

Sigma = matrix(c(sigma1^2, sigma12, sigma12, sigma2^2), 2, 2, byrow = TRUE) # Create the covariance matrix 

Sigma_0.5 = chol(Sigma) # the Cholesky factorization of the covariance matrix to only get the upper-triangular form 

n = 2
set.seed(123)
Z = rnorm(n) # Compute the random variable Z ~ N(mu, I_2)

X = mu * Sigma_0.5%*%Z # Compute X from the of 2 iid standard normal random variables
```

Now, the vector \textbf{X} stems from a vector of iid standard normal random variables \textbf{Z} and has the properties of (is proportional to) $\mu$ and $\scriptstyle\sum$ as first two moments (mean and variance). 

### Portfolio Construction and Mathematical Properties using Matrix Algebra

As we now covered the main baseline principles of matrix algebra, we can utilise this knowledge and combine it with the fundamentals of portfolio theory. 

For that, we assume that we have a portfolio consisting of \textbf{three assets}. These all are \textbf{jointly normally distributed} with means, variances and covariances:

$$
E[R_i] = \mu_i \\ 
var(R_i) = \sigma_i^2\\
cov(R_i, R_j) =  \sigma_{ij}
$$

Furthermore, we consider portfolio weights $x_i$ within our setting, whereas $\sum x_i = 1$. That is, the entire portfolio consists of only these three assets. . As such, the \textit{value-weighted portfolio return} is given as:

$$
R_{p,x} = x_1\mu_1 + x_2\mu_2 + x_3\mu_3
$$

And the portfolio variance is given as:

$$
\begin{align*}
\sigma_{p,x}^2 &= \sigma_1^2x_1^2 + \sigma_2^2x_2^2 + \sigma_3^2x_3^2 + 2\sigma_{1,2}x_1x_2 + 2\sigma_{2,3}x_2x_3 + 2\sigma_{13x_1x_3} \\
&= \sigma_1^2x_1^2 + \sigma_2^2x_2^2 + \sigma_3^2x_3^2 + 2\sigma_1\sigma_2\rho_{1,2}x_1x_2 + 2\sigma_2\sigma_3\rho_{2,3}x_2x_3 + 2\sigma_1\sigma_3\rho_{13}x_1x_3
\end{align*}
$$

Please remember that $\sigma_{i,j} = \sigma_i\sigma_j\rho_{i,j}$.

We can now substantially simplify this expression by using matrix algebra:

$$
\textbf{R} = 
\begin{bmatrix}
R_1 \\
R_2 \\
R_3
\end{bmatrix}, 
\textbf{x} = 
\begin{bmatrix}
x_1 \\
x_2 \\
x_3
\end{bmatrix}
$$

As all of the variables are perfectly characterised, by assumptions, through their mean, variance and covariance structures, we can easily characterise them by using: 

$$
E[\textbf{R}] = E\left(\begin{bmatrix}
R_1 \\
R_2 \\
R_3
\end{bmatrix}\right) = 
\begin{bmatrix}
\mu_1 \\
\mu_2 \\
\mu_3
\end{bmatrix} = \mu 
$$
$$
var(R) = 
\begin{bmatrix}
\sigma_1^2 & \sigma_{12} & \sigma_{13}\\
\sigma_{21} & \sigma_2^2 & \sigma_{23}\\
\sigma_{31} & \sigma_{32} & \sigma_3^2
\end{bmatrix} = \scriptstyle\sum
$$

Here, the covariance is \textbf{symmetric} per definition. That is, upper-left triangle values equal lower-left triangle values and the matrix transposed is identical to the original matrix. 

We can easily see how to write the portfolio expected returns and variances in matrix notation. The expected return of the portfolio is given by:

$$
E[R_{pf}] = \begin{bmatrix} x_1 & x_2 & x_3 \end{bmatrix} \begin{bmatrix} \mu_1 \\ \mu_2 \\ \mu_3 \end{bmatrix} = x_1\mu_1 + x_2\mu_2 + x_3\mu_3
$$
And the portfolio variance is given by: 

$$
\begin{align*}
var(R_{p,x}) 
&= var(\textbf{x}'\textbf{R}) \\
&=\textbf{x}' \textbf{R} \textbf{x}\\
&= 
\begin{bmatrix} x_1, & x_2, & x_3 \end{bmatrix} 
\begin{bmatrix}
\sigma_1^2 & \sigma_{12} & \sigma_{13}\\
\sigma_{21} & \sigma_2^2 & \sigma_{23}\\
\sigma_{31} & \sigma_{32} & \sigma_3^2
\end{bmatrix} 
\begin{bmatrix}
x_1 \\ x_2 \\ x_3
\end{bmatrix} \\ &= 
\begin{bmatrix}
x_1\sigma_1^2 + x_2\sigma_{21} + x_3\sigma_{31}, & x_1\sigma_{12} + x_2\sigma_2^2 + x_3\sigma_{32}, &  x_1\sigma_{13} + x_2\sigma_{23} + x_3\sigma_3^2
\end{bmatrix} \begin{bmatrix}
x_1 \\ x_2 \\ x_3
\end{bmatrix} \\
&= \sigma_1^2x_1^2 + \sigma_2^2x_2^2 + \sigma_3^2x_3^2 + 2\sigma_{1,2}x_1x_2 + 2\sigma_{2,3}x_2x_3 + 2\sigma_{13}x_1x_3
\end{align*}
$$

This is a very easy expression. Accordingly, we can also compute the covariance between the return on portfolio $\textbf{x}$ and $\textbf{y}$ using matrix algebra: 

$$
\begin{align*}
\sigma_{xy} = cov(R_{p,x},R_{p,y}) &= cov(\textbf{x}'\textbf{R}, \textbf{y}'\textbf{R}) \\
&= E[(\textbf{x}'\textbf{R} - E[\textbf{x}'\textbf{R}]), (\textbf{y}'\textbf{R} - E[\textbf{y}'\textbf{R}])] \\
&= E[(\textbf{x}'\textbf{R} - \textbf{x}'\mu_x)(\textbf{y}'\textbf{R} - \textbf{y}'\mu_y)] \\ 
&= E[\textbf{x}'(\textbf{R} - \mu_x)(\textbf{R} - \mu_y)'\textbf{y}] \\
&= \textbf{x}'E[(\textbf{R} - \mu_x)(\textbf{R} - \mu_y)]\textbf{y}\\
&= \textbf{x}'\scriptstyle\sum_{xy}\textstyle\textbf{y}
\end{align*}
$$






