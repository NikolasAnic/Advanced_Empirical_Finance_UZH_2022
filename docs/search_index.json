[["index.html", "Advanced Empirical Finance Chapter 1 Introduction to the course objectives and organisation of the course", " Advanced Empirical Finance Nikolas Anic &amp; Lorenz Gassmann 2022-03-04 Chapter 1 Introduction to the course objectives and organisation of the course This is the R Bookdown file used for the course Advanced Empirical Finance taught by Prof. Dr. Per Östberg in the Spring Semester of 2022 at the University of Zurich. The course covers a wide range of topics in Finance as well as Financial Economics and Econometrics topics. While Prof. Dr. Östberg is covering the theoretical aspetcs of the subjects, this Bookdown file covers empirical applications of the topics discussed. Especially, the exercise sessions will introduce both the statistical properties of the methods under considerations and simultaneously offer concise empirical application challenges in which students will learn to apply the lecture topics in the R programming language. "],["the-programming-environment.html", "Chapter 2 The Programming Environment 2.1 Markdown Files 2.2 Coding In R - A concise overview", " Chapter 2 The Programming Environment The first week covers two baseline subjects required to work within the field of empirical financial applications. The first topic covers the application environment. Therein, we cover the structure and use of the R Markdown file, on which we will write all of the subjects covered during the Lab sessions. We offer a discussion as how to create a markdown file, what makes these files so special in empirical research, how to organise and shuffle them as well as their connectivity to latex. Especially, we will consider how to write functions, how to add text-based outputs, how to construct and use coding cells as well as how to render the files. 2.1 Markdown Files The first part of the programming environment concerns the general programming environment. Markdown files will be used as programming interface for the Exercise and Lab sessions. Within these files, the students will describe the theoretical questions and solve the empirical exercises. Furthermore, the students will write a report and present their results in Markdown files. R Markdown files work as combination of R files and Latex files. That is, they enable the use of coding results and writing a report automatically. The handy feature arises in that you don’t have to copy your code output, tables or other things into a latex file in which you write your report. However, although we advise the use of a markdown file for writing reports, it is important to stress that we do not recommend writing entire papers or theses within markdown files, as the compiling burden could render mistakes within the program. As such, we are indifferent whether you decide to provide markdown or Latex files 2.1.1 How to create a Markdown File To create a markdown file, you simply open your R Studio, in the side bar you click on Edit, then you select New File and R Markdown.... It will pop up a new window where you need to enter the title as well as the authors. After choosing appropriate names, you can select one of the options HTML, PDF or Word as an output window. This is the first handy feature of markdown files: They enable different outputs, meaning you can have a multitude of channels to present it in. Further note that on the left-hand side, there is a sidebar which allows you to choose other file types, such as Shiny (supporting interactive graphs) or presentation files. Although we do not cover these throughout the lecture, they offer exciting new options. Although we would advise you to use PDF as output, you can also choose HTML to see how a HTML output would look like, just click anywhere here. Once you choose your desired output format, you will see that a new window pops up that looks like this: Figure 1: Markdown First file structure In order to comprehend the structure of a markdown file, it is firstly important to understand that a markdown file consists of white spaces and grey code Chunks The white spaces are used to write text, whereas the code Chunks are used to write code. Importantly, you won’t need the part below the first code block, so just delete everything from the “## R Markdown”\" title on. This will leave you with only the first code block. The first code block is important because there you can get your working directory set for the entire markdown session. You set your working directory in the following way # knitr::opts_knit$set(root.dir = &quot;/This/is/your/path/to/your/files/&quot;) The working directory is your source file from which R gets all %&gt;% and to which R sends all outputs, by default. You can later change your working directory or the path from which files should be read / should be written to, but it is handy to define one source that can be used for most of the files. Once you enter your preferred working directory, you can continue to start entering the packages you want to use. For that, add a code block by clicking on the green tool box above and select the option “R”, just as in the image below. Figure 2: Markdown Select R Codeblock Once you have your codenblock, you can enter the packages you need to use. It is handy to define functions which either install the packages or load the packages for that: packs.inst &lt;- c(&quot;readxl&quot;,&quot;foreign&quot;,&quot;dplyr&quot;,&quot;tidyr&quot;,&quot;ggplot2&quot;,&quot;stargazer&quot;,&quot;haven&quot;,&quot;dummies&quot;,&quot;Hmisc&quot;, &quot;lmtest&quot;,&quot;sandwich&quot;, &quot;doBy&quot;, &quot;multiwayvcov&quot;, &quot;miceadds&quot;, &quot;car&quot;, &quot;purrr&quot;, &quot;knitr&quot;, &quot;zoo&quot;, &quot;readstata13&quot;, &quot;tidyverse&quot;, &quot;psych&quot;, &quot;wesanderson&quot;, &quot;lubridate&quot;,&quot;reporttools&quot;, &quot;data.table&quot;, &quot;devtools&quot;, &quot;rmarkdown&quot;,&quot;estimatr&quot;, &quot;ivpack&quot;, &quot;Jmisc&quot;, &quot;lfe&quot;, &quot;plm&quot;, &quot;tinytex&quot;, &quot;xts&quot;, &quot;psych&quot;, &quot;PerformanceAnalytics&quot;, &quot;roll&quot;, &quot;rollRegres&quot;, &quot;glmnet&quot;, &quot;hdm&quot;, &quot;broom&quot;, &quot;RCurl&quot;, &quot;learnr&quot;, &quot;maps&quot;, &quot;fGarch&quot;, &quot;remotes&quot;, &quot;RPostgreSQL&quot;, &quot;wrds&quot;, &quot;DBI&quot;, &quot;RPostgreSQL&quot;, &quot;remotes&quot;, &quot;RPostgres&quot;) packs.inst &lt;- c() lapply(packs.load, require, character.only = TRUE) lapply(packs.inst, install.packages, character.only = FALSE) The underlying idea of this code block is that you just create a list with all the packages you need to load first and assign it a name (packs.load). Then, you uncomment the first code line (# lapply(packs.inst, require, character.only = TRUE)) to load the respective packages. However, it might be the case that you did not yet install all of the necessary packages. In that case, just take all the packages you did not yet install and add them to the list called packs.inst. Then, uncomment the second code line (lapply(packs.inst, install.packages, character.only = FALSE)) and run that code. The idea behind the code block is that you apply the install.packages or require command for a list of packages, instead of manually installing or loading each of them. Although these are more packages than needed for the firs sessions, it still is handy to have an overview and automate a rather cumbersome procedure. 2.1.2 The R Markdown Structure We just have created the markdown file. Now it comes to understanding how to work on this thing. For that, we first need to understand the underlying structure. In general, R Markdown files are comprised of two parts: White Spaces and Grey Code Chunks White spaces are the spaces you are currently reading this text. They serve as string-based Chunks, meaning that you can write human language based sentences within these cells. As such, these white spaces serve as areas for writing results of reports, explanations, formulas or in general everything that has no code-based output at the end. Opposed to this, Code Chunk serve as fields in which you write your actual code. The two boxes we just added before are coding boxes. To use them, just follow th approach I described earlier. Code Chunks incorporate many nice features, which we will cover next. 2.1.2.1 The use of Code Chunks Code Chunks incorporate a number of handy features that can be defined quite easily. To set certain code chunk properties, we make use of the packages knitr. Maybe you realised this already, we used this package before to define the working directory source. Knitr, in general, is used to handle most of the R syntax and background, or administrative, tasks, R needs to do in order to operate. To set code chunk options, we just access the {r} field in the top row of each code chunk. Markdown offers quite a range of potential options to include, so make sure to check them out on this link. In general, the most used settings involve the following: eval = FALSE: Do not evaluate (or run) this code chunk when knitting the RMD document. The code in this chunk will still render in our knitted html echo = FALSE: Hide the code in the output. The code is evaluated when the Rmd file is knit, however only the output is rendered on the output document. results = hide: The code chunk will be evaluated but the results or the code will not be rendered on the output document. This is useful if you are viewing the structure of a large object (e.g. outputs of a large data.frame which is the equivalent of a spreadsheet in R). include = FALSE: runs the code, but doesn’t show the code or results in the final document. Use this for setup code that you don’t want cluttering your report. message = FALSE or warnings = FALSE: prevents messages or warnings from appearing in the finished file error = TRUE: causes the render to continue even if code returns an error. This is rarely something you’ll want to include in the final version of your report, but can be very useful if you need to debug exactly what is going on inside your Rmd. tidy = TRUE: tidy code for display cache = TRUE: When the cache is turned on, knitr will skip the execution of this code chunk if it has been executed before and nothing in the code chunk has changed since then. This is useful if you have very time-consuming codes. These would then be skipped if they haven’t been changed since the last rendering. comment=\"\": By default, R code output will have two hashes ## inserted in front of the text output. We can alter this behavior through the comment chunk option by e.g. stating an empty space. Furthermore, we can use specific options for figures to include in the files: fig.width =: Defines the figure width in the chunk fig.height =: Defines the figure height in the chunk fig.align =: Defines the alignment of the figure (left, center, right) fig.cap =; Defines the caption of the figure 2.1.2.2 The use of White Spaces White spaces are commonly used to define the text we write in a markdown file. Markdown white spaces have the handy feature that they follow the same syntax as Latex files. Consequently, it is very easy to write formulas, expressions, definitions or other features commonly used in Latex based tools or applications. However, since this is not a drag-and-drop interface, commonly used ways of working in other programs, such as Microsoft Word, do not work here. But since we are working within an academic setting, we should definitively foster the use of non Latex-based writing. We also call the white space writing Pandoc’s Markdown. The most important features of it are the following: Structure of an Rmd File To structure different markdown files, we need to define headers that indicate in which part of the file we currently are. We define headers by using the # sign. We have, in total, six headers available, that increase in specification with increasing amount of #: #: Header 1 ##: Header 2 ###: Header 3 ####: Header 4 #####: Header 5 ######: Header 6 Different Font Styles In general, we use three different styles to design the font: textbf{} or ** **: This is to indicate a bold text (write either in brackets or between the arcteryx) textit{} or * *: This is to indicate an italic text underline{}: This is to indicate an underlined text verbatim: This is to indicate the verbatim text - it is a text in this grey box which is more strongly visible Numbering and Lists We can define lists with several properties: *: This will render an item in an unordered list +: This will render a sub-item in an unordered list -: This will render a sub-sub-item in an unordered list 1.: This will render an item in an ordered list i): This will render a sub-item in an ordered list A.: This will render a sub-sub-item in an ordered list Jumping and Linking [link](name): This will render a link to a specific URL code Jump to [Header 1](#anchor): This will jump to a certain part in the Rmd. In this case, Header 1 Images and how to include them ![Caption](File/to/png/file): This will include the image from a given file path on your computer and add a caption \\(\\textbf{Showing output in markdown file}\\) $$ $$: This forms a code block in which math mode or other modes can be used (just like with latex). Neatly, the output is shown immediately $ $: This is the same as the double-dollar-sign, but you can use it in a sentence just to write some mathematical notion in a string-based text Mathematical Symbols Rmd offers a great range of different mathematical symbols and equations. They follow the same syntax as in latex files. To get a good overview, just visit the following link. Importantly, if you want to use lower- or upper subscripted letters, such as in mathematical notations, you need to use x_t or x^t to do so. Note that, if you have more than one letter or one mathematical symbol, these subscripts must always be in {}. That is: x_{t,p} or x^{t,p}. Equations To create equations, we simply use the following syntaxes: If we only want to use an easy mathematical equation, we can use the following: \\[ p(x) = \\sum_{w_i \\in E} p_i \\] If we want to use more complex mathematical symbols, we need to use the \\begin{equation} ... \\end{equation} command: \\[ \\begin{equation} \\textbf{C} = \\left( \\begin{matrix} 1 &amp; \\rho_{12} &amp; \\dots &amp; \\rho_{1n} \\\\ \\rho_{12} &amp; 1 &amp; \\dots &amp; \\rho_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\rho_{1n} &amp; \\rho_{2n} &amp; \\dots &amp; 1 \\end{matrix} \\right) \\end{equation} \\] Lastly, if we have proofs to write or want an order to show many calculation steps within a formula, then we need to use the \\begin{align} ... \\end{align} command: \\[ \\begin{align} var(aX + bY) &amp;= E[(aX + bY - E[aX]E[bY])^2]\\\\ &amp;= E[((aX - E(aX)) + (bY - E(bY)))^2] \\\\ &amp;= E[(a(X-E(X)) + b(Y-E(Y)))^2] \\\\ &amp;= a^2E[X-E(X)]^2 + b^2(Y-E(Y))^2 + 2 ab(X-E(X))(Y-E(Y))\\\\ &amp;= a^2\\cdot var(X) + b^2\\cdot var(Y) + 2\\cdot a \\cdot b \\cdot cov(X,Y) \\end{align} \\] 2.2 Coding In R - A concise overview Now that we understand the most important concepts of creating and maintaining the programming environment, it is now time to actually start programming. While we are doing this in this file, I want to emphasize that much of the content is taken by Derek L. Sonderegger, who created an awesome bookdown book in which he explains the most important first steps of using R very concisely. We will cover most of what he’s written in this Markdown. But, if you have any further questions, you can also stick to his files. Note that we do not expect you to understand everything that is written here. However, it should serve as a guideline to show you what is all available for R and what you could use R for. Furthermore, we will introduce some of the most important concepts of his work within our own domain. 2.2.1 Introduction In the first chapter, we cover the basics of R. R is a open-source program that is commonly used in Statistics. It runs on almost every platform and is completely free and is available at www.r-project.org. Most of the cutting-edge statistical research is first available on R. Finding help about a certain function is very easy. At the prompt, just type help(function.name) or ?function.name. If you don’t know the name of the function, your best bet is to go the the web page which will search various R resources for your keyword(s). Another great resource is the coding question and answer site stackoverflow. 2.2.2 Vectors 2.2.2.1 Creating Vectors R operates on vectors where we think of a vector as a collection of objects, usually numbers. In R, we can define vectors with three functions. Arbitrary Vectors The first thing we need to be able to do is define an arbitrary collection using the c() function. # Define a vector of numbers: a = c(1,2,3,4) str(a) ## num [1:4] 1 2 3 4 An important feature of vector creation is that we can repeat certain entries multiple times to get a vector of only those entries (instead of manually typing in the same number n times). We can do so by using the function rep(x, times), which just repeats x a the number times specified by times: Vectors with repeating elements # This repeats 2 and 3 five times rep(c(2,3), 5) ## [1] 2 3 2 3 2 3 2 3 2 3 Vectors with a sequence If we want to define a sequence of numbers, we can do so by using the function seq(from, to, by, length.out). # Define a sequence from 1 to 10 but only take each second element: seq(from=1, to=10, by=2) ## [1] 1 3 5 7 9 The by = argument is especially handy here, since it allows us to draw any arbitrary sequence. 2.2.2.2 Accessing Vector Elements # We create a vector first: vec &lt;- c(1,3,8,5,1,9) Accessing Single Elements We can access elements of vectors by calling a squared bracket around the vector item []. # Get the 3rd element vec[3] ## [1] 8 Accessing Multiple Elements As before, we can access multiple elements by using the c() function in the squared brackets # Get the second and fourth element vec[c(2,4)] ## [1] 3 5 Special Accessing If we only want to get a sequence, we do so with :. # Get elements 1 to 3 vec[1:3] ## [1] 1 3 8 If we want to get a sequence, we do so by using : as well as a comma separating the non-sequential gap. # Get elements 1 to 3 and 5. vec[c(1:3,5)] ## [1] 1 3 8 1 If we want everything but the first element, we use a - sign. # Get everything except the first element vec[-1] ## [1] 3 8 5 1 9 If we want everything except the first i elements, we use -1*c(1,i). # Get everything except the first three elements and the fifth element vec[-1*c(1:3, 5)] ## [1] 5 9 2.2.2.3 Scalar Functions and Vector Algebra We can perform operations on all elements of a vector simultaneously. These operations could be additive, multiplicative, exponentially, logarithmic, absolute etc. Pretty much any calculation you know from mathematics can be done using vectors. Although we introduce to you vector operations in Chapter 3.2.1 and 3.2.2 expect you to go over it yourself, let’s give you a quick introduction what we mean by working on vectors: # Define a vector x_vec &lt;- seq(-10, 10, 2) x_vec ## [1] -10 -8 -6 -4 -2 0 2 4 6 8 10 # Get the absolute values abs(x_vec) ## [1] 10 8 6 4 2 0 2 4 6 8 10 # Get divided by two values x_vec/2 ## [1] -5 -4 -3 -2 -1 0 1 2 3 4 5 # Get exponential values exp(x_vec) ## [1] 4.539993e-05 3.354626e-04 2.478752e-03 1.831564e-02 1.353353e-01 1.000000e+00 7.389056e+00 5.459815e+01 4.034288e+02 2.980958e+03 2.202647e+04 Furthermore, we can use vector algebra such as we can use matrix algebra in R. # Define another vector y_vec &lt;- seq(10,20,1) # Use vector additivity x_vec + y_vec ## [1] 0 3 6 9 12 15 18 21 24 27 30 Commonly used Vector functions There are some commonly used vector functions: ## Vector Function Description ## 1 min(x) Minimum value in vector x ## 2 max(x) Maximum value in vector x ## 3 length(x) Number of elements in vector x ## 4 sum(x) Sum of all the elements in vector x ## 5 mean(x) Mean of the elements in vector x ## 6 median(x) Median of the elements in vector x ## 7 var(x) Variance of the elements in vector x ## 8 sd(x) Standard deviation of the elements in x 2.2.3 Data Types R operates with five major data types: Integers, Numerics, Character Strings, Factors and Logicals. Integers These are the integer numbers. That is, they can only be full numbers without any decimal point. To convert a number to an integer, you use as.integer(). # Transform to integer as.integer(1.233) ## [1] 1 Numerics These could be any number (whole number or decimal). To convert another type to numeric you may use the function as.numeric(). as.numeric(1.20) ## [1] 1.2 Strings These are a collection of characters (example: Storing a student’s last name). To convert another type to a string, use as.character(). x &lt;- &quot;Hello World&quot; as.character(x) ## [1] &quot;Hello World&quot; Factors R handles categorical variables in terms of factors. R does this in a two step pattern. First it figures out how many categories there are and remembers which category an observation belongs two and second, it keeps a vector character strings that correspond to the names of each of the categories. This is done with the function factor(). z_vec &lt;- rep(c(&quot;A&quot;, &quot;B&quot;, &quot;D&quot;), 4) factor(z_vec) ## [1] A B D A B D A B D A B D ## Levels: A B D Often we wish to take a continuous numerical vector and transform it into a factor. The function cut() takes a vector of numerical data and creates a factor based on your give cut-points. # Divide x_vec into four groups of equal length: cut(x_vec, breaks=4) ## [1] (-10,-5] (-10,-5] (-10,-5] (-5,0] (-5,0] (-5,0] (0,5] (0,5] (5,10] (5,10] (5,10] ## Levels: (-10,-5] (-5,0] (0,5] (5,10] We can also set the break points individually # Set breaks individually cut(x_vec, breaks = c(-10, 0, 2.5, 5.0, 7.5, 10)) ## [1] &lt;NA&gt; (-10,0] (-10,0] (-10,0] (-10,0] (-10,0] (0,2.5] (2.5,5] (5,7.5] (7.5,10] (7.5,10] ## Levels: (-10,0] (0,2.5] (2.5,5] (5,7.5] (7.5,10] Logicals We can test vector levels in R. This is useful if we need to make a comparison and test if something is equal to something else, or if one thing is bigger than another. To test these, we will use the &lt;, &lt;= ==, &gt;=, &gt;, and != operators. # See which values are larger than or equal to zero x_vec &gt;= 0 ## [1] FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE TRUE # which vector elements are &gt; 0 which(x_vec &gt; 0) ## [1] 7 8 9 10 11 # Grab these x_vec[which(x_vec &gt; 0)] ## [1] 2 4 6 8 10 In order to make multiple comparisons we use the &amp; = and as well as | = or signs. # Let&#39;s make an and comparison: x_vec[which(x_vec &gt; 0 &amp; y_vec &gt; 5)] ## [1] 2 4 6 8 10 # Let&#39;s make an or comparison: x_vec[which(x_vec &gt; 0 | y_vec &gt; 5)] ## [1] -10 -8 -6 -4 -2 0 2 4 6 8 10 2.2.4 Data Frames Data Frames are the most common tool to display matrices within R. To generalize our concept of a matrix to include these types of data, R incorporates a structure called a data.frame. These are like simple Excel spreadsheet where each column represents a different trait or measurement type and each row will represent an individual. For instance, we create a data frame object like this: # create a data frame object df &lt;- data.frame( Type = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), Solution = c(&quot;9&quot;, &quot;10&quot;, &quot;4&quot;) ) df ## Type Solution ## 1 A 9 ## 2 B 10 ## 3 C 4 If we already have vectors that we may want to combine to a data frame, we can also create data frames by using the functions cbind() and rbind(). cbind binds vectors into columns and rbind binds vectors into rows. # Let&#39;s create a matrix and corresponding data frame with cbind() column_bind &lt;- cbind(x_vec, y_vec) df_cb &lt;- as.data.frame(column_bind) # Let&#39;s change the column names: colnames(df_cb) &lt;- c(&quot;X Vector&quot;, &quot;Y Vector&quot;) # Print it: df_cb ## X Vector Y Vector ## 1 -10 10 ## 2 -8 11 ## 3 -6 12 ## 4 -4 13 ## 5 -2 14 ## 6 0 15 ## 7 2 16 ## 8 4 17 ## 9 6 18 ## 10 8 19 ## 11 10 20 Because a data frame feels like a matrix, R also allows matrix notation for accessing particular values. # Let&#39;s grab the first row and column element: df_cb[1,1] ## [1] -10 # Grab all elements from the first column df_cb[,1] ## [1] -10 -8 -6 -4 -2 0 2 4 6 8 10 # Grab all elements except for the first three and fifth row df_cb[-1*c(1:3,5),] ## X Vector Y Vector ## 4 -4 13 ## 6 0 15 ## 7 2 16 ## 8 4 17 ## 9 6 18 ## 10 8 19 ## 11 10 20 # Grab all elements of the second column except for the first three and fifth row df_cb[-1*c(1:3,5), 2] ## [1] 13 15 16 17 18 19 20 Note that we need to adjust the access formula by separating rows and columns with a comma (“,”). In front of the coma, we define options for rows, and behind for columns. Although data frames are just generalised matrices, we still cover the underlying principles of matrices in Chapter 3.2 and expect you to understand this on your own. 2.2.5 Importing Data Now that we know more about how to handle data, we need to understand how to actually read in the data into R. It is most common for data to be in a data-frame like storage, such as a MS Excel workbook, so we will concentrate on reading data into a data.frame. 2.2.5.1 Working Directory As we have already discussed, the Working Directory is the source in which R will look for files that you need to upload. As we already covered, you can set a global working directory in R Markdown in the first grey code chunk that appears when opening a new file, with the command knitr::opts_knit$set(root.dir = \"/This/is/your/path/to/your/files/\"). To see where your working directory currently is, use the function getwd(). getwd() ## [1] &quot;/Users/nikolasanic/Desktop/Advanced_Empirical_Finance_UZH_2022&quot; If you need to temporarily adjust your working directory, you can do so by defining it new for any given code chunk. That is, you can define your new working directory when calling an individual path to the files you want to upload instead of taking the given path. For instance, if I want to just upload a csv file called first_return from the usual working directory, I do so by calling: read_csv(\"first_return.csv\"). However, if I need to get it from another source (let’s say from the folder “Empirical Codes”), I need to enter: read_csv(\"~/Desktop/Empirical_Codes/first_return.csv\"). As such, we need to define the source path which otherwise would automatically be assumed by R to be the given working directory. 2.2.5.2 Reading in Data In order to read in the data, we need to understand the data path, the data type as well as the separation properties of the data and the general structure of columns and rows. In general, this gives us three questions: What is the path of the data (is it on your laptop or external)? What is the separation character of the data (comma, semicolon etc.)? Is the first row column names or already data to work on? Common data formats The most common data formats that you will encounter are text, csv and Stata files. You read in text and csv files by calling the function read_csv(), and stata files by calling read_dta13(). Both reading functions have certain attributes you need to specify: ## Argument ## 1 file ## 2 header ## 3 sep ## 4 skip ## 5 na.strings ## Description ## 1 A character string denoting the file location ## 2 Is the first line column headers? ## 3 What character separates columns ## 4 The number of lines to skip before reading data. This is useful when there are lines of text that describe the data or aren’t actual data ## 5 What values represent missing data. Can have multiple. E.g. c(&#39;NA&#39;, -9999) Let’s now read in some data: # We read in some dataset with read.csv(&quot;~/Desktop/Master UZH/Data/A1_dataset_01_Ex_Session.txt&quot;, header = T, sep = &quot;\\t&quot;, dec = &#39;.&#39;) ## Date ABB Actelion Adecco Alcon Alusuisse Baloise Ciba_SC Ciba_Geigy_I Ciba_Geigy_PS Clariant Compagnie_Financiere_Richemont ## 1 1988-06-30 5.294 NA 155.845 NA NA NA NA 668.55 305.75 NA NA ## 2 1988-07-29 4.982 NA 157.232 NA NA NA NA 649.82 301.81 NA NA ## 3 1988-08-31 5.049 NA 162.319 NA NA NA NA 648.83 315.62 NA NA ## 4 1988-09-30 5.527 NA 171.476 NA NA NA NA 668.55 327.45 NA NA ## 5 1988-10-31 5.928 NA 175.730 NA NA NA NA 685.31 353.10 NA NA ## 6 1988-11-30 6.161 NA 171.080 NA NA NA NA 534.45 398.47 NA NA ## 7 1988-12-30 6.117 NA 168.004 NA NA NA NA 521.63 418.19 NA NA ## 8 1989-01-31 6.473 NA 153.299 NA NA NA NA 595.58 441.86 NA NA ## 9 1989-02-28 6.539 NA 149.935 NA NA NA NA 621.22 507.95 NA NA ## 10 1989-03-31 7.162 NA 146.475 NA NA NA NA 685.31 559.24 NA NA ## 11 1989-04-28 7.763 NA 158.777 NA NA NA NA 694.19 579.95 NA NA ## 12 1989-05-31 8.386 NA 154.741 NA NA NA NA 702.08 542.47 NA NA ## 13 1989-06-30 9.609 NA 160.988 NA NA NA NA 767.16 638.14 NA NA ## 14 1989-07-31 9.553 NA 163.391 NA NA NA NA 828.29 645.04 NA NA ## 15 1989-08-31 10.899 NA 170.119 NA NA NA NA 888.44 694.36 NA NA ## Credit_Suisse_Group Credit_Suisse_Holding Elektrowatt EMS_Chemie_Holding Geberit Georg_Fischer Givaudan Holcim Jacobs_Suchard Julius_Baer_Group ## 1 NA 76.45 296.70 NA NA NA NA 98.75 7890 NA ## 2 NA 77.05 294.23 NA NA NA NA 97.82 7700 NA ## 3 NA 75.54 286.81 NA NA NA NA 97.36 7500 NA ## 4 NA 80.83 279.89 NA NA 184.400 NA 98.29 7750 NA ## 5 NA 87.03 292.75 NA NA 179.771 NA 99.77 7680 NA ## 6 NA 81.28 274.94 NA NA 169.741 NA 95.04 6850 NA ## 7 NA 81.59 281.87 NA NA 189.030 NA 95.04 6815 NA ## 8 NA 84.46 291.76 NA NA 196.745 NA 94.11 7175 NA ## 9 NA 80.38 286.81 NA NA 208.318 NA 93.19 7260 NA ## 10 NA 87.48 288.79 NA NA 223.749 NA 94.02 7300 NA ## 11 NA 87.09 288.79 NA NA 216.034 NA 98.29 7370 NA ## 12 NA 82.58 274.94 NA NA 217.577 NA 104.78 6860 NA ## 13 NA 84.33 293.73 NA NA 243.038 NA 113.12 7025 NA ## 14 NA 96.75 308.57 NA NA NA NA 111.08 7330 NA ## 15 NA 98.32 301.65 NA NA NA NA 109.41 7490 NA ## Kudelski LafargeHolcim Lonza_Group Merck_Serono Nestle_I Nestle_PS Nobel_Biocare_Holding Novartis_I Novartis_N OC_Oerlikon_Corporation ## 1 NA NA NA NA 834.87 4.103 NA NA NA 17.294 ## 2 NA NA NA NA 817.19 4.006 NA NA NA 16.325 ## 3 NA NA NA NA 816.69 4.123 NA NA NA 19.715 ## 4 NA NA NA NA 859.42 4.133 NA NA NA 21.444 ## 5 NA NA NA NA 873.17 4.264 NA NA NA 20.960 ## 6 NA NA NA NA 675.26 5.829 NA NA NA 21.790 ## 7 NA NA NA NA 711.11 6.521 NA NA NA 25.456 ## 8 NA NA NA NA 721.91 6.618 NA NA NA 26.771 ## 9 NA NA NA NA 690.48 6.233 NA NA NA 25.595 ## 10 NA NA NA NA 715.04 6.452 NA NA NA 28.293 ## 11 NA NA NA NA 740.57 6.560 NA NA NA 27.324 ## 12 NA NA NA NA 697.36 6.209 NA NA NA 26.840 ## 13 NA NA NA NA 786.00 7.260 NA NA NA 27.808 ## 14 NA NA NA NA 839.00 8.002 NA NA NA 28.846 ## 15 NA NA NA NA 884.00 8.338 NA NA NA 28.500 ## Pargesa_Holding Partners_Group Roche_Holding SAirGroup Sandoz_PS Sandoz_N Schweizerische_Volksbank_StN Schweizerische_Volksbank_ST SGS Sika ## 1 30.672 NA 11.166 NA 467.50 NA 158.67 NA NA NA ## 2 31.068 NA 11.329 NA 491.85 NA 155.73 NA NA NA ## 3 31.661 NA 11.119 NA 478.21 NA 154.75 NA NA NA ## 4 33.541 NA 11.935 NA 481.13 NA 152.79 NA NA NA ## 5 32.057 NA 11.842 NA 479.19 NA NA NA NA NA ## 6 30.276 NA 11.655 NA 411.98 NA NA NA NA NA ## 7 32.255 NA 11.795 NA 373.02 NA NA NA NA NA ## 8 31.266 NA 12.844 NA 381.79 NA NA NA NA NA ## 9 30.672 NA 12.727 NA 384.71 NA NA NA NA NA ## 10 32.453 NA 13.753 NA 394.45 NA NA NA NA NA ## 11 33.640 NA 15.152 NA 441.20 NA NA NA NA NA ## 12 34.630 NA 15.105 NA 412.96 NA NA NA NA NA ## 13 34.176 NA 15.781 NA 467.50 NA NA 170.5 NA NA ## 14 36.774 NA 17.879 NA 467.50 NA NA 185.5 NA NA ## 15 35.675 NA 20.031 NA 498.66 NA NA 182.0 NA NA ## Societe_Internationale_Pirelli Sulzer Swiss_Bank_I Swiss_Bank_PS Swiss_Bank_N Swiss_Life_Holding_I Swiss_Life_Holding_N Swiss_Re Swissair ## 1 222.89 NA 351 292 NA NA NA 16.554 1068.24 ## 2 217.00 NA 380 299 NA NA NA 16.637 1125.22 ## 3 212.80 NA 357 291 NA NA NA 16.802 1082.49 ## 4 207.75 NA 375 303 NA NA NA 16.940 1101.48 ## 5 224.57 NA 391 313 NA NA NA 17.518 1125.22 ## 6 209.43 NA 363 296 NA NA NA 19.006 1035.01 ## 7 212.80 NA 339 292 NA NA NA 20.493 1025.51 ## 8 222.89 NA 338 296 NA NA NA 19.667 968.54 ## 9 233.83 NA 316 274 NA NA NA 19.832 978.04 ## 10 231.30 NA 323 292 NA NA NA 21.044 1044.50 ## 11 244.76 NA 320 284 NA NA NA 22.793 987.53 ## 12 262.42 NA 292 265 NA NA NA 20.590 1013.93 ## 13 317.10 NA 336 282 NA NA NA 21.210 1057.81 ## 14 338.12 NA 372 303 NA NA NA 24.928 1145.55 ## 15 344.85 NA 368 307 NA NA NA 24.446 1403.91 ## Swisscom Syngenta Synthes The_Swatch_Group_I The_Swatch_Group_N Transocean UBS_N UBS_PS UBS_I Winterthur Zurich_Insurance_Group_I ## 1 NA NA NA NA NA NA NA 112.5 610.87 955.02 1406.93 ## 2 NA NA NA NA NA NA NA 116.0 648.19 972.96 1397.30 ## 3 NA NA NA NA NA NA NA 113.5 624.61 937.09 1319.00 ## 4 NA NA NA NA NA NA NA 114.5 639.35 1008.82 1373.21 ## 5 NA NA NA NA NA NA NA 123.0 675.68 998.96 1409.34 ## 6 NA NA NA NA NA NA NA 114.0 613.81 824.99 1144.34 ## 7 NA NA NA NA NA NA NA 114.5 628.54 755.05 1038.34 ## 8 NA NA NA NA NA NA NA 121.0 642.29 744.29 1068.45 ## 9 NA NA NA NA NA NA NA 112.0 599.08 735.32 1046.77 ## 10 NA NA NA NA NA NA NA 117.5 634.44 772.98 1150.36 ## 11 NA NA NA NA NA NA NA 116.0 636.40 842.93 1204.57 ## 12 NA NA NA NA NA NA NA 113.5 599.08 780.16 1088.93 ## 13 NA NA NA NA NA NA NA 117.0 654.08 802.58 1148.65 ## 14 NA NA NA NA NA NA NA 128.5 750.32 936.10 1371.09 ## 15 NA NA NA NA NA NA NA 137.5 795.50 950.50 1386.89 ## Zurich_Insurance_Group_N ## 1 NA ## 2 NA ## 3 NA ## 4 NA ## 5 NA ## 6 NA ## 7 NA ## 8 NA ## 9 NA ## 10 NA ## 11 NA ## 12 NA ## 13 NA ## 14 NA ## 15 NA ## [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 378 rows ] # Here: ## file = &quot;~/Desktop/Master UZH/Data/A1_dataset_01.txt&quot; --&gt; an own defined path different from the current working directory ## header = T --&gt; the first column is a header ## sep = &quot;\\t&quot; --&gt; the separation is a \\t sign ## dec = &quot;.&quot; --&gt; decimals should be separated with a dot 2.2.6 Data Manipulation In order to explore and modify our data, we use data manipulation techniques. Most of the time, our data is in the form of a data frame and we are interested in exploring the relationships. In order to effectively manipulate the data, we can work with the function dplyr. Although it requires you to think a little differently about the code and how it is constructed, it is a very handy feature to explore your data. 2.2.6.1 Classical functions We first start with more classical functions. Summary summary() is to calculate some basic summary statistics (minimum, 25th, 50th, 75th percentiles, maximum and mean) of each column. If a column is categorical, the summary function will return the number of observations in each category. # Let&#39;s use the dataset from before: df_1 &lt;- read.csv(&quot;~/Desktop/Master UZH/Data/A1_dataset_01_Ex_Session.txt&quot;, header = T, sep = &quot;\\t&quot;, dec = &#39;.&#39;) # We just cut it quickly such that it is better visible df_1 &lt;- df_1[,2:7] # Create a summary summary(df_1) ## ABB Actelion Adecco Alcon Alusuisse Baloise ## Min. : 1.458 Min. : 29.45 Min. : 8.766 Min. :45.10 Min. : 142.7 Min. : 31.60 ## 1st Qu.:10.778 1st Qu.: 45.59 1st Qu.: 51.257 1st Qu.:54.85 1st Qu.: 234.3 1st Qu.: 59.15 ## Median :18.060 Median : 57.37 Median : 61.820 Median :58.39 Median : 416.6 Median : 94.50 ## Mean :17.048 Mean : 84.13 Mean : 68.789 Mean :57.05 Mean : 479.2 Mean : 96.42 ## 3rd Qu.:21.800 3rd Qu.:111.77 3rd Qu.: 75.948 3rd Qu.:58.94 3rd Qu.: 732.0 3rd Qu.:124.77 ## Max. :38.775 Max. :274.75 Max. :175.730 Max. :64.00 Max. :1081.1 Max. :180.81 ## NA&#39;s :289 NA&#39;s :72 NA&#39;s :370 NA&#39;s :276 NA&#39;s :235 Apply apply() is commonly used if we want the ability to pick another function to apply to each column and possibly to each row. To demonstrate this, let’s say we want the mean value of each column for the given data frame. # Summarize each column by calculating the mean. apply(df_1, # Set the dataframe you want to apply something on MARGIN = 2, # Set whether rows or cols should be accessed. # rows = 1, columns = 2, (same order as [rows, cols] FUN = mean, # Set which function should be accessed na.rm = T) # Set whether NA values should be ignored ## ABB Actelion Adecco Alcon Alusuisse Baloise ## 17.04789 84.12846 68.78897 57.05391 479.15547 96.41629 This gives us the mean value of each company’s prices. If we want to get the standard deviation of prices on the first ten dates, then we just need to change MARGIN == 1. # Summarize each column by calculating the mean. apply(df_1[1:10,], # Set the dataframe you want to apply something on MARGIN = 1, # Set whether rows or cols should be accessed. # rows = 1, columns = 2, (same order as [rows, cols] FUN = sd, # Set which function should be accessed na.rm = T) # Set whether NA values should be ignored ## 1 2 3 4 5 6 7 8 9 10 ## 106.45563 107.65701 111.20668 117.34366 120.06815 116.61534 114.47140 103.82166 101.39628 98.50917 2.2.6.2 Package dplyr The package dplyr strives to provide a convenient and consistent set of functions to handle the most common data frame manipulations and a mechanism for chaining these operations together to perform complex tasks. The most important operator using the deployer (= dplyr) package is the pipe command %&gt;%. It allows for better or easier readable code. The idea is that the %&gt;% operator works by translating the command a %&gt;% f(b) to the expression f(a,b). The handy feature arises once you have multiple functions you need to incorporate into each other. For example, if we wanted to start with x, and first apply function f(), then g(), and then h(), the usual R command would be h(g(f(x))) which is hard to read because you have to start reading at the innermost set of parentheses. Using the pipe command %&gt;%, this sequence of operations becomes x %&gt;% f() %&gt;% g() %&gt;% h(). In dplyr, all the functions below take a data set as its first argument and outputs an appropriately modified data set. This will allow me to chain together commands in a readable fashion. Verbs The foundational operations to perform on a data set are: – select: Selecting a subset of columns by name or column number. – filter: Selecting a subset of rows from a data frame based on logical expressions. – slice: Selecting a subset of rows by row number. arrange: Re-ordering the rows of a data frame. mutate: Add a new column that is some function of other columns. summarise: Calculate some summary statistic of a column of data. This collapses a set of rows into a single row. Let’s apply these commands now in the sliced Swiss Markets dataset of earlier. select We already understand how we can simply select columns by using square brackets [, col], where col is (are) the column(s) of interest. Instead, we can also use select in order to select the columns we want: # First, let&#39;s select the first three columns. To not exagerate the output, let&#39;s only take the first five rows df_1[1:5,] %&gt;% select(c(1:3)) ## ABB Actelion Adecco ## 1 5.294 NA 155.845 ## 2 4.982 NA 157.232 ## 3 5.049 NA 162.319 ## 4 5.527 NA 171.476 ## 5 5.928 NA 175.730 # Now, we can also write the names of the columns instead of their indexes df_1[1:5,] %&gt;% select(c(ABB:Adecco)) ## ABB Actelion Adecco ## 1 5.294 NA 155.845 ## 2 4.982 NA 157.232 ## 3 5.049 NA 162.319 ## 4 5.527 NA 171.476 ## 5 5.928 NA 175.730 # Also, not selecting works as well. For instance, get all except the second column. df_1[1:5,] %&gt;% select(-c(2)) ## ABB Adecco Alcon Alusuisse Baloise ## 1 5.294 155.845 NA NA NA ## 2 4.982 157.232 NA NA NA ## 3 5.049 162.319 NA NA NA ## 4 5.527 171.476 NA NA NA ## 5 5.928 175.730 NA NA NA The select() command has a few other tricks. There are functional calls that describe the columns you wish to select that take advantage of pattern matching. The most commonly used are: starts_with(): Show columns that start with a certain pattern (e.g. Letter, Name) ends_with(): Show columns that end with a certain pattern contains(): Show columns containing a certain pattern matches(): Show columns with a regular expression df_1[1:5,] %&gt;% select(starts_with(&quot;B&quot;)) ## Baloise ## 1 NA ## 2 NA ## 3 NA ## 4 NA ## 5 NA df_1[1:5,] %&gt;% select(ends_with(&quot;B&quot;)) ## ABB ## 1 5.294 ## 2 4.982 ## 3 5.049 ## 4 5.527 ## 5 5.928 df_1[1:5,] %&gt;% select(contains(&quot;B&quot;)) ## ABB Baloise ## 1 5.294 NA ## 2 4.982 NA ## 3 5.049 NA ## 4 5.527 NA ## 5 5.928 NA filter It is common to want to select particular rows where we have some logically expression to pick the rows. # Let&#39;s only select the rows in which ABB has a larger Share Price than 30 df_1 %&gt;% filter(., ABB &gt; 30) ## ABB Actelion Adecco Alcon Alusuisse Baloise ## 1 36.045 NA 122.465 NA 1078.38 124.621 ## 2 33.778 NA 122.564 NA 1081.14 129.793 ## 3 32.852 NA 130.366 NA 936.93 129.296 ## 4 35.351 NA 113.576 NA 964.48 142.623 ## 5 35.444 NA 139.748 NA 1001.22 142.922 ## 6 38.775 NA 131.551 NA 981.02 157.343 ## 7 36.138 NA 136.884 NA 976.42 161.123 ## 8 36.832 NA 134.711 NA 1054.50 172.560 ## 9 36.091 NA 131.847 NA 1061.85 175.842 ## 10 31.094 NA 110.613 NA 946.11 167.090 ## 11 31.973 NA 100.737 NA NA 177.036 ## 12 30.169 NA 108.638 NA NA 176.041 ## 13 32.512 NA 68.788 NA NA 123.100 ## 14 30.963 NA 62.319 NA NA 109.300 ## 15 30.440 NA 60.492 NA NA 111.500 ## 16 31.597 NA 58.714 NA NA 118.600 We can also use the &amp; and | operators to combine filtering. # Select only rows in which ABB is above 30 and Adecco below 90 df_1 %&gt;% filter(&quot;ABB&quot; &gt; 30 &amp; &quot;Adecco&quot; &lt; 90) ## [1] ABB Actelion Adecco Alcon Alusuisse Baloise ## &lt;0 rows&gt; (or 0-length row.names) We can also now start combining the select and filter commands. # Select only ABB, Adecco and Baloise and then filter such that ABB is above 30 and Adecco below 90 df_1 %&gt;% select(ABB, Adecco, Baloise) %&gt;% filter(ABB &gt; 30 &amp; Adecco &lt; 90) ## ABB Adecco Baloise ## 1 32.512 68.788 123.1 ## 2 30.963 62.319 109.3 ## 3 30.440 60.492 111.5 ## 4 31.597 58.714 118.6 slice When you want to filter rows based on row number, this is called slicing. This is the same as with columns, just for rows # Take only the first three rows df_1 %&gt;% slice(1:3) ## ABB Actelion Adecco Alcon Alusuisse Baloise ## 1 5.294 NA 155.845 NA NA NA ## 2 4.982 NA 157.232 NA NA NA ## 3 5.049 NA 162.319 NA NA NA arrange We often need to re-order the rows of a data frame. Although this is, in a security setting with a time-series, not the most useful tool (given that anyway anything is sorted according to date), it still is very useful for many other applications. # Arrange according to the Share Prices of ABB df_1[1:5,] %&gt;% arrange(ABB) ## ABB Actelion Adecco Alcon Alusuisse Baloise ## 1 4.982 NA 157.232 NA NA NA ## 2 5.049 NA 162.319 NA NA NA ## 3 5.294 NA 155.845 NA NA NA ## 4 5.527 NA 171.476 NA NA NA ## 5 5.928 NA 175.730 NA NA NA Although this tells us not much in the given context, it stil is useful for other sorting purposes. For that, let’s create another small example. How about treatment level and then gender. That would make more sense to have an overview then. # Create a dataframe ## Note that treat is a factor and thus must be assigned a level to show which values are &quot;lower&quot; than others. treatment &lt;- data.frame(treat = factor(c(&quot;Low&quot;, &quot;Med&quot;, &quot;Med&quot;, &quot;Low&quot;, &quot;High&quot;, &quot;High&quot;, &quot;Extreme&quot;), levels = c(&quot;Low&quot;, &quot;Med&quot;, &quot;High&quot;, &quot;Extreme&quot;)), gender = factor(c(&quot;M&quot;, &quot;F&quot;, &quot;X&quot;, &quot;M&quot;, &quot;M&quot;,&quot;F&quot;, &quot;F&quot;), levels = c(&quot;F&quot;, &quot;M&quot;, &quot;X&quot;)), age = c(41,40,88,14,10,55,31), y = c(3,4,5,1,2,7,4)) # Now arrange / sort the dataframe according to multiple rows: treatment %&gt;% arrange(treat, gender, desc(age)) ## treat gender age y ## 1 Low M 41 3 ## 2 Low M 14 1 ## 3 Med F 40 4 ## 4 Med X 88 5 ## 5 High F 55 7 ## 6 High M 10 2 ## 7 Extreme F 31 4 Great, now we have a better overview, as we sorted first according to treatment, then within treatment according gender, and then within gender according to age (descending). This “layered sorting” is quite handy when using the arrange function. mutate The perhaps most important function is mutate. It is used to create a new column according to functions used from other variables. To do so, let’s show you the two different ways to create new variables. # Let&#39;s do it the old-fashioned way. We define a high indicator each time ABB is larger its average df_1$High_Indicator &lt;- ifelse(df_1$ABB &gt; mean(df_1$ABB, na.rm = T), 1, 0) df_1$High_Indicator ## [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [72] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## [143] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [214] 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 ## [285] 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## [356] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 # Let&#39;s delete the column again: df_1$High_Indicator &lt;- NULL # Now, the dplyr way. df_1[1:5,] %&gt;% mutate(High_Indicator = ifelse(ABB &gt; mean(ABB, na.rm = T), 1,0)) ## ABB Actelion Adecco Alcon Alusuisse Baloise High_Indicator ## 1 5.294 NA 155.845 NA NA NA 0 ## 2 4.982 NA 157.232 NA NA NA 0 ## 3 5.049 NA 162.319 NA NA NA 0 ## 4 5.527 NA 171.476 NA NA NA 1 ## 5 5.928 NA 175.730 NA NA NA 1 The nice thing is that you can also use it to create multiple columns simultaneously, and you can even refer to columns that were created in the same mutate() command. # Let&#39;s do this again. Let&#39;s create the High Indicator and create a High times Price element df_1 %&gt;% mutate(High_Indicator = ifelse(ABB &gt; mean(ABB, na.rm = T), 1, 0), High_Price = High_Indicator * ABB) ## ABB Actelion Adecco Alcon Alusuisse Baloise High_Indicator High_Price ## 1 5.294 NA 155.845 NA NA NA 0 0.000 ## 2 4.982 NA 157.232 NA NA NA 0 0.000 ## 3 5.049 NA 162.319 NA NA NA 0 0.000 ## 4 5.527 NA 171.476 NA NA NA 0 0.000 ## 5 5.928 NA 175.730 NA NA NA 0 0.000 ## 6 6.161 NA 171.080 NA NA NA 0 0.000 ## 7 6.117 NA 168.004 NA NA NA 0 0.000 ## 8 6.473 NA 153.299 NA NA NA 0 0.000 ## 9 6.539 NA 149.935 NA NA NA 0 0.000 ## 10 7.162 NA 146.475 NA NA NA 0 0.000 ## 11 7.763 NA 158.777 NA NA NA 0 0.000 ## 12 8.386 NA 154.741 NA NA NA 0 0.000 ## 13 9.609 NA 160.988 NA NA NA 0 0.000 ## 14 9.553 NA 163.391 NA NA NA 0 0.000 ## 15 10.899 NA 170.119 NA NA NA 0 0.000 ## 16 10.788 NA 173.963 NA NA NA 0 0.000 ## 17 11.411 NA 153.299 NA NA NA 0 0.000 ## 18 11.455 NA 163.391 NA NA NA 0 0.000 ## 19 11.411 NA 150.174 NA NA NA 0 0.000 ## 20 12.011 NA 129.901 NA NA NA 0 0.000 ## 21 12.389 NA 109.252 NA NA NA 0 0.000 ## 22 11.733 NA 116.385 NA NA NA 0 0.000 ## 23 12.345 NA 103.996 NA NA NA 0 0.000 ## 24 13.567 NA 125.020 NA NA NA 0 0.000 ## 25 13.288 NA 118.262 NA NA NA 0 0.000 ## 26 13.779 NA 108.596 NA NA NA 0 0.000 ## 27 11.233 NA 87.653 NA NA NA 0 0.000 ## 28 8.777 NA 72.527 NA NA NA 0 0.000 ## 29 9.849 NA 70.588 NA NA NA 0 0.000 ## 30 8.844 NA 69.036 NA NA NA 0 0.000 ## 31 8.665 NA 69.036 NA NA NA 0 0.000 ## 32 8.866 NA 61.667 NA 193.45 NA 0 0.000 ## 33 10.519 NA 75.242 NA 234.28 NA 0 0.000 ## 34 10.608 NA 68.261 NA 245.03 NA 0 0.000 ## 35 9.916 NA 65.934 NA 229.98 NA 0 0.000 ## 36 10.385 NA 61.279 NA 232.13 NA 0 0.000 ## 37 9.938 NA 62.831 NA 232.13 NA 0 0.000 ## 38 10.742 NA 70.373 NA 223.97 NA 0 0.000 ## 39 10.452 NA 63.580 NA 206.34 NA 0 0.000 ## 40 9.849 NA 62.396 NA 202.04 NA 0 0.000 ## 41 9.112 NA 63.186 NA 193.45 NA 0 0.000 ## 42 7.169 NA 35.226 NA 169.80 NA 0 0.000 ## 43 7.258 NA 31.514 NA 171.09 NA 0 0.000 ## 44 8.017 NA 34.673 NA 174.10 NA 0 0.000 ## 45 8.218 NA 36.569 NA 182.70 NA 0 0.000 ## 46 8.576 NA 32.699 NA 197.74 NA 0 0.000 ## 47 9.067 NA 31.751 NA 204.62 NA 0 0.000 ## 48 9.670 NA 29.618 NA 214.94 NA 0 0.000 ## 49 9.469 NA 29.223 NA 206.34 NA 0 0.000 ## 50 8.375 NA 17.534 NA 182.27 NA 0 0.000 ## 51 8.129 NA 17.218 NA 167.22 NA 0 0.000 ## 52 8.218 NA 18.166 NA 158.20 NA 0 0.000 ## 53 7.749 NA 14.849 NA 142.72 NA 0 0.000 ## 54 7.660 NA 15.638 NA 144.01 NA 0 0.000 ## 55 8.017 NA 15.480 NA 176.25 NA 0 0.000 ## 56 8.017 NA 12.400 NA 188.72 NA 0 0.000 ## 57 8.955 NA 11.215 NA 202.90 NA 0 0.000 ## 58 9.156 NA 8.925 NA 210.64 NA 0 0.000 ## 59 9.223 NA 8.766 NA 200.32 NA 0 0.000 ## 60 9.346 NA 11.430 NA 207.18 NA 0 0.000 ## 61 9.034 NA 12.375 NA 209.81 NA 0 0.000 ## 62 10.005 NA 12.891 NA 233.46 NA 0 0.000 ## 63 10.161 NA 15.039 NA 237.84 NA 0 0.000 ## 64 9.916 NA 11.860 NA 235.21 NA 0 0.000 ## 65 11.189 NA 13.235 NA 236.53 NA 0 0.000 ## 66 11.245 NA 14.438 NA 235.65 NA 0 0.000 ## 67 12.127 NA 18.567 NA 282.96 NA 0 0.000 ## 68 12.808 NA NA NA 277.26 NA 0 0.000 ## 69 11.725 NA NA NA 281.64 NA 0 0.000 ## 70 13.087 NA NA NA 282.60 NA 0 0.000 ## 71 14.405 NA NA NA 309.94 NA 0 0.000 ## 72 14.070 NA NA NA 300.83 NA 0 0.000 ## 73 13.109 NA NA NA 293.54 NA 0 0.000 ## 74 13.724 NA NA NA 319.97 NA 0 0.000 ## 75 13.478 NA NA NA 315.41 NA 0 0.000 ## 76 12.395 NA NA NA 307.21 NA 0 0.000 ## 77 12.037 NA NA NA 285.33 NA 0 0.000 ## 78 12.428 NA NA NA 296.73 NA 0 0.000 ## 79 12.585 NA NA NA 298.55 NA 0 0.000 ## 80 12.473 NA NA NA 302.20 NA 0 0.000 ## 81 12.026 NA NA NA 299.00 NA 0 0.000 ## 82 12.015 NA NA NA 279.86 NA 0 0.000 ## 83 12.629 NA NA NA 288.98 NA 0 0.000 ## 84 13.299 NA NA NA 313.59 NA 0 0.000 ## 85 13.310 NA NA NA 329.09 NA 0 0.000 ## 86 13.523 NA NA NA 349.14 NA 0 0.000 ## 87 14.192 NA NA NA 379.68 NA 0 0.000 ## 88 14.952 NA NA NA 390.62 NA 0 0.000 ## 89 14.706 NA NA NA 394.27 NA 0 0.000 ## 90 15.075 NA NA NA 434.83 NA 0 0.000 ## 91 14.963 NA NA NA 416.60 NA 0 0.000 ## 92 15.577 NA NA NA 429.36 NA 0 0.000 ## 93 16.035 NA NA NA 438.94 NA 0 0.000 ## 94 16.158 NA NA NA 455.34 NA 0 0.000 ## 95 16.705 NA NA NA 457.17 NA 0 0.000 ## 96 16.705 NA NA NA 448.05 NA 0 0.000 ## 97 17.286 NA NA NA 470.84 NA 1 17.286 ## 98 15.834 NA NA NA 420.25 39.246 0 0.000 ## 99 16.515 NA NA NA 428.91 41.699 0 0.000 ## 100 17.129 NA NA NA 428.91 42.925 1 17.129 ## 101 17.442 NA NA NA 420.25 43.170 1 17.442 ## 102 18.212 NA NA NA 474.94 47.586 1 18.212 ## 103 18.592 NA NA NA 486.34 43.988 1 18.592 ## 104 20.323 NA NA NA 532.83 45.787 1 20.323 ## 105 18.715 NA NA NA 540.12 47.422 1 18.715 ## 106 19.318 NA NA NA 554.25 48.076 1 19.318 ## 107 19.932 NA NA NA 569.75 50.856 1 19.932 ## 108 21.685 NA NA NA 608.49 50.856 1 21.685 ## 109 24.678 NA NA NA 689.17 56.907 1 24.678 ## 110 24.537 NA NA NA 626.73 60.341 1 24.537 ## 111 25.039 NA NA NA 583.42 63.284 1 25.039 ## 112 24.457 NA NA NA 649.52 74.981 1 24.457 ## 113 20.838 NA NA NA 572.03 83.979 1 20.838 ## 114 21.660 NA NA NA 596.64 84.645 1 21.660 ## 115 20.952 NA NA NA 639.49 90.077 1 20.952 ## 116 21.466 NA NA NA 735.66 94.476 1 21.466 ## 117 22.836 NA NA NA 772.58 106.473 1 22.836 ## 118 26.010 NA NA NA 843.23 103.807 1 26.010 ## 119 28.088 NA NA NA 875.14 115.771 1 28.088 ## 120 28.659 NA NA NA 907.04 117.470 1 28.659 ## 121 25.576 NA NA NA 888.52 123.527 1 25.576 ## 122 25.222 NA NA NA 842.34 148.591 1 25.222 ## 123 19.239 NA NA NA 676.55 109.802 1 19.239 ## 124 16.042 NA NA NA 609.13 84.042 0 0.000 ## 125 18.520 NA NA NA 714.88 111.194 1 18.520 ## [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 268 rows ] Another situation I often run into is the need to select many columns, and calculate a sum or mean across them. This can be used when combining the mutate function with the apply function we discussed earlier. # Here, we only take the mean values of ABB, Actelion and Adecco and create another variable called Average_Price_AAAD df_1[1:5,] %&gt;% mutate(Average_Price_AAAD = select(., ABB:Adecco) %&gt;% apply(1, mean, na.rm = T)) ## ABB Actelion Adecco Alcon Alusuisse Baloise Average_Price_AAAD ## 1 5.294 NA 155.845 NA NA NA 80.5695 ## 2 4.982 NA 157.232 NA NA NA 81.1070 ## 3 5.049 NA 162.319 NA NA NA 83.6840 ## 4 5.527 NA 171.476 NA NA NA 88.5015 ## 5 5.928 NA 175.730 NA NA NA 90.8290 summarise summarise is used to calculate summary statistics using any or all of the data columns. # Let&#39;s get the mean and standard deviation for ABB df_1 %&gt;% summarise(ABB_Mean = mean(ABB), ABB_SD = sd(ABB)) ## ABB_Mean ABB_SD ## 1 17.04789 7.376436 We can also use summarise_all to get the summary statistics of each variable we selected # Here, we take the variables of ABB and Actelion and calculate their mean, variances and standard deviation df_1 %&gt;% select(.,ABB:Actelion) %&gt;% summarise_all(funs(mean, var, sd), na.rm = T) ## ABB_mean Actelion_mean ABB_var Actelion_var ABB_sd Actelion_sd ## 1 17.04789 84.12846 54.41181 3045.52 7.376436 55.18623 2.2.6.3 Using multiple commands including split, apply and combine functions The major strength of the dplyr package is the ability to split a data frame into a bunch of sub-data frames, apply a sequence of one or more of the operations we just described, and then combine results back together. Thus, let’s now look at how to combine some of the commands. # Let&#39;s create a slightly more advanced data frame than we had previously by looking at the treatment data set again and enhancing it treatment &lt;- data.frame(treat = factor(c(&quot;Low&quot;, &quot;Med&quot;, &quot;Med&quot;, &quot;Low&quot;, &quot;High&quot;, &quot;High&quot;, &quot;Extreme&quot;, &quot;Med&quot;, &quot;High&quot;, &quot;Low&quot;, &quot;Low&quot;, &quot;Med&quot;), levels = c(&quot;Low&quot;, &quot;Med&quot;, &quot;High&quot;, &quot;Extreme&quot;)), gender = factor(c(&quot;M&quot;, &quot;F&quot;, &quot;X&quot;, &quot;M&quot;, &quot;M&quot;,&quot;F&quot;, &quot;F&quot;, &quot;X&quot;, &quot;F&quot;, &quot;M&quot;, &quot;M&quot;, &quot;F&quot;), levels = c(&quot;F&quot;, &quot;M&quot;, &quot;X&quot;)), age = c(41,40,88,14,10,55,31, 55, 23, 46, 97, 12), glyphs = c(3,4,5,1,2,7,4,5,1,12,8,6), cells = factor(c(&quot;W&quot;, &quot;W&quot;, &quot;R&quot;, &quot;W&quot;, &quot;R&quot;, &quot;W&quot;,&quot;W&quot;, &quot;W&quot;, &quot;R&quot;, &quot;W&quot;, &quot;R&quot;, &quot;W&quot;), levels = c(&quot;R&quot;, &quot;W&quot;)), smoke = c(&quot;Y&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;Y&quot;, &quot;N&quot;, &quot;Y&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;Y&quot;, &quot;N&quot;)) # Let&#39;s do the following now: ## 1) Select all except for the &quot;glyphs&quot; column ## 2) Filter according to male and female individuals without extreme treatment status ## 3) Group by cells ## 4) Calculate the mean age for each group ## 5) Calculate smoking average age levels treatment %&gt;% select(-glyphs) %&gt;% filter(treat != &quot;Extreme&quot; &amp; gender != &quot;X&quot;) %&gt;% group_by(cells) %&gt;% mutate(mean_age = mean(age)) %&gt;% mutate(smoke_mean_age = ifelse(smoke == &quot;Y&quot;, mean_age, &quot;NA&quot;)) ## # A tibble: 9 × 7 ## # Groups: cells [2] ## treat gender age cells smoke mean_age smoke_mean_age ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Low M 41 W Y 34.7 34.6666666666667 ## 2 Med F 40 W N 34.7 NA ## 3 Low M 14 W N 34.7 NA ## 4 High M 10 R Y 43.3 43.3333333333333 ## 5 High F 55 W N 34.7 NA ## 6 High F 23 R N 43.3 NA ## 7 Low M 46 W N 34.7 NA ## 8 Low M 97 R Y 43.3 43.3333333333333 ## 9 Med F 12 W N 34.7 NA 2.2.7 Data Reshaping Another important feature in R is Data Reshaping. The idea behind data reshaping is the ability to shape the data frame between a wide and a long format. Long format is a format in which each row is an observation and each column is a covariate (such as time-series formats). In practice, the data is often not stored like that and the data comes to us with repeated observations included on a single row. This is often done as a memory saving technique or because there is some structure in the data that makes the ‘wide’ format attractive. Building on this, we may want a way to join data frames together. This makes the data easier to modify, and more likely to maintain consistence. 2.2.7.1 tidyr As described, the perhaps most important tidying action we need is to transform wide to long format tables. Just like with other packages, there are two main arguments to note here. gather: Gather multiple columns that are related into two columns that contain the original column name and the value. spread: This is the opposite of gather. This takes a key column (or columns) and a results column and forms a new column for each level of the key column(s) Although this sound somewhat complicated now, it’s actually quite easy to get your head around. gather # Let&#39;s create another treatment dataframe treat_tests &lt;- data.frame(Patient = factor(c(&quot;A&quot;, &quot;B&quot;,&quot;C&quot;, &quot;D&quot;), levels = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;)), Test_1 = c(1,3,5,4), Test_2 = c(2,4,6,4), Test_3 = c(5,3,2,1)) treat_tests ## Patient Test_1 Test_2 Test_3 ## 1 A 1 2 5 ## 2 B 3 4 3 ## 3 C 5 6 2 ## 4 D 4 4 1 # Now, we want to understand the overall test scores (or average test scores). We do so by using the long format to get the overall scores per patient. treat_tests_all &lt;- treat_tests %&gt;% gather(key = Test, value = Scores, Test_1:Test_3) treat_tests_all ## Patient Test Scores ## 1 A Test_1 1 ## 2 B Test_1 3 ## 3 C Test_1 5 ## 4 D Test_1 4 ## 5 A Test_2 2 ## 6 B Test_2 4 ## 7 C Test_2 6 ## 8 D Test_2 4 ## 9 A Test_3 5 ## 10 B Test_3 3 ## 11 C Test_3 2 ## 12 D Test_3 1 Accordingly, we now have an observation per patient and test score, instead of three test observations in one row. This is the main idea of transforming from wide to long formats. It basically constitutes that you define columns that you transform into rows, based on given attributes. In our case, we used the Test Scores columns and transformed them into one column where we indicated with Test gave which score. This is also quite handy to use with time-series transformation, which is what we will look at later in the course. spread Spread is the opposite of gather. That is, you transform one column to multiple columns, based on some attributes. # Let&#39;s transform it back treat_test_back &lt;- treat_tests_all %&gt;% spread(key = Test, value = Scores) treat_test_back ## Patient Test_1 Test_2 Test_3 ## 1 A 1 2 5 ## 2 B 3 4 3 ## 3 C 5 6 2 ## 4 D 4 4 1 As such, we just reversed what we did before. That is, we transformed one column according to the key attributes from another column. 2.2.8 Beautiful Graphs with ggplot2 Plotting and graphing data, commonly known as visualising, is perhaps one of the most important abilities you can have as data scientist or analyst. Although the usual plot functions work on R, they are quite ugly. Consequently, I would not recommend you to present any of your output in a general baseline plot, unless you just want to assess the data yourself and no one except you can see it. However, once you need to present your data, visualisation is key. Even the best analysis may easily fail with an ugly plot, but you can stretch a bad analysis pretty far if you can deliver on the plot. As such, we will introduce you to the plotting technique required for this course. This comes in form of the package ggplot. Although it may be a little tricky to understand its inner workings, you will get the hang out of it soon. And, once you get it, the effort-return relation is pretty nice. So, let’s get started. 2.2.8.1 Basic Workings and Options First, it should be noted that ggplot2 is designed to act on data frames. It is actually hard to just draw three data points and for simple graphs it might be easier to use the base graphing system in R. However for any real data analysis project, most data will already be in the required format. There are many defined graphing functions within ggplot. This makes it easy to stack multiple graphs on each other and create quite complicated structures with just some lines of code. The entire list of options can be found here, but the main geometries to display data are the following: Figure 3: Geometries in ggplot A graph can be built up layer by layer, where: Each layer corresponds to a geom, each of which requires a dataset and a mapping between an aesthetic and a column of the data set. If you don’t specify either, then the layer inherits everything defined in the ggplot() command. You can have different datasets for each layer! Layers can be added with a +, or you can define two plots and add them together (second one over-writes anything that conflicts). 2.2.8.2 Metling a dataframe An important notion for using ggplots is that you need to define which columns should be accessed. This is where the wide to long format transformation comes into play. Let’s say we have multiple columns that we would like to plot. In order for ggplot to understand which columns we want to print, it is often easier to transform the columns of interest into a long format, implying that you summarise the columns into one single column and have an index column telling you which values belong to which former column. This can be done by using the melt() function. The melt function does exactly what we did before when transofming wide to long formats. As such, I will primarily stick with this function while using ggplots. 2.2.8.3 Graph Geometry Options In this section, we quickly show you the most important geometries and how to apply them. We will use the mpg dataset for the illustration purposes. This is a dataset summarising car attributes. # Load the data data(mpg, package = &quot;ggplot2&quot;) str(mpg) ## tibble [234 × 11] (S3: tbl_df/tbl/data.frame) ## $ manufacturer: chr [1:234] &quot;audi&quot; &quot;audi&quot; &quot;audi&quot; &quot;audi&quot; ... ## $ model : chr [1:234] &quot;a4&quot; &quot;a4&quot; &quot;a4&quot; &quot;a4&quot; ... ## $ displ : num [1:234] 1.8 1.8 2 2 2.8 2.8 3.1 1.8 1.8 2 ... ## $ year : int [1:234] 1999 1999 2008 2008 1999 1999 2008 1999 1999 2008 ... ## $ cyl : int [1:234] 4 4 4 4 6 6 6 4 4 4 ... ## $ trans : chr [1:234] &quot;auto(l5)&quot; &quot;manual(m5)&quot; &quot;manual(m6)&quot; &quot;auto(av)&quot; ... ## $ drv : chr [1:234] &quot;f&quot; &quot;f&quot; &quot;f&quot; &quot;f&quot; ... ## $ cty : int [1:234] 18 21 20 21 16 18 18 18 16 20 ... ## $ hwy : int [1:234] 29 29 31 30 26 26 27 26 25 28 ... ## $ fl : chr [1:234] &quot;p&quot; &quot;p&quot; &quot;p&quot; &quot;p&quot; ... ## $ class : chr [1:234] &quot;compact&quot; &quot;compact&quot; &quot;compact&quot; &quot;compact&quot; ... Bar Charts The simplest graphs are Bar Charts and Histograms. In them, we try to understand some statistical properties about the data, without assessing any relationship thoroughly. We may be interested in knowing the count of each class of car first. # General Structure of a ggplot: # Lastly mpg %&gt;% ggplot(aes(x=class)) + # you first define the dataset you want to operate on and assing the tidy sign %&gt;% # Then, you call the ggplot() function and define the aesthetics. Those are the values of the x and y variable, if required geom_bar() # Lastly, you call the respective geometry you want for the given display The general way how we constructed a data frame here is: The data set we wish to use is specified using mpg %&gt;% ... The column in the data that we wish to investigate is defined in the aes(x=class) part. This means the x-axis will be the car’s class, which is indicated by the column named class The way we want to display this information is using a bar chart, which define after the + sign Histograms We use histograms if we want to find the distribution of a single discrete variable. # Let&#39;s analyse the distribution of cty: mpg %&gt;% ggplot(aes(x=cty)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. geom_histogram breaks up the distribution of the variable into distinct bins. The default bin size is 30. However, we can change the bin size manually: # Let&#39;s analyse the distribution of cty and change the bin size: mpg %&gt;% ggplot(aes(x=cty)) + geom_histogram(bins = 10) Often, instead of using counts, we want to display the density on the y-axis. In order to calculate the density instead of counts, we simply add the option y=..density.. to the aes() list that specifies that the y-axis should be the density. # Let&#39;s analyse the distribution of cty and change the bin size and density on the y axis: mpg %&gt;% ggplot(aes(x=cty, y = ..density..)) + geom_histogram(bins = 10) Scatter Plots Scatter Plots are used to portray a relationship between two variables based on two to three attributes. mpg %&gt;% ggplot(aes(x=cty, y = hwy)) + geom_point() The only difference between histograms / bar charts and scatter plots is that we need to define a value on the y-axis as we are plotting a relationship. In our case, this relationship is highly positively correlated. If we want to add a further attribute to distinguish the data, we do so by calling the color = argument. # Still add the relationship between hwy and cty, but now we also add a third dimension in form of the colours for easch distinct transmitter object mpg %&gt;% ggplot(aes(x = cty, y = hwy, color = trans)) + # Here, just add color geom_point(size = 1.3) Line Plots Line Plots are quite similar to scatterplots, but they just connect the dots with actual lines. # Let&#39;s quickly create a line plot with new data new_dat_line &lt;- data.frame(dose = c(1,2,3,4,5,6,7,8,9,10), treat = c(2,5,2,7,8,10,4,3,7,9)) new_dat_line %&gt;% ggplot(aes(x=dose, y = treat)) + geom_line() We can fine-tune the underlying relation then a little # Let&#39;s quickly create a line plot and fine tune the relation # Here, we create a dashed line plot with red dots indicating the x value observation new_dat_line %&gt;% ggplot(aes(x=dose, y = treat)) + geom_line(linetype = &quot;dashed&quot;, col = &quot;blue&quot;) + geom_point(size = 1.3, col = &quot;red&quot;) Box Plots Box Plots are used to show a categorical variable on the x-axis and continuous on the y-axis. # Let&#39;s create a boxplot for each class and define its cty distributional parameters mpg %&gt;% ggplot(aes(x=class, y = cty)) + geom_boxplot() Note again how a boxplot is constructed: line below box: show the smallest non-outlier observations lower line of box: show the 25th percentile observation fat middle line of box: show the median, or 50th percentile, observation upper line of box: show the 75th percentile observation line above box: show the largest non-outlier observations dots: show the outliers Density Plot If we want to plot the distribution of one continuous variable, we need to plot a density distribution plot. This is the continuous conuterpart to a histogram. In order to do so, R requires us to define a y variable that can plot the values on the y-axis, such that we can have dot combinations that show a filed area. In order to do so, we need to modify our plot of a density distribution. That is, we need to make use of both the melt() function as well as the geom_density() geometry as a plot. # Let&#39;s create the function for only one variable distribution plot hwy &lt;- mpg[,c(&quot;hwy&quot;)] melt_hwy&lt;- melt(hwy) # This creates of the two columns wide format a one column long format which summarises both columns (indicated as value column). Based on this, we have a column that indicates which of the two former columns is assigned to which value, to now be able to plot multiple column density plots. melt_hwy %&gt;% ggplot(aes(x = value, fill = variable)) + geom_density(alpha = 0.2) 2.2.8.4 Added Geometry Options Smooth If we have a scatterplot, we can easily add a line to show the average relation between both variables. This is done using the geom_smooth() geometry option. mpg %&gt;% ggplot(aes(x = cty, y = hwy)) + geom_point(size = 1.3) + geom_smooth(method = &quot;lm&quot;) # This is the geom_smmoth option ## `geom_smooth()` using formula &#39;y ~ x&#39; Here, we add a linear model regression line to define the relation between both variables. The grey area around it is the 95% Confidende Interval. Inerestingly, if we would add a third dimension in form of the colour argument, then we could see that R produces mutliple smooth lines for each of the z variable values, if the z variable is categorical # Let&#39;s create a smooth line for each unique element of the z variable called &quot;drv&quot; mpg %&gt;% ggplot(aes(x = cty, y = hwy, color = drv)) + geom_point(size = 1.3) + geom_smooth(method = &quot;lm&quot;) # This is the geom_smmoth option ## `geom_smooth()` using formula &#39;y ~ x&#39; Errorbar Error Bars are also a useful feature to determine more about the structure of the underlying data. You can use error bars in the same fashion as you would use a confidence interval band we’ve seen in the geom_smooth example. Just opposed to the smooth example, error bars are not continuous. They basically indicate an upper and lower bar compared to some pre-defined value you set. In order to use error bars, we make use of the geom_errorbar() geometry. The most common case to define error bars is to set a mean value and then draw the benchmarks that are \\(\\pm\\) 1 standard deviation in proximity to the mean. As such, we mainly use it for further statistical analysis of our underlying variable(s) based on different characteristics. For instance, we could aim to find out what the upper and lower standard deviation bounds are for the cty characteristic, and distinguish this for the individual classes. # First, define new variables for each class. Here, we: mpg &lt;- mpg %&gt;% # Group by classes group_by(class) %&gt;% # Get the mean and sd values for the cty variable based on its class mutate(mean_cty = mean(cty)) %&gt;% mutate(sd_cty = sd(cty)) %&gt;% # Create lower and upper bounds per class mutate(lwr_cty = mean_cty - 1.96*sd_cty) %&gt;% mutate(upr_cty = mean_cty + 1.96*sd_cty) # Define the error bar plot for each class mpg %&gt;% ggplot(aes(x = class)) + # Still get in the x axis the individual classes # Get the error bars per class geom_errorbar(aes(ymin=lwr_cty, ymax=upr_cty)) + # Get the mean points per class geom_point(aes(y = mean_cty), col = &quot;blue&quot;, size=3, shape=21, fill=&quot;white&quot;) 2.2.8.5 Plotting Multiple Variables Plotting multiple variables can, at times, be tricky in ggplot. However, especially when considering time-series data or distributional characteristics, it may be important to display multiple columns in the same plot. This is the case when considering histograms or density plots. In order to circumvent this issue, one (although not the only) approach is to transform multiple columns into one column. We’ve seen this already when we introduced the wide to long transformation format. This type of transformation is possible by using the function melt() that we’ve introduced while working with density plots. In essence, melt takes the columns of interest and creates two additional columns: A value column, which includes all the values of the baseline columns stacked beneath each other, as well as a variable column, which assigns to each value cell the respective variable name of the baseline columns. By using melt, R understands that multiple values are assigned specific to specific variables. By using a two-column organisation, ggplot is able to draw a x-y combination of data, making it easier to handle the relationship. Let’s apply this. For instance, take two variables: # To create a density function, we first need to define a density to melt the respective variables into one column hwy_cty &lt;- mpg[,c(&quot;hwy&quot;, &quot;cty&quot;)] melt_hwy_cty &lt;- melt(hwy_cty) # This creates of the two columns wide format a one column long format which summarises both columns (indicated as value column). Based on this, we have a column that indicates which of the two former columns is assigned to which value, to now be able to plot multiple column density plots. melt_hwy_cty %&gt;% ggplot(aes(x = value, fill = variable)) + geom_density(alpha = 0.2) # We can also create a histogram from it melt_hwy_cty %&gt;% ggplot(aes(x = value, fill = variable)) + geom_histogram(alpha = 0.2) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 2.2.8.6 Plotting Time-Series Plotting time-series objects will be especially important throughout this course. In essence we want to see how different variables behave throughout time. Remember that we always use an xts object transformation to work with time-series data. Also note that all data manipulation strategies work exactly in the same way as when working with non time-series based data. Line Plots # Let&#39;s load some of the data on the SMI again df_2 &lt;- read.csv(&quot;~/Desktop/Master UZH/Data/A1_dataset_01_Ex_Session.txt&quot;, header = T, sep = &quot;\\t&quot;, dec = &#39;.&#39;)[,c(&quot;Date&quot;, &quot;ABB&quot;, &quot;Adecco&quot;, &quot;Nestle_PS&quot;, &quot;Roche_Holding&quot;)] # Use a Date column df_ts_date &lt;- as.Date(df_2$Date) # Transform the new dataset into an xts object df_ts &lt;- xts(df_2[,-1], order.by = df_ts_date) # Calculate the Returns (We show you everything on this next week) df_2_ret_ts &lt;- Return.calculate(df_ts) df_2_ret_ts &lt;- df_2_ret_ts[&quot;1999-09-30/2010-12-31&quot;] df_2_ret_ts_cum &lt;- cumprod(1+df_2_ret_ts) head(df_2_ret_ts_cum) ## ABB Adecco Nestle_PS Roche_Holding ## 1999-09-30 1.0032523 1.004790 0.9431438 0.991155 ## 1999-10-29 0.9935303 1.106589 0.9832776 1.044520 ## 1999-11-30 1.0178003 1.217969 0.9571906 1.095894 ## 1999-12-31 1.2605351 1.485036 0.9755853 1.078769 ## 2000-01-31 1.1812555 1.486237 0.9050167 1.015982 ## 2000-02-29 1.1488722 1.580845 0.9408027 1.026258 Now, we created a time-series return data frame that we will use for further elaboration. Especially, we are interested in understanding the return behaviour over time of the securities under consideration. Doing so, we need to work with the tidy() function that we introduced earlier. # Here, we create a tidied time-series function on which we then run the ggplot tidy(df_2_ret_ts_cum) %&gt;% ggplot(aes(x = index, y = value, color = series)) + # The ggplot aesthetics block takes three arguments: ## 1) x = index (Time-Series (Years here)) ## 2) y = value (The actual cumulative returns data) ## 3) color = series (The different securities) geom_line() This is a very handy feature to define a ggplot time series. Note the steps that we performed: We created a tidy-time-series (TTS). This is, to some extent, a similar format as what we get when using the melt() function. From this, the program operates as if we merge the multiple columns of the time-series returns for the different securities into one column and assign the respective security names to each time-series return structure. We then created a ggplot aesthetics block with the following arguments: x = index This takes the index column of the dataframe and assigns it to the x-axis of the plot Since we work with an xts object, the date column is the index column We assigned the index status to the date column by setting the group.by = to the respective date column we defined y = value This takes the artificially created value column and prints its values on the y-axis Since we used a tidy() command for our dataset, we artificially melted the different security columns into one stock column This column is now assigned the name “value” and is the long-format from the previous wide format of securities color = series This takes the artificially created variable column and assigns the time-series relation of each distinct value component to its respective group (in this case: the respective company) Since we used a tidy() command for our dataset, we artificially melted the different column names of the wide dataset for each security return into one company column This column is now assigned the name “series” and is identical to the variable column we defined in melt Density Plots Working with time-series, we can use the same format structure to create many plots: # Let&#39;s plot the densities: tidy(df_2_ret_ts_cum) %&gt;% ggplot(aes(x = value, fill = series, color = series)) + # We define here: ## 1) x = value: These are the cumulative returns used on the x-axis ## 2) fill &amp; color = series: These are the y-axis attributes that give you ### (1) the color which defines the density attributes for each stock ### (2) the filling of the density for each security (if you remove the fill command, you just get lines) geom_density(alpha = 0.2) Here, we defined the same set-up as with the line plots, but just had to re-shuffle in order not to use the time-series character of the data. Melt and Tidy By using this set-up, we are able to draw a time-series relationship of multiple stocks and distinguish them by color. Note that this is identical to the melt set-up we used to create two differently coloured histograms or density plots for each of the two variables. In effect, looking at the both datasets, we see their identity: tidy(df_2_ret_ts_cum) ## # A tibble: 544 × 3 ## index series value ## &lt;date&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1999-09-30 ABB 1.00 ## 2 1999-09-30 Adecco 1.00 ## 3 1999-09-30 Nestle_PS 0.943 ## 4 1999-09-30 Roche_Holding 0.991 ## 5 1999-10-29 ABB 0.994 ## 6 1999-10-29 Adecco 1.11 ## 7 1999-10-29 Nestle_PS 0.983 ## 8 1999-10-29 Roche_Holding 1.04 ## 9 1999-11-30 ABB 1.02 ## 10 1999-11-30 Adecco 1.22 ## # … with 534 more rows head(melt_hwy) ## variable value ## 1 hwy 29 ## 2 hwy 29 ## 3 hwy 31 ## 4 hwy 30 ## 5 hwy 26 ## 6 hwy 26 The only, but MAJOR, difference is, that you should use: A tidy dataset for Time-Series A melt dataset for Cross-Sections 2.2.8.7 Further Aesthetics We now have seen how to use the basic plotting options, how to add geometry features and how to apply them to multiple variables in both cross-sectional as well as time-series structures. Now, we can start fine-tuning our plots. That is, we can start to make them more visually appealing and add important information surrounding the data. The entire documentation can be found here, but we will go over the major arguments. Colours First, let’s look at the color scales offered. ggplot offers over 150 distinct colors that you can use. These colours can be used on any plot, to define lines, histogram fillings, points, bars, regions etc. A general overview of all available colours is in the image below Figure 4: Colors in ggplot Further, we can use colour scales. These are ranges of colours that enable the use of continuous colour formats, which are often used enable a better distinction between certain values based on the color attribute (e.g. blue are low and red high values). Figure 5: Color Ranges in ggplot In general, ggplot offers some color combinations for all geometries. There are numerous main arguments to use: scale_color_manual(): Manually add colors for lines and points in the plots scale_colour_steps(): Step-wise add colors for lines and points in the plots scale_colour_gradient(): Gradient color format for lines and points in the plots (e.g. scale colors) scale_colour_grey(): Grey-scale colours for lines and points in the plots scale_fill_manual(): Manually add colors used for the filling of areas or densities scale_fill_steps(): Step-wise add colors used for the filling of areas or densities scale_fill_gradient(): Gradient color format for the filling of areas or densities scale_fill_grey(): Grey-scale colours used for the filling of areas or densities Shapes, Radii, Sizes Despite distinguishing data by colour, we can also find discrepancies by using different forms: scale_linetype(): Automatically assign line types to different variables scale_shape(): Automatically assign point shape types to different variables scale_size(): Automatically assign point size or radius to different variables Scales ggtitle(): Set the title of a plot xlab(), ylab(): Set the x-axis and y-axis titles xlim(), ylim(): Set the x-axis and y-axis limits labs(): Set the title of the legend Themes theme(): Used as an umbrella term to define sizes, margins, colours and grids of the background of a plot. In it you get: plot.title: Used to define the plot title design. Main arguments used: element_text(): Used to define the elements of the title design, including: size: Title Size, color = Title Colour, hjust = Horizontal Adjustment (How centered the title should be), lineheight = Height between the lines if the title has multiple lines, margin = The exact positioning between the upper end of the image and the actual plot axis.title.x, axis.title.y: Used to define the plot x-axis and y-axis title design. Main arguments used are identical to plot.title panel.background: Used to define the background of the panel (part that includes the roaster with data). Main argument used: element_rect: Used to define the filling (fill) and the colour (colour) of the panel background plot.background: Used to define the background of the plot excluding the panel. Main arguments used are identical to panel.background panel.grid.major.x, panel.grid.major.y: Used to define the design of the major vertical and horizontal lines on the x and y axis (if needed). Main arguments are: size: Get the size of the line linetype: Get the type of the line colour: Get the colour of the line element_blank(): Do not add any line panel.grid.minor.x, panel.grid.minor.y: Used to define the design of the minor vertical and horizontal lines on the x and y axis (if needed). Main arguments identical to the major options axis.line: Used to define the design of the x and y axis. Main argument is: element_line(): Gives the colour of the x and y axis # Create the plot figure &lt;- tidy(df_2_ret_ts_cum) %&gt;% # Plot design in general ggplot(aes(x=index,y=value, color=series)) + # Plot geometry(ies) geom_line() + # Line, Point and Fill colours scale_color_manual(values=c(&quot;goldenrod&quot;, &quot;darkorchid4&quot;, &quot;darkorange2&quot;,&quot;dodgerblue1&quot;, &quot;springgreen2&quot;, &quot;darkorchid4&quot;, &quot;dodgerblue4&quot;)) + scale_fill_manual(values=c(&quot;goldenrod&quot;, &quot;darkorchid4&quot;, &quot;darkorange2&quot;,&quot;dodgerblue1&quot;, &quot;springgreen2&quot;, &quot;darkorchid4&quot;, &quot;dodgerblue4&quot;)) + # IMPORTANTLY, IF YOU HAVE MULTIPLE COLOURS, THE COLOUR SORTING (WHICH VARIABLE GETS WHICH COLOUR) IS BASED ON THE NAME OF THE VARIABLE, NOT ITS POSITION IN THE DATAFRAME. E.G.: HERE, THE SEQUENCE OF VARIABLE NAMES IS: 10K-PHYS, 10K-REG, 8K-GEN, MC-GEN, MC-PHYS, MC-REG, MC-OPPTY # --&gt; THIS IS THE WAY HOW GGPLOT ASSIGNS COLOURS ABOVE! # X and Y axis string ylab(&quot;Cumulative Returns&quot;) + xlab(&quot;Time&quot;) + # Title string ggtitle(&quot;Relationship of Cumulative Returns of some SMI Securities&quot;) + labs(color=&#39;Factor Portfolios&#39;) + theme( # Title Elements plot.title= element_text(size=14, color=&quot;grey26&quot;, hjust=0.3,lineheight=0.4, margin=margin(15,0,15,0)), # Axis Elements axis.title.y = element_text(color=&quot;grey26&quot;, size=12, margin=margin(0,10,0,10)), axis.title.x = element_text(color=&quot;grey26&quot;, size=12, margin=margin(10,0,10,0)), # Background colour and fill panel.background = element_rect(fill=&quot;#f7f7f7&quot;), plot.background = element_rect(fill=&quot;#f7f7f7&quot;, color = &quot;#f7f7f7&quot;), # Major Panel Grids panel.grid.major.x = element_blank(), panel.grid.major.y = element_line(size = 0.5, linetype = &quot;solid&quot;, color = &quot;grey&quot;), # Minor Panel Grids panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank(), # Line colour of the x and y axis axis.line = element_line(color = &quot;grey&quot;)) figure 2.2.8.8 Facetting plots The goal with faceting is to make many panels of graphics where each panel represents the same relationship between variables, but based on attributes of a third variable. This is basically the same as if we would print relationships according to colour, where colour defines the third attribute under consideration. We use this tool with the function facet_grid(). # Let&#39;s create a faceted ggplot by drawing the relationship of cty and hwy on a scatterplot mpg %&gt;% ggplot(aes(x = cty, y = hwy)) + geom_point() + # Here, we define that it should print the same relation just for each year separately facet_grid(. ~ year) The facet_grid() tells ggplot to plot the relationship of hwy and cty twice, for each year separately. Note that we can easily extend this relation up to multiple dimensions. Let’s take another variable into the equation. mpg %&gt;% ggplot(aes(x = cty, y = hwy)) + geom_point() + # Here, we define that it should print the same relation just for each year separately facet_grid(drv ~ year) Here, we take the relation between hwy and cty in a scatterplot, but we do this depending on the combination of two variables now. The first is the year of construction, the second is the driving gear (four-wheel, rear or forward drive). As such, we introduce another dimension on the facet grid. As such, we can see relations for both the driving gear and the year of production. 2.2.8.9 Adjusting Scales It is often useful to scale the x and y axis in order to display logarithms, exponentials or other forms of transformations. For this, let’s take a data set that incorporates income and age. # Load the data set install.packages(&quot;Lock5Data&quot;) ## Error in install.packages : Updating loaded packages library(Lock5Data) data(ACS) First print income to age in a normal relation: # Unadjusted scatterplot ACS %&gt;% ggplot(aes(x = Age, y = Income)) + geom_point() Here, the data is not well structured, as certain values skew the display of the data considerably. Consequently, we need two adjustments: We need to adjust the y values in order to get a better shape of the data We need to adjust the y-axis to represent the scaled the y values correctly If we now want to use a log-transformation, we do so by the following code: ACS %&gt;% ggplot(aes(x = Age, y = Income)) + geom_point() + # Here, we add the log-transformation on the y-axis scale_y_log10(breaks=c(1,10,100, 1000), # First define the major break lines according to the log-scale minor=c(1:10, seq(10, 100,by=10 ), seq(100,1000,by=100))) + # Then, set the minor break lines as sequence from to and by a given amount # to get the e^10 distributional marks ylab(&quot;Income in 1&#39;000 US Dollars&quot;) ## Warning: Transformation introduced infinite values in continuous y-axis The handy property of the scaling of the y-axis is that it directly transforms the values on the y axis such that they are correctly positioned in the graph. As such, we see that the area between 1 and 10 is the same 10 to 100 and 100 to 1000. Consequently, we can better show the relations, since they are proportionally transformed. Note that we also added some minor gridlines in the form of either 1000, 10000 or 100000 for the respective horizontal levels. This is to better distinguish and identify the appropriate salary levels. More Transformations Apparently, logarithmic transformation is not the only scaling option. To get an overview, visit the summary page of ggplot here. 2.2.9 Dates and Times We will be working often times with time-series data. This implies that dates are key when administering and operating with the data. However, dates come in many forms and shapes and are quite unsimilar to each other sometimes. Although the financial industry tries to have common notions regarding date formatting, there still is a discrepancy between the European and Anglosaxen way of writing dates. 2.2.9.1 Creating Date and Time Objects To create a Date object, we need to take a string or number that represents a date and tell the computer how to figure out which bits are the year, which are the month, and which are the day. The lubridate package is designed for exactly this purpose. Figure 5: Different Date types in lubridate Usually, in time-series formats for asset pricing, we always use the ymd() specification. Accordingly, the lubridate package works such that it is able to transform any of these six orders into the ymd() format. To see this, look at the following: # Let&#39;s transform a date written in month, day and year to a ymd() object mdy( &#39;May 31, 1996&#39;, &#39;May 31 96&#39;, &#39;5-31-96&#39;, &#39;05-31-1996&#39;, &#39;5/31/96&#39;, &#39;5-31/96&#39;) ## [1] &quot;1996-05-31&quot; &quot;1996-05-31&quot; &quot;1996-05-31&quot; &quot;1996-05-31&quot; &quot;1996-05-31&quot; &quot;1996-05-31&quot; # Let&#39;s transform a date written in day, month and year to a ymd() object dmy(&quot;31st May 1996&quot;, &quot;31.05.1996&quot;, &quot;31 May 96&quot;, &quot;31 May 1996&quot;, &quot;31-5-96&quot;, &quot;31-5-1996&quot;) ## [1] &quot;1996-05-31&quot; &quot;1996-05-31&quot; &quot;1996-05-31&quot; &quot;1996-05-31&quot; &quot;1996-05-31&quot; &quot;1996-05-31&quot; As we can see, you can tell lubridate which format your date cell is in and it will automatically transform the date into a dmy() format. Especially, we see that the all of the other common date formats can be transformed to this. Especially, we see that this all is possible: “31.05.1996”: German / European Date format “31st May 1996”: Anglosaxen format with String in dmy “May 31, 1996”: Anglosaxen format with String in mdy ‘5-31-96’, ‘05-31-1996’: Anglosaxen format without String in mdy In our case, we usually need to create a date() variable which we then use to define an order in an xts object. We can easily do so by combining a lubridate command with the as.Date() command. # Let&#39;s say our data is in a European format. This implies the entire date column of the data frame is given as: dd.mm.yyyy (e.g. 31.05.1996) # We can now easily transform this to a ymd() and then to a date column date_test &lt;- as.Date(dmy(&quot;31.05.1996&quot;)) date_test ## [1] &quot;1996-05-31&quot; And here, we see that we were able to transform the data into the correct format which we can use for xts objects. 2.2.9.2 Adding time to date If we want to add a time to a date, we will use a function with the suffix _hm() or hms(), (= hour-minute-second). # Let&#39;s transform a date written in day, month and year to a ymd_hms() object dmy_hms(&quot;31st May 1996 16:05:11&quot;, &quot;31.05.1996 16:05:11&quot;, &quot;31 May 96 16:05:11&quot;, &quot;31 May 1996 16:05:11&quot;, &quot;31-5-96 16:05:11&quot;, &quot;31-5-1996 16:05:11&quot;) ## [1] &quot;1996-05-31 16:05:11 UTC&quot; &quot;1996-05-31 16:05:11 UTC&quot; &quot;1996-05-31 16:05:11 UTC&quot; &quot;1996-05-31 16:05:11 UTC&quot; &quot;1996-05-31 16:05:11 UTC&quot; ## [6] &quot;1996-05-31 16:05:11 UTC&quot; # Let&#39;s transform a date written in day, month and year including HMS to a ymd_hms() object dmy_hms(&quot;31st May 1996 16:05:11&quot;, &quot;31.05.1996 16:05:11&quot;, &quot;31 May 96 16:05:11&quot;, &quot;31 May 1996 16:05:11&quot;, &quot;31-5-96 16:05:11&quot;, &quot;31-5-1996 16:05:11&quot;) ## [1] &quot;1996-05-31 16:05:11 UTC&quot; &quot;1996-05-31 16:05:11 UTC&quot; &quot;1996-05-31 16:05:11 UTC&quot; &quot;1996-05-31 16:05:11 UTC&quot; &quot;1996-05-31 16:05:11 UTC&quot; ## [6] &quot;1996-05-31 16:05:11 UTC&quot; As we see, the exact same strategy works as before. Consequently, the lubridate package can transform both only dates as well as date and time objects into a classical date and time structure. This is also interesting for us when transforming date and time objects of a different structure into date objects of structure yyyy-mm-dd. For instance, some dates we get online incorrectly include a time attribute. This is the same time attribute for each date and it contains no information about the data. We can get rid of this time infromation accordingly: # Let&#39;s say again our data is in a European format. This implies the entire date column of the data frame is given as: dd.mm.yyyy (e.g. 31.05.1996). Further we get the same time object, e.g. 16:05:11 (maybe b/c the data was gathered then). # We can now easily transform this to a ymd_hms() and then to a pure date column date_time_test &lt;- as.Date(dmy_hms(&quot;31.05.1996 16:05:11&quot;)) date_time_test ## [1] &quot;1996-05-31&quot; As we can see, we excluded the time part of the object and can thus only work with dates. 2.2.9.3 Extracting information from a date We can get the specific information of a date column in R. For instance, we can get the weekday, the month, the second, or the quarter in which the date is. This may be handy for transformation and grouping purposes of time-series data. A full list is available below: Figure 6: Extracting information from a date-time object in lubridate 2.2.9.4 The time-series of Returns - Transformations in R Note that security analysis is always given within a time-series format. We now saw how we can transform date objects to get a common fomrat or how we can extract information from date objects. Next, we will show you how to create a time-series object. We will often work with data which is not pre-defined as a time-series object. However, in order to use many of the analytics functions needed in empirical asset management, we first need to modify the data set into a time-series format. We do so by using the package xts. In the next code block, we will show you how to create such an object and what the structure of this object is. This is the common procedure of transformation in order to get the data in a format with which we can use all the required packages for our analysis. # Load in the dataset with your path A1 &lt;- read.csv(&quot;~/Desktop/Master UZH/Data/A1_dataset_01_Ex_Session.txt&quot;, header = T, sep = &quot;\\t&quot;, dec = &#39;.&#39;) # Look the data set to get a better overview head(A1) ## Date ABB Actelion Adecco Alcon Alusuisse Baloise Ciba_SC Ciba_Geigy_I Ciba_Geigy_PS Clariant Compagnie_Financiere_Richemont ## 1 1988-06-30 5.294 NA 155.845 NA NA NA NA 668.55 305.75 NA NA ## 2 1988-07-29 4.982 NA 157.232 NA NA NA NA 649.82 301.81 NA NA ## 3 1988-08-31 5.049 NA 162.319 NA NA NA NA 648.83 315.62 NA NA ## 4 1988-09-30 5.527 NA 171.476 NA NA NA NA 668.55 327.45 NA NA ## 5 1988-10-31 5.928 NA 175.730 NA NA NA NA 685.31 353.10 NA NA ## 6 1988-11-30 6.161 NA 171.080 NA NA NA NA 534.45 398.47 NA NA ## Credit_Suisse_Group Credit_Suisse_Holding Elektrowatt EMS_Chemie_Holding Geberit Georg_Fischer Givaudan Holcim Jacobs_Suchard Julius_Baer_Group ## 1 NA 76.45 296.70 NA NA NA NA 98.75 7890 NA ## 2 NA 77.05 294.23 NA NA NA NA 97.82 7700 NA ## 3 NA 75.54 286.81 NA NA NA NA 97.36 7500 NA ## 4 NA 80.83 279.89 NA NA 184.400 NA 98.29 7750 NA ## 5 NA 87.03 292.75 NA NA 179.771 NA 99.77 7680 NA ## 6 NA 81.28 274.94 NA NA 169.741 NA 95.04 6850 NA ## Kudelski LafargeHolcim Lonza_Group Merck_Serono Nestle_I Nestle_PS Nobel_Biocare_Holding Novartis_I Novartis_N OC_Oerlikon_Corporation ## 1 NA NA NA NA 834.87 4.103 NA NA NA 17.294 ## 2 NA NA NA NA 817.19 4.006 NA NA NA 16.325 ## 3 NA NA NA NA 816.69 4.123 NA NA NA 19.715 ## 4 NA NA NA NA 859.42 4.133 NA NA NA 21.444 ## 5 NA NA NA NA 873.17 4.264 NA NA NA 20.960 ## 6 NA NA NA NA 675.26 5.829 NA NA NA 21.790 ## Pargesa_Holding Partners_Group Roche_Holding SAirGroup Sandoz_PS Sandoz_N Schweizerische_Volksbank_StN Schweizerische_Volksbank_ST SGS Sika ## 1 30.672 NA 11.166 NA 467.50 NA 158.67 NA NA NA ## 2 31.068 NA 11.329 NA 491.85 NA 155.73 NA NA NA ## 3 31.661 NA 11.119 NA 478.21 NA 154.75 NA NA NA ## 4 33.541 NA 11.935 NA 481.13 NA 152.79 NA NA NA ## 5 32.057 NA 11.842 NA 479.19 NA NA NA NA NA ## 6 30.276 NA 11.655 NA 411.98 NA NA NA NA NA ## Societe_Internationale_Pirelli Sulzer Swiss_Bank_I Swiss_Bank_PS Swiss_Bank_N Swiss_Life_Holding_I Swiss_Life_Holding_N Swiss_Re Swissair ## 1 222.89 NA 351 292 NA NA NA 16.554 1068.24 ## 2 217.00 NA 380 299 NA NA NA 16.637 1125.22 ## 3 212.80 NA 357 291 NA NA NA 16.802 1082.49 ## 4 207.75 NA 375 303 NA NA NA 16.940 1101.48 ## 5 224.57 NA 391 313 NA NA NA 17.518 1125.22 ## 6 209.43 NA 363 296 NA NA NA 19.006 1035.01 ## Swisscom Syngenta Synthes The_Swatch_Group_I The_Swatch_Group_N Transocean UBS_N UBS_PS UBS_I Winterthur Zurich_Insurance_Group_I ## 1 NA NA NA NA NA NA NA 112.5 610.87 955.02 1406.93 ## 2 NA NA NA NA NA NA NA 116.0 648.19 972.96 1397.30 ## 3 NA NA NA NA NA NA NA 113.5 624.61 937.09 1319.00 ## 4 NA NA NA NA NA NA NA 114.5 639.35 1008.82 1373.21 ## 5 NA NA NA NA NA NA NA 123.0 675.68 998.96 1409.34 ## 6 NA NA NA NA NA NA NA 114.0 613.81 824.99 1144.34 ## Zurich_Insurance_Group_N ## 1 NA ## 2 NA ## 3 NA ## 4 NA ## 5 NA ## 6 NA Here, we see how the data is structured. Although it already is in a long format, we still need to transform it to a time-series. Especially, we need to ensure that the Date column is located in the index of the data frame. We do so by following this code: # Define the date column in the dataset as as.Date() date = as.Date(A1[,1]) # Here, we first assign a date format to the date variable, otherwise the xts package cannot read it. # Other forms of transformation (as.POSIXct etc.) would certainly also work. A1ts &lt;- xts(x = A1[,-1], order.by = date) str(A1ts) # Print the structure of Returns ## An &#39;xts&#39; object on 1988-06-30/2021-02-26 containing: ## Data: num [1:393, 1:62] 5.29 4.98 5.05 5.53 5.93 ... ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : NULL ## ..$ : chr [1:62] &quot;ABB&quot; &quot;Actelion&quot; &quot;Adecco&quot; &quot;Alcon&quot; ... ## Indexed by objects of class: [Date] TZ: UTC ## xts Attributes: ## NULL As you can see, the date column is now an index. We need this to be an index to be better able to calculate through it. Small Primer into using lag() lag() is one of the most commonly used functions when working with time-series. You can use lag(1) for instance to define a variable that is lagged by 1 period. Like that, you can calculate returns manually. However, there is an important caveat when using lag(). That is: if the package dplyr is loaded, lag won’t work when trying to render or knit the document. This is b/c the lag function is available in both the stats and dplyr package. Thus, when we do ont specify from which package we should take the lag function, it will take the function automatically from a “higher-order” package, in this case dplyr. However, since the dplyr lag function cannot be rendered, we need to specify that we want to take the lag function explicitly from the stats package. That is, we need to write: stats::lag() to use the function. 2.2.9.5 Checking the time-series dimensions of different dataframes Usually, we may work with different dataframes. It is not given that they share the same format of dates. Luckily, we found a way to circumvent this issue and to put all respective dates to the same format. However, another issue which commonly occurs is related to the correct timing of the dates. This implies that different datasets have a slightly different date for certain observations. It might be that some dates are just incorrectly defined. Other times, it can be that different dataframes have different rules for which date they want to use. For instance, when looking at monthly data, some datasets may regard the last weekday date of a given month as final date. However, others may regard the actual last date of a given month as final date. To put this clearly, let’s say data frame A regards the last Friday of a month as final date. Let’s say that this is the 29th of January in 2018. However, other datasets regard the last actual day of the month as final date. In our case, this would be the Sunday, 31st of January in 2018. This is, unfortunately, a common issue when considering periodic datasets. Unfortunately, many financial algorithms we use require an exact match of dates when we feed them with data of different sources. Otherwise, they will just return an error. And often, this error is not automatically traced back to the date issue, and you may spend hours looking for the bug before realizing that it was a simple date mismatch. Consequently, it is essential to understand how we can notice and flag non-matching date observations and to comprehend how we can match the dates such that we have the same date structure and format for all our distinct data. Let’s show you how to do this: # Load two datasets with different dates A1 &lt;- read.csv(&quot;~/Desktop/Master UZH/Data/A2_dataset_01_Ex_Session.txt&quot;, header = T, sep = &quot;\\t&quot;) A2 &lt;- read.csv(&quot;~/Desktop/Master UZH/Data/A2_dataset_02_Ex_Session.txt&quot;, header = T, sep = &quot;\\t&quot;) # Create a time-series as usual and calculate the bond return as usual A1_date &lt;- as.Date(A1$Date) A1_ts &lt;- xts(x = A1[,-1], order.by = A1_date) A1_ts_returns &lt;- Return.calculate(A1_ts, method = &quot;discrete&quot;)[-1,] A2_date &lt;- as.Date(A2$Date) A2_ts &lt;- xts(x = A2[,-1], order.by = A2_date) A2_ts_1_year &lt;- A2_ts$SWISS.CONFEDERATION.BOND.1.YEAR...RED..YIELD / 100 A2_ts_1_year_monthly &lt;- ((1 + A2_ts_1_year)^(1/12) - 1)[-1,] # Combine the two dataframes df_dates_comp &lt;- as.data.frame(cbind(index(A1_ts_returns), index(A2_ts_1_year_monthly))) # Create the dates again (transform them from numeric arguments to dates again) df_dates_comp_real &lt;- as.data.frame(lapply(df_dates_comp, function(x) as.Date(x))) # Add an index for later sourcing df_dates_comp_real$ID &lt;- seq.int(nrow(df_dates_comp_real)) # Column names colnames(df_dates_comp_real) &lt;- c(&quot;Stocks_TS&quot;, &quot;Risk_Free_TS&quot;, &quot;Index&quot;) # Match the dates df_dates_comp_real$compared &lt;- ifelse(df_dates_comp_real$Stocks_TS == df_dates_comp_real$Risk_Free_TS, 1, 0) # Filter which dates are not aligned df_dates_comp_real %&gt;% filter(compared == 0) ## Stocks_TS Risk_Free_TS Index compared ## 1 2018-01-29 2018-01-31 355 0 Here, we see the small discrepancy that may be very annoying in our time. Of nearly 400 observations, there is exactly one that does not match due to an issue we don’t know. However, in order to use the funcitonalities of time-series management in R, we need to align these dates. Luckily, we also defined an index column. Based on this index column, we can then change the respective observation in the risk-free dataframe. # Change the respective index index(A2_ts_1_year_monthly)[355] &lt;- &quot;2018-01-29&quot; A2_ts_1_year_monthly[355] ## SWISS.CONFEDERATION.BOND.1.YEAR...RED..YIELD ## 2018-01-29 -0.0006263197 And here we are. We now were able to change the date accordingly. 2.2.10 String Manipulations in R: The stringr() package Strings are a major component of data in R. They are needed to perform a wide range of data manipulation tasks. However, in order to use strings, we cannot use the exactly same syntax and approaches as we did for numeric data. Using strings will enable us to vastly widen our programming abilities, since we need strings in large data classes. For instance, the correct use of string manipulation may be extremely handy if you need to perform large, identical operations for dozens or hundreds of individual data frames. Things like loading multiple frames with different, but similar names, assigning names to columns / rows, extracting features of specific data frames and creating new data frames with individual names are very common manipulation techniques as data scientist. These operations are often used in terms of for loops, where you take multiple distinct data sources, run the exact same experiment on each, and obtain individual outputs with distinct names again. You can repeat all steps manually for each data source, but if you have more than 10 sources, then this will get quite messy and exhausting. So let’s see how we can lever these techniques. stringr() is the main package when operating and manipulating strings in R. It adds more functionality to the base functions for handling strings in R, and is also compatible with regular expressions (we’ll cover what they are in the second part, but they are immensely powerful!). In general, stringr offers a wide range of string manipulation techniques and is using consistent function arguments to match structural patterns with other R packages. 2.2.10.1 Basic string operations The main basic functions are the following: Figure 7: Main basic manipulation operations in stringr Concatenate: str_c() The function str_c() is used to concatenate different strings with one another. This is equivalent to the paste() function we saw earlier when printing some of the results. # Let&#39;s create a small example: str_c(&quot;The&quot;, &quot;SMI&quot;, &quot;gained&quot;, &quot;five&quot;, &quot;percent&quot;, &quot;annually&quot;, sep = &quot; &quot;) ## [1] &quot;The SMI gained five percent annually&quot; Character length: str_length() We use str_length() to count the character length of each string under consideration. That is: # Let&#39;s create a small example to count the length of each word str_length(c(&quot;The&quot;, &quot;SMI&quot;, &quot;gained&quot;, &quot;five&quot;, &quot;percent&quot;, &quot;annually&quot;)) ## [1] 3 3 6 4 7 8 Substring characters: str_sub() To extract substrings from a character vector stringr provides str_sub(). The main function of it is the following. # Let&#39;s only get part of the string text &lt;- c(&quot;The&quot;, &quot;SMI&quot;, &quot;gained&quot;, &quot;five&quot;, &quot;percent&quot;, &quot;annually&quot;) str_sub(text, start = 1, end = 3) ## [1] &quot;The&quot; &quot;SMI&quot; &quot;gai&quot; &quot;fiv&quot; &quot;per&quot; &quot;ann&quot; v This returns the substring that contains the first until the third letter of the word “hello”. We can basically perform any manipulation operation as we did it when using select and other methods: # Let&#39;s only get the last three strings: str_sub(text, start = -3, end = -1) ## [1] &quot;The&quot; &quot;SMI&quot; &quot;ned&quot; &quot;ive&quot; &quot;ent&quot; &quot;lly&quot; We can use the command to replace things # Let&#39;s increase all strings: str_sub(text, 1, 3) &lt;- &quot;$$&quot; text ## [1] &quot;$$&quot; &quot;$$&quot; &quot;$$ned&quot; &quot;$$e&quot; &quot;$$cent&quot; &quot;$$ually&quot; Duplicate: str_dup() We can also duplicate words: # Let&#39;s duplicate: str_dup(text, 2) ## [1] &quot;$$$$&quot; &quot;$$$$&quot; &quot;$$ned$$ned&quot; &quot;$$e$$e&quot; &quot;$$cent$$cent&quot; &quot;$$ually$$ually&quot; Padding with str_pad() Here, the idea is to take a string and pad it with leading or trailing characters to a specified total width. That is, you get a character and you define its overall length, then you can pad it to the specific length with some filling character(s) of your choice. The general form of it is: str_pad(text, width = , side = , pad = ). For instance, we can add two connecting lines on either side of the word “hashtag”, by defining the following function: str_pad(&quot;hashtag&quot;, width = 11, side = &quot;both&quot;, pad = &quot;-&quot;) ## [1] &quot;--hashtag--&quot; Trimming with str_trim() One of the typical tasks of string processing is that of parsing a text into individual words. Usually, you end up with words that have blank spaces, called whitespaces, on either end of the word. In this situation, you can use the str_trim() function to remove any number of whitespaces. For instance: text_white &lt;- c(&quot;This&quot;, &quot; example &quot;, &quot;has several &quot;, &quot; whitespaces &quot;) str_trim(text_white, side = &quot;right&quot;) ## [1] &quot;This&quot; &quot; example&quot; &quot;has several&quot; &quot; whitespaces&quot; Like this, we create a text with only one white space and make it readable for text. Replacing with str_replace() str_replace() is quite handy if we need to replace some sub-string within our data. For instance, let’s assume we have dates. # Create a date vector: dates &lt;- c(&quot;2008:Feb&quot;, &quot;2010-Mar&quot;, &quot;2005-Jun&quot;, &quot;2003-Nov&quot;) # Replace the : with - str_replace(dates, &quot;:&quot;, &quot;-&quot;) ## [1] &quot;2008-Feb&quot; &quot;2010-Mar&quot; &quot;2005-Jun&quot; &quot;2003-Nov&quot; Splitting with str_split() Also, we can use str_split() to split strings at given intervals. # Split at the - signs str_split_fixed(str_replace(dates, &quot;:&quot;, &quot;-&quot;), pattern=fixed(&#39;-&#39;), n=2) ## [,1] [,2] ## [1,] &quot;2008&quot; &quot;Feb&quot; ## [2,] &quot;2010&quot; &quot;Mar&quot; ## [3,] &quot;2005&quot; &quot;Jun&quot; ## [4,] &quot;2003&quot; &quot;Nov&quot; These are the most common string manipulation operations. 2.2.10.2 Using for loops with paste0() and assign() to create new variables dynamically Let’s now get to an exciting use case of string transformation: Repeating string manipulations for multiple data frames using for loops. Imagine that you have multiple data frames and you need to perform the same set of operations for each of them. Either, you can do this manually by writing the function for each frame itself, but this may be extremely cumbersome once you need to work with potentially hundreds or thousands of individual data frames. Consequently, we need to find a way load, store and output each file automatically to get the desired output. We can facilitate this operation by using the functions paste0() and assign(). assign() basically assigns a name to a variable. This is nothing special in general, but it is handy when considering that we can use the function to assign a variable name to a newly generated variable, which then is unique based on the for-loop characteristic. paste0() is similar to the paste function we know from earlier. In a for-loop characteristic, we can use the function ot define a unique string argument that the for loop should create. This is handy, as it can also automatically paste strings into a function, to indicate, for instance, which variable should be created or selected. In essence, we can use the assign function in combination with the paste0 function to create new variable names within a for-loop dynamically. For that, let’s assume that we have multiple portfolios and we need to load them, transform them to a time-series object and, ultimately, plot their cumulative returns. The exact operation will make more sense once we covered risk and return, but we focus more on the technical aspects of creating variables and data frames dynamically. # We have a folder consisting of seven distinct but identical datasets. It is our aim to perform the same operations on each of the dataset. For that, we need to define a way to make R understand which datasets it should read in, how to differentiate these datasets from each other, and how to use the same functions on all of the datasets to get the outputs of interest. # As we have multiple data frames, we want to do this all within a for loop structure. # First, we want to read in the data. To do so, we need to tell R what the name of the different data files is, and where they are located. # We know that the usual command for reading in a csv file is: read_csv(&quot;way/to/path/file.csv&quot;, header = , sep = &quot;&quot;, dec = &quot;&quot;). # Further, we know that the path to our files is the same for each of them, namely: &quot;~/Desktop/EMMA/Excel_Files/Cumulative_Returns/filenames.csv&quot; # As a consequence, we need to change the &quot;filenames.csv&quot; to the respective name of the individual csv files such that read_csv() can read them in. # To do this in a for loop, we need to define the respective names of each csv file in a vector. This is done below. data_names &lt;- c(&quot;Cum_Ret_MC_Gen&quot;, &quot;Cum_Ret_MC_Phys&quot;, &quot;Cum_Ret_MC_Reg&quot;, &quot;Cum_Ret_MC_Oppty&quot;, &quot;Cum_Ret_10K_Phys&quot;, &quot;Cum_Ret_10K_Reg&quot;, &quot;Cum_Ret_8K_Gen&quot;) # Now, we have the names. We now need to tell R to: ## loop through each element of these names ## paste each element together with the path structure to get the file path of each csv file ## run the csv_read() command for the respective filepath to read in each file ## assign a name to each uploaded csv file (otherwise we would just overwrite each file over and again while running the loop, resulting in only the last file being read in and saved) # This is one of the ways to proceed for (i in 1:length(data_names)){ # First, we tell R to loop through each element in the vector &quot;data_names&quot; which contains the names of the csv files paste0(assign(paste0(&quot;data&quot;, i), # Here, we create a name for the data frame we read in (in our case data1 - data7). These are the names of the read-in csv files # Here, we assign the created names to the individual read-in csv files (this is identical to e.g.: data1 = read.csv(...)) which is how we would assign a name if we would read in the data manually read.csv(paste0(&quot;~/Desktop/MA_Fundamentals_Non_Coding/Excel_Files/Cumulative_Returns/&quot;, data_names[i], &quot;.csv&quot;), sep = &quot;,&quot;))) # Here, we read-in the data. We use another paste0 command to paste together the: ## 1) path to the files ## 2) respective data name ## 3) csv indicator # E.g. for the first element of the files, paste0 creates the following string: ##&quot;~/Desktop/EMMA/Excel_Files/Cumulative_Returns/Cum_Ret_MC_Gen.csv&quot; # This looks identical to when we usually type in the data path and name manually. # We repeat this for all the individual csv files at the given location. As such, we read-in seven csv files and named them data1 - data7 } # Great, we now know how to read in data from multiple sources into R. If we look at them, we realize they are cumulative products of portfolios for a given time period. However, they are not yet in the desired xts object format. In order to change that, let&#39;s use another for loop to create xts objects for each of the data frames we have now. # In order to create time-series objects from the data frames defined, we need to re-frame the data frames. That is, we need to define a vector with all current data frames and define a list with names to define how their time-series objects should be called. This is done below: list_df &lt;- list(data1, data2, data3, data4, data5, data6, data7) list_names &lt;- c(&quot;Cum_Ret_MC_Gen_ts&quot;, &quot;Cum_Ret_MC_Phys_ts&quot;, &quot;Cum_Ret_MC_Reg_ts&quot;, &quot;Cum_Ret_MC_Oppty_ts&quot;, &quot;Cum_Ret_10K_Phys_ts&quot;, &quot;Cum_Ret_10K_Reg_ts&quot;, &quot;Cum_Ret_8K_Gen_ts&quot;) # Now, we need to again loop through each element of the list with all the data frames for (i in 1:length(list_df)){ # First, we tell R to loop through each element in the vector &quot;list_df&quot; which contains the read-in csv files # Now, we just use the same steps as before to create an xts object, just within a loop format list_df[[i]] &lt;- list_df[[i]][,-1] # First, we exclude the first column from the current data frames since this is just an empty column. Note that we need a double bracket [[]] as # we first need to access the individual file from the file list and then the variable from that file list_df[[i]]$Date &lt;- format(as.Date(list_df[[i]]$Date), &quot;%Y-%m-%d&quot;) # Next, we need to transform the dates into an actual date column of ymd format (note this is equivalent to: as.Date(ymd(list_df[[i]]$Date))) list_df_Date &lt;- as.Date(list_df[[i]]$Date) # Then, we create another variable which is just the date variable (this is identical to the step 2 of creating an xts object) assign(paste(list_names[i],sep=&#39;&#39;), xts(x = list_df[[i]][,-1], order.by = list_df_Date)) # Here, we again create the xts objects for each data frame, simply by defining getting all except for the date column and then ordering # them by the respective ymd date. # Lastly, we again need to assign a name to the newly created xts objects. We do so by pasting the respective element from the list_names vector # and then assigning this name to the created time-series file } # Like that, we performed the time-series transformation for each of the variables and now have seven distinct xts objects named with the elements of list_names. # Lastly, we want to create ggplots for each of the files. # For that, we define a list again with the xts objects. list_df_ts &lt;- list(Cum_Ret_MC_Gen_ts, Cum_Ret_MC_Phys_ts, Cum_Ret_MC_Reg_ts, Cum_Ret_MC_Oppty_ts, Cum_Ret_10K_Phys_ts, Cum_Ret_10K_Reg_ts, Cum_Ret_8K_Gen_ts) # Running the loop to create multiple plots is actually quite straight forward, the only thing we need to adjust the name of each plot for (i in 1:length(list_df_ts)){ # Here, R loops through each xts object again assign(paste(&quot;figure_&quot;,list_names[i],sep = &quot;&quot;), # Here, we define the name again for each created ggplot tidy(list_df_ts[[i]]) %&gt;% # Here, we then assign the name to the ggplot that we created, based on each tidy element of the list_df_ts (our xts objects) # Below, we just run our usual code to define some good-looking plots! ggplot(aes(x=index,y=value, color=series)) + geom_line() + # Line, Point and Fill colours scale_color_manual(values=c(&quot;goldenrod&quot;, &quot;darkorchid4&quot;, &quot;darkorange2&quot;,&quot;dodgerblue1&quot;, &quot;springgreen2&quot;, &quot;darkorchid4&quot;, &quot;dodgerblue4&quot;)) + scale_fill_manual(values=c(&quot;goldenrod&quot;, &quot;darkorchid4&quot;, &quot;darkorange2&quot;,&quot;dodgerblue1&quot;, &quot;springgreen2&quot;, &quot;darkorchid4&quot;, &quot;dodgerblue4&quot;)) + # X and Y axis string ylab(&quot;Cumulative Returns&quot;) + xlab(&quot;Time&quot;) + # Title string ggtitle(&quot;Relationship of Cumulative Returns of some SMI Securities&quot;) + labs(color=&#39;Factor Portfolios&#39;) + theme( # Title Elements plot.title= element_text(size=14, color=&quot;grey26&quot;, hjust=0.3,lineheight=0.4, margin=margin(15,0,15,0)), # Axis Elements axis.title.y = element_text(color=&quot;grey26&quot;, size=12, margin=margin(0,10,0,10)), axis.title.x = element_text(color=&quot;grey26&quot;, size=12, margin=margin(10,0,10,0)), # Background colour and fill panel.background = element_rect(fill=&quot;#f7f7f7&quot;), plot.background = element_rect(fill=&quot;#f7f7f7&quot;, color = &quot;#f7f7f7&quot;), # Major Panel Grids panel.grid.major.x = element_blank(), panel.grid.major.y = element_line(size = 0.5, linetype = &quot;solid&quot;, color = &quot;grey&quot;), # Minor Panel Grids panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank(), # Line colour of the x and y axis axis.line = element_line(color = &quot;grey&quot;))) } # Note that we can even further aggregate these things by, for instance, changing the ggtitle accordingly. Try this out yourself :-) And there we are! This may all be a little much if you have never really worked with such formats before. We do not expect you to draw these things on your own, but we still find it very important to have because exactly these structures can save you potentially a lifetime during your projects. This is especially because these functions are massively scalable. If you want to do this for 10000 data frames, no problem. All you have to do is execute the code and wait until R has done the job (and the best part, if you’re getting paid for it you can write the hours it takes R to compute :-) ). Now, let’s look at one of the outputs: figure_Cum_Ret_MC_Phys_ts Great, we now have seen how to use string adjustments to enforce highly scalable data manipulation techniques! 2.2.10.3 Matching Patterns with Regular Expressions (RegEx) Regexps are a very terse language that allow you to describe patterns in strings. They take a little while to get your head around, but once you understand them, you’ll find them extremely useful. To learn regular expressions, we’ll use str_view()and str_view_all(). These functions take a character vector and a regular expression, and show you how they match. 2.2.11 Database Queries We understand how to retrieve data from local sources. However, most larger datasets are not usually stored somewhere locally, but rather they exist within databases. Databases are remote hubs that allow the user to access data remotely. The main advantage, at least of relational databases, is that you can retrieve much larger datasets from a multitude of sources. Operating with databases in R is a great method for working efficiently. This is because you can largely automate any data retrieval process, instead of having to drop-and-drag all the desired variables on the company web pages or Desktop sources. We will show you that we can retrieve thousands of parameters in a short period of time with these algorithms. In order to access databases, we need to work with queries. These are methods to access the server where the data is stored through a range of functions. We do so by retrieving a host and, if necessary, using certain credentials to open up a connection. The perhaps most known data source that can operate on queries in both Python and R is the WRDS database. As most relational databases, it works with a SQL based syntax, so it’s syntax is comparable to most other databases. It serves as a consortium for multiple data providers, such as Compustat, SECM, Reuters or Bloomberg. The handy thing about the WRDS connection is that you can retrieve data from multiple sources simultaneously, thereby enabling an even more efficient use of database queries. 2.2.11.1 Open the connection to WRDS We first need to import two packages in order to be able to open a connection. Then, we define a dbConnect() function which accesses the WRDS host. We need to define the host, port, database name, as well as the user and password. # Import the important packages: library(remotes) library(RPostgres) # Open the connection wrds &lt;- dbConnect(Postgres(), host=&#39;wrds-pgdata.wharton.upenn.edu&#39;, port=9737, dbname=&#39;wrds&#39;, sslmode=&#39;require&#39;, user=&#39;gostlow&#39;, password = &quot;climaterisk8K&quot;) 2.2.11.2 Data Sources on WRDS The most important data sources that we can retrieve with the WRDS host are the following: CompuStat: Comp.funda: Annual Financial Data only for Domestic Firms Comp.fundq: Quarterly Financial Data only for Domestic Firms Comp.g_funda: Annual Financial Data only for International Firms Comp.g_fundq: Quarterly Financial Data only for International Firms Comp.secm: Monthly stock file For CRSP: Crsp.dsf: Daily stock file Crsp.msf: Monthly stock file Crsp.dsi: Daily market returns Crsp.msi: Monthly market returns For I/B/E/S: Ibes.actpsum_epsus: Summary history EPS actual and pricing - adjusted Ibes.actpsumu_epsus:Summary history EPS actual and pricing - unadjusted For Fama-French Factors: FF.factors_daily: Daily portfolio four factors FF.factors_monthly: Monthly portfolio four factors For a full list, visit this page. From these four databases, we are likely to retrieve all the data that we need on company financials as well as security data for portfolio management purposes. From each of the respective data sources, we can retrieve specific variables, such as the stock price, total assets or dividends. However, we need to know the names of the respective variables. The easiest way to do this is to access the WRDS Data retrieval homepage and simply source manually for the variables of interest. For instance, when you open this link and type in valid credentials, you will be redirected to the US Security Monthly database source from Compustat. Therein, you can choose the selections on query variables and then take the shortname of the required variables. These shortnames are used when you access variables either from R or Python (e.g. if you want the Ticker Symbol, the shortname is: TIC). Later, we will show you a full list of potential variables. 2.2.11.3 Syntax of SQL in WRDS Note that we won’t thoroughly teach you how to write in the SQL way. But note that it’s actually quite quick to understand the logic behind it. SQL is a mostly “text-written” language, so, compared to R or Python, the input you have to write to get what you need is more intuitive. In general, in order to retrieve data from multiple sources on WRDS, we need the following logic: # Run the Query manually res &lt;- dbSendQuery(wrds, &quot;select a.gvkey, a.datadate, a.tic, a.conm, a.at, a.lt, b.prccm, b.cshoq from comp.funda a join comp.secm b on a.gvkey = b.gvkey and a.datadate = b.datadate where a.tic = &#39;IBM&#39; and a.datafmt = &#39;STD&#39; and a.consol = &#39;C&#39; and a.indfmt = &#39;INDL&#39;&quot;) # Fetch data data &lt;- dbFetch(res) # Close database dbClearResult(res) # See data head(data) ## gvkey datadate tic conm at lt prccm cshoq ## 1 006066 1962-12-31 IBM INTL BUSINESS MACHINES CORP 2112.301 731.700 389.9996 NA ## 2 006066 1963-12-31 IBM INTL BUSINESS MACHINES CORP 2373.857 782.119 506.9994 NA ## 3 006066 1964-12-31 IBM INTL BUSINESS MACHINES CORP 3309.152 1055.072 409.4995 NA ## 4 006066 1965-12-31 IBM INTL BUSINESS MACHINES CORP 3744.917 1166.771 498.9991 NA ## 5 006066 1966-12-31 IBM INTL BUSINESS MACHINES CORP 4660.777 1338.149 371.4997 NA ## 6 006066 1967-12-31 IBM INTL BUSINESS MACHINES CORP 5598.668 1767.067 626.9995 NA Therein, we make use of the function dbSendQuery(), which effectively accesses and retrieves the data we specified from the WRDS host. The syntax is as follows: select ** This command is needed to select all the identifiers and quantitative variables we want to gather (such as Ticker, Company Name, Price etc.) ** We can retrieve different variables from multiple data sources easily ** To symbol SQL which variables belong to which data source, we need to add a specific letter and a dot in front of each variable *** e.g.: a.gvkey selects one of the identifier variables for companies from the first data source *** e.g.: b.prccm selects the share price variable for companies from the second data source from ** This command tells SQL from which primary data source (if we have multiple) we retrieve the variables ** If we have multiple, we also need to add the same letter as for the variables *** e.g.: comp.funda a tells that we retrieve all the variables indicated by a. from this data sources join ** This command is needed if we have MULTIPLE data sources ** In this case we have comp.funda and comp.secm and retrieve different variables from either source ** We define for both data sources the respective letter such that SQL knows which variables belong to which source *** e.g. comp.funda a join comp.secm b then joins the variables from both sources on ** This command is needed to tell SQL on which variables we join both data sources ** e.g. here we join them based on identity of the GVKEY company identifier and the Date where ** This command is needed to tell SQL for which companies to retrieve the data and in what format the data should be displayed ** Usually, you can leave datafmt, consol and indfmt as they are defined above ** Then, to define which companies you want, you can select based on four identifiers *** tic: Ticker of companies *** gvkey: GVKEY number of companies *** usip: CUSIP number of companies *** cik: CIK / SIC number of companies In essence, the command above means, literally translated, the following: Select the GVKEY identifier, Date, Ticker, Company Name, Total Assets and Total Liabilities from the source Compustat - Annual Fundamentals Select the Share Price and the Shares Outstanding from the source Compustat - Securities Monthly Merge both data sources on GVKEY identifier and Date For the company whose ticker is “IBM” And display it with the specified data format, consol and keyset format This is quite handy and allows us to retrieve data with a simple command line. 2.2.11.4 Automating SQL with a pre-defined function We now have seen how to retrieve some variables from one company. However, what if we want to fetch hundreds of variables from thousand of companies? In this case, it may be very cumbersome to manually write all the required commands yourself. Rather, it may be smart to have a function in which you only need to enter the respective variables and then let the function do the writing of the query and compiling of the database access. This is what we provide you with below. Don’t worry, the function looks more complicated than it is. But the main idea behind it is that it creates a query fetch syntax, just as we have seen above, from variables we specify. Like that, instead of manually copy-pasting hundreds of company tickers, we can just define the respective column of a dataframe in which all the tickers are saved and enter this into the function. # Create a function to automate the data retrieval jobs on SQL dataset_a = NULL dataset_b = NULL dataset_sql = NULL datafmt = NULL consol = NULL indfmt = NULL sic = NULL gvkey = NULL tic= NULL cusip= NULL isin = NULL filters_list= NULL filters_list_final= NULL filters_list_tweaked= NULL filters_list_tweaked_final= NULL query_sql &lt;- function(dataset_a, dataset_b, column_a, column_b, column_sql, start, end, datafmt, consol, indfmt, sic, gvkey, tic, cusip, isin, multi_function = TRUE, reuters_ds = FALSE){ if (reuters_ds == FALSE){ if (!is.null(column_a)) { for (i in 1:length(column_a)) { column_a[i] = paste(&quot;a.&quot;, column_a[i], sep = &quot;&quot;) column_filter_a = paste(column_a, collapse = &#39;,&#39;) } } else { columns_filter_a = &quot;&quot; } if (!is.null(column_b)) { for (i in 1:length(column_b)) { column_b[i] = paste(&quot;b.&quot;, column_b[i], sep = &quot;&quot;) column_filter_b = paste(column_b, collapse = &#39;,&#39;) } } else { columns_filter_b = &quot;&quot; } if (!is.null(column_sql)) { for (i in 1:length(column_sql)) { column_sql[i] = paste(&quot;b.&quot;, column_sql[i], sep = &quot;&quot;) column_filter_sql = paste(column_sql, collapse = &#39;,&#39;) } } else { columns_filter_sql = &quot;&quot; } if (!is.null(start) &amp; !is.null(end)){ date_filter = paste(&quot;a.datadate BETWEEN &#39;&quot;, start, &quot;&#39; AND &#39;&quot;, end, &quot;&#39;&quot;) } sic_filter = NULL if (!is.null(sic)) { for (i in 1:length(sic)) { sic[i] = paste(&quot;&#39;&quot;, sic[i], &quot;&#39;&quot;, sep = &quot;&quot;) sic_filter = paste(&quot;a.sic IN (&quot;, paste(sic, collapse = &#39;,&#39;), &quot;)&quot;) } } gvkey_filter = NULL if (!is.null(gvkey)) { for (i in 1:length(gvkey)) { gvkey[i] = paste(&quot;&#39;&quot;, gvkey[i], &quot;&#39;&quot;, sep = &quot;&quot;) gvkey_filter = paste(&quot;a.gvkey IN (&quot;, paste(gvkey, collapse = &#39;,&#39;), &quot;)&quot;) } } tic_filter = NULL if (!is.null(tic)) { for (i in 1:length(tic)) { tic[i] = paste(&quot;&#39;&quot;, tic[i], &quot;&#39;&quot;, sep = &quot;&quot;) tic_filter = paste(&quot;a.tic IN (&quot;, paste(tic, collapse = &#39;,&#39;), &quot;)&quot;) } } cusip_filter = NULL if (!is.null(cusip)) { for (i in 1:length(cusip)) { cusip[i] = paste(&quot;&#39;&quot;, cusip[i], &quot;&#39;&quot;, sep = &quot;&quot;) cusip_filter = paste(&quot;a.cusip IN (&quot;, paste(cusip, collapse = &#39;,&#39;), &quot;)&quot;) } } if (!is.null(datafmt)) { for (i in 1:length(datafmt)) { datafmt[i] = paste(&quot;a.datafmt = &#39;&quot;, datafmt[i], &quot;&#39;&quot;, sep = &quot;&quot;) datafmt_filter = paste(datafmt, collapse = &#39;,&#39;) } } if (!is.null(consol)) { for (i in 1:length(consol)) { consol[i] = paste(&quot;a.consol = &#39;&quot;, consol[i], &quot;&#39;&quot;, sep = &quot;&quot;) consol_filter = paste(consol, collapse = &#39;,&#39;) } } if (!is.null(indfmt)) { for (i in 1:length(indfmt)) { indfmt[i] = paste(&quot;a.indfmt = &#39;&quot;, indfmt[i], &quot;&#39;&quot;, sep = &quot;&quot;) indfmt_filter = paste(indfmt, collapse = &#39;,&#39;) } } filters = c(date_filter, cusip_filter, tic_filter, gvkey_filter, sic_filter, datafmt_filter, consol_filter, indfmt_filter) for (i in 1:length(filters)){ if (!is.null(filters[i])){ filters_list[i] = paste(filters[i], sep = &quot;&quot;) filters_list_final = paste(&quot; WHERE &quot;, paste(filters_list, collapse = &quot; AND &quot;)) } } filters_tweaked = c(date_filter, cusip_filter, tic_filter, gvkey_filter, sic_filter) if (!is.null(filters_tweaked[i])){ for (i in 1:length(filters_tweaked)){ filters_list_tweaked[i] = paste(filters_tweaked[i], sep = &quot;&quot;) filters_list_tweaked_final = paste(&quot; WHERE &quot;, paste(filters_list_tweaked, collapse = &quot; AND &quot;)) } } if (multi_function == TRUE){ sql = (paste(&quot;SELECT &quot;, column_filter_a, &quot;, &quot;, column_filter_b, &quot; FROM &quot;, dataset_a, &quot; a&quot;, &quot; inner join &quot;, dataset_b, &quot; b&quot;, &quot; on &quot;, column_a[1], &quot; = &quot;, column_sql[1], &quot; and &quot;, column_a[2], &quot; = &quot;, column_sql[2], &quot; and &quot;, column_a[3], &quot; = &quot;, column_sql[3], filters_list_final)) } else { sql = (paste(&quot;SELECT &quot;, column_filter_a, &quot; FROM &quot;, dataset_a, &quot; a&quot;, filters_list_tweaked_final)) } } else { if (!is.null(column_a)) { for (i in 1:length(column_a)) { column_a[i] = paste(&quot;a.&quot;, column_a[i], sep = &quot;&quot;) column_filter_a = paste(column_a, collapse = &#39;,&#39;) } } else { columns_filter_a = &quot;&quot; } if (!is.null(column_b)) { for (i in 1:length(column_b)) { column_b[i] = paste(&quot;b.&quot;, column_b[i], sep = &quot;&quot;) column_filter_b = paste(column_b, collapse = &#39;,&#39;) } } else { columns_filter_b = &quot;&quot; } if (!is.null(column_sql)) { for (i in 1:length(column_sql)) { column_sql[i] = paste(&quot;b.&quot;, column_sql[i], sep = &quot;&quot;) column_filter_sql = paste(column_sql, collapse = &#39;,&#39;) } } else { columns_filter_sql = &quot;&quot; } if (!is.null(start) &amp; !is.null(end)){ date_filter = paste(&quot;a.year_ BETWEEN &#39;&quot;, start, &quot;&#39; AND &#39;&quot;, end, &quot;&#39;&quot;) } sic_filter = NULL if (!is.null(sic)) { for (i in 1:length(sic)) { sic[i] = paste(&quot;&#39;&quot;, sic[i], &quot;&#39;&quot;, sep = &quot;&quot;) sic_filter = paste(&quot;a.sic IN (&quot;, paste(sic, collapse = &#39;,&#39;), &quot;)&quot;) } } gvkey_filter = NULL if (!is.null(gvkey)) { for (i in 1:length(gvkey)) { gvkey[i] = paste(&quot;&#39;&quot;, gvkey[i], &quot;&#39;&quot;, sep = &quot;&quot;) gvkey_filter = paste(&quot;a.gvkey IN (&quot;, paste(gvkey, collapse = &#39;,&#39;), &quot;)&quot;) } } tic_filter = NULL if (!is.null(tic)) { for (i in 1:length(tic)) { tic[i] = paste(&quot;&#39;&quot;, tic[i], &quot;&#39;&quot;, sep = &quot;&quot;) tic_filter = paste(&quot;a.item5601 IN (&quot;, paste(tic, collapse = &#39;,&#39;), &quot;)&quot;) } } cusip_filter = NULL if (!is.null(cusip)) { for (i in 1:length(cusip)) { cusip[i] = paste(&quot;&#39;&quot;, cusip[i], &quot;&#39;&quot;, sep = &quot;&quot;) cusip_filter = paste(&quot;a.item6004 IN (&quot;, paste(cusip, collapse = &#39;,&#39;), &quot;)&quot;) } } isin_filter = NULL if (!is.null(isin)) { for (i in 1:length(isin)) { isin[i] = paste(&quot;&#39;&quot;, isin[i], &quot;&#39;&quot;, sep = &quot;&quot;) isin_filter = paste(&quot;a.item6008 IN (&quot;, paste(isin, collapse = &#39;,&#39;), &quot;)&quot;) } } filters = c(date_filter, cusip_filter, tic_filter, gvkey_filter, sic_filter, isin_filter) for (i in 1:length(filters)){ if (!is.null(filters[i])){ filters_list[i] = paste(filters[i], sep = &quot;&quot;) filters_list_final = paste(&quot; WHERE &quot;, paste(filters_list, collapse = &quot; AND &quot;)) } } filters_tweaked = c(date_filter, cusip_filter, tic_filter, gvkey_filter, sic_filter, isin_filter) if (!is.null(filters_tweaked[i])){ for (i in 1:length(filters_tweaked)){ filters_list_tweaked[i] = paste(filters_tweaked[i], sep = &quot;&quot;) filters_list_tweaked_final = paste(&quot; WHERE &quot;, paste(filters_list_tweaked, collapse = &quot; AND &quot;)) } } if (multi_function == TRUE){ sql = (paste(&quot;SELECT &quot;, column_filter_a, &quot;, &quot;, column_filter_b, &quot; FROM &quot;, dataset_a, &quot; a&quot;, &quot; inner join &quot;, dataset_b, &quot; b&quot;, &quot; on &quot;, column_a[1], &quot; = &quot;, column_sql[1], &quot; and &quot;, column_a[2], &quot; = &quot;, column_sql[2], &quot; and &quot;, column_a[3], &quot; = &quot;, column_sql[3], filters_list_final)) } else { sql = (paste(&quot;SELECT &quot;, column_filter_a, &quot; FROM &quot;, dataset_a, &quot; a&quot;, filters_list_tweaked_final)) } } } Let’s see how this works. In the next text block, we define the function inputs. As we see, the function requires the following arguments (or variables): dataset_a: Defines the first data source (e.g. comp.funda) dataset_b Defines the second data source (e.g. comp.secm) column_a: Defines the list of variables we want to retrieve from the first data source column_b: Defines the list of variables we want to retrieve from the second data source column_sql: Defines on which variables we want to merge both data sources start: Start date of the retrieval end: End data of the retrieval datafmt: Data Format (usually “STD”) consol: Consol (usually “C”) indfmt: Key Format (usually “INDL”) sic: Potential Company Identifier gvkey: Potential Company Identifier tic: Potential Company Identifier cusip: Potential Company Identifier multi_function: Logical argument idicating if we draw data from multiple source or only one source. Default: TRUE reuters_ds: Logical argument indicating if we take data from Reuters Datastream or Compustat/CRSP. Default: FALSE Note that if we want to take data from Reuters, we need different input parameters as when we take data from CRSP / Compustat. As such, we need to change both input parameters for date and company variables. This will be shown in a later chapter. Great, now we can just define lists for the variables within column_a, column_b and column_sql, enter which sources we want to retrieve the data from in dataset_a and dataset_b and which identifier to use (here I used a list of unique gvkeys for over 5’000 companies). Note that, by default, all arguments are NULL and thus, if you don’t want to change some of the variables you can just leave them with the identity command (e.g. sic = sic, etc.) What this does now is it creates a string-based query input identical to the one above. We can directly take this query input and send it to the database through dbSendQuery(wrds, query) (whereas query represents the fetching command we wrote manually before). # This is the first database, used for the keys from the fundamentals annually list column_a = list(&#39;gvkey&#39;, &#39;iid&#39;, &#39;datadate&#39;, &#39;cik&#39;, &#39;conm&#39;, &#39;tic&#39;, &#39;cusip&#39;, &#39;exchg&#39;, &#39;atq&#39;, &#39;actq&#39;, &#39;ltq&#39;, &#39;lctq&#39;, &#39;ceqq&#39;, &#39;teqq&#39;, &#39;revtq&#39;, &#39;cogsq&#39;, &#39;xintq&#39;, &#39;xsgaq&#39;, &#39;ugiq&#39;, &#39;niq&#39;, &#39;epspxq&#39;, &#39;dvpspq&#39;) # This is to connect the both databases without duplicating their output in the end column_sql = list(&#39;gvkey&#39;, &#39;iid&#39;, &#39;datadate&#39;) # This is the second database, used for the keys from the securities monthly list column_b = list(&#39;secstat&#39;, &#39;cshtrm&#39;) # This is the query output. It is a string that replicates what one would write in the SQL server to obtain data from a (multiple) source(s) gvkey &lt;- read.csv(&quot;~/Desktop/Advanced_Empirical_Finance_Documents/Data/SQL_Query_Check/calls_us_climate_final_df.csv&quot;, header = T)[,-1] gvkey = unique(gvkey) # Get the quarterly data on company financials query = query_sql(dataset_a = &quot;comp.fundq&quot;, dataset_b = &quot;comp.secm&quot;, multi_function = TRUE, column_a = column_a, column_b = column_b, column_sql = column_sql, gvkey = gvkey, sic = sic, tic = tic, cusip = cusip, datafmt = &#39;STD&#39;, consol = &#39;C&#39;, indfmt = &#39;INDL&#39;, start = &#39;2000-01-01&#39;, end = &#39;2010-01-01&#39;) res &lt;- dbSendQuery(wrds, query) # Fetch data data &lt;- dbFetch(res) # See data colnames(data) &lt;- c(&#39;GVKEY&#39;, &#39;IID&#39;, &#39;Date_t&#39;, &#39;CIK&#39;, &#39;Comp_name&#39;, &#39;Ticker&#39;, &#39;CUSIP&#39;, &#39;Exchange_Code&#39;, &#39;Total_Assets_t&#39;, &#39;Current_Assets_t&#39;, &#39;Total_Liab_t&#39;, &#39;Current_Liab_t&#39;, &#39;Common_Equity_t&#39;, &#39;Shareholder_Equity_t&#39;, &#39;Revenue_Tot_t&#39;, &#39;COGS_t&#39;, &#39;Interest_Exp_t&#39;, &#39;SGA_t&#39;, &#39;Gross_Income_t&#39;, &#39;Net_Income_t&#39;, &#39;EPS_t&#39;, &#39;DPS_t&#39;, &#39;Security_Status&#39;, &quot;Traded_Shares&quot;) head(data) ## GVKEY IID Date_t CIK Comp_name Ticker CUSIP Exchange_Code Total_Assets_t Current_Assets_t Total_Liab_t ## 1 110721 01 2000-01-31 0001057709 CARREKER CORP CANI 144433109 14 82.823 71.943 17.417 ## 2 117768 01 2000-01-31 0001045810 NVIDIA CORP NVDA 67066G104 14 202.250 173.175 77.687 ## 3 118267 01 2000-01-31 0000056679 KORN FERRY KFY 500643200 11 403.259 222.951 191.364 ## 4 118321 01 2000-01-31 0001073967 SERENA SOFTWARE INC SRNA.1 817492101 14 149.059 118.938 34.535 ## 5 120513 01 2000-01-31 0001080359 ALLOY INC ALOY 019855303 14 57.668 42.639 12.460 ## 6 120774 01 2000-01-31 0001009626 BROCADE COMMUNICATIONS SYS BRCD 111621306 14 135.112 124.951 41.932 ## Current_Liab_t Common_Equity_t Shareholder_Equity_t Revenue_Tot_t COGS_t Interest_Exp_t SGA_t Gross_Income_t Net_Income_t EPS_t DPS_t ## 1 15.413 65.406 NA 21.599 9.214 NA 8.320 NA 2.612 0.14 0 ## 2 76.225 124.563 NA 128.455 79.809 NA 24.944 NA 14.587 0.47 0 ## 3 136.430 209.058 NA 122.075 68.514 1.437 35.345 NA 8.285 0.23 0 ## 4 29.307 114.524 NA 24.373 3.835 0.000 10.960 NA 6.010 0.24 0 ## 5 12.460 45.208 NA 16.947 6.824 NA 15.030 NA -4.359 -0.30 0 ## 6 41.932 93.180 NA 42.740 20.084 NA 14.591 NA 7.308 0.12 0 ## Security_Status Traded_Shares ## 1 I 2658200 ## 2 A 10294300 ## 3 A 1235200 ## 4 I 3061200 ## 5 I 5301400 ## 6 I 28782800 This is great. In under a minute we were able to retrieve over 36’000 rows and 24 columns of data. This is definitively quicker than doing this via drag-and-drop (btw: now you know why GS Investment Banking Interns work 100+ hours - b/c they don’t know these commands :-) ). 2.2.11.5 Upsample Data We now retrieved much of the data we need. But, unfortunately, most of company fundamentals data is only available in quarterly or annual format (this makes sense as they only report in this periodicity). However, most of the security data (returns, dividends, share amounts, OHLC) is given in much higher intervals. Now, if we want to combine both data sources, we need to sample them both onto the same periodicity. Throughout the lecture, we mostly work with monthly data. As such, in order to be able to compare fundamentals with security data, we should upsample the periodicity of the fundamentals. In principle, this means that we need to duplicate the rows within a given period. For instance, if we have quarterly data and transform it to monthly data, we need to replicate each quarter row twice to get three identical rows for quarterly financials of a given company per quarter. Although this does not necessarily add any new information, this will make it extremely handy to use the data for later analysis (especially for factor creation). # Create the upsampled time-series to obtain monthly intervals (from quarterly) ## First, delete rows that SQL incorrectly retrieved multiple times data_unique &lt;- data %&gt;% distinct(GVKEY, Date_t, .keep_all = T) ## Define, for each company and quarter, the last date of the quarter (e.g. if your date is 2000-03-31, then Last_Date_Q_t = 2000-06-30) data_unique$Last_Date_Q_t &lt;- data_unique$Date_t %m+% months(2) ## Define the interval, in months, between both the first and last date of each quarter and add 1 (to get the absolute period interval) data_unique$Interval_Date_Q &lt;- (interval(data_unique$Date_t, data_unique$Last_Date_Q_t) %/% months(1)) + 1 # Tidy command to create duplicated observations for each month (going from quarterly to monthly observations) data_monthly &lt;- expandRows(data_unique, &quot;Interval_Date_Q&quot;, drop = FALSE) %&gt;% arrange(GVKEY, Date_t) %&gt;% group_by(GVKEY, Date_t, Last_Date_Q_t) %&gt;% mutate(Date_Monthly = seq(as.Date(ceiling_date(first(Date_t), unit = &quot;month&quot;)), as.Date(ceiling_date(first(Last_Date_Q_t) , &quot;month&quot;)), by = &quot;months&quot;) - days(1)) %&gt;% ungroup() # This command is a little nasty. So nasty, that it even looks complicated with a tidy command. But fear not, it can be defined accordingly: ## expandRows: This expands or duplicates each row based on the Interval variable. That is, if the interval = 3, then we get three duplicated rows per observation. ## arrange: This is needed such that tidy understands how to sort each expansion ## group_by: Here, we then group by GVKEY First Date and Last Date of the Quarter to tell R that it should execute the following commands for each group individually (as such, no duplication of the commands can be ensured) ## mutate: Here, we create the new date variables per group. This basically then creates for each group defined a variable that the last day of each month for the specific quarter as a Date. We do this in the following fashion: ### seq(): We first enter for each quarter-company group a sequence, starting with the first day of the first month of each quarter and ending with the first day of the last month of the next quarter. By defines the interval fromt the first to the last observation ### unit = &quot;month&quot; defines that we want to have the specific date in a month format (e.g. 2000-03-31) ### ceiling_date(): This command ensure that we get the first day of the respective month ### days(1) We then subtract from the respective dates one day, by doing so, we get the last days of each month for each quarter (we want the last days as, in monthly intervals of financial analysis, the dates are always w.r.t the last day of each month) # Like this, we create a column which includes, for each quarter-company combination, the three end dates of the respective quarter. For instance, for the quarter between 2000-03-31 and 2000-06-30, we get. 2000-03-31, 2000-04-30 and 2000-05-31. Like that, we successfully duplicated each company-quarter analysis three times and assigned the correct end-of-month dates to each of the rows. # That is, we finally created a monthly upsampled dataset from quarterly observations. Halleluja (this actually took an entire day to code so don&#39;t worry if you don&#39;t get it from the start). # Lastly, we can create a Time-Series Object : data_monthly &lt;- data_monthly %&gt;% select(-c(Date_t, Interval_Date_Q, Last_Date_Q_t)) data_monthly_ts &lt;- xts(data_monthly[, -which(names(data_monthly) == &quot;Date_Monthly&quot;)] , order.by = as.Date(as.POSIXct(data_monthly$Date_Monthly))) Now, we have upsampled the quarterly fundamentals data set. We may now take data on securities and combine it with the fundamentals data. # Get the monthly data on share prices column_a = list(&#39;gvkey&#39;, &#39;iid&#39;, &#39;datadate&#39;, &#39;cik&#39;, &#39;conm&#39;, &#39;tic&#39;, &#39;cusip&#39;, &#39;prccm&#39;, &#39;cshom&#39;, &#39;cshtrm&#39;, &#39;trt1m&#39;, &#39;dvrate&#39;) # Note, here we say multi_function = FALSE as we only retrieve data from one source. Note that if we use it from only one source, we nevertheless need to type in an entry for dataset_b and column_b. query = query_sql(dataset_a = &quot;comp.secm&quot;, dataset_b = &quot;comp.secm&quot;, multi_function = FALSE, column_a = column_a, column_b = column_b, column_sql = column_sql, gvkey = gvkey, sic = sic, tic = tic, cusip = cusip, datafmt = &#39;STD&#39;, consol = &#39;C&#39;, indfmt = &#39;INDL&#39;, start = &#39;2000-01-01&#39;, end = &#39;2010-01-01&#39;) res_shares &lt;- dbSendQuery(wrds, query) ## Warning in result_create(conn@ptr, statement, immediate): Closing open result set, cancelling previous query data_shares &lt;- dbFetch(res_shares) # Here, we define again the names for each variable colnames(data_shares) &lt;- c(&#39;GVKEY&#39;, &#39;IID_check&#39;, &#39;Date_t&#39;, &#39;CIK_check&#39;, &#39;Comp_name&#39;, &#39;Ticker&#39;, &#39;CUSIP&#39;, &#39;Closing_Price_t&#39;, &#39;Shares_Outst_t&#39;, &#39;Trading_Volume_t&#39;, &#39;HRT_t&#39;, &#39;Div_Rate_t&#39;) # Transform to Time-Series Object data_shares_unique &lt;- data_shares %&gt;% distinct(GVKEY, Date_t, .keep_all = T) data_shares_monthly_ts &lt;- xts(data_shares_unique[, -which(names(data_shares_unique) == &quot;Date_t&quot;)] , order.by = as.Date(as.POSIXct(data_shares_unique$Date_t))) # Close database dbClearResult(res) ## Warning: Expired, result set already closed Now, we can merge both dataframes # Merge both dataframes and convert to one Time-Series Object ## Change variable name of data_monthly Date data_monthly$Date_t &lt;- data_monthly$Date_Monthly data_monthly$Date_Monthly &lt;- NULL ## Create a merged object data_final_monthly &lt;- merge(data_monthly, data_shares_unique, by=c(&quot;GVKEY&quot;, &quot;Date_t&quot;, &quot;CUSIP&quot;, &quot;Comp_name&quot;, &quot;Ticker&quot;)) ## Create a Time-Series Object data_final_monthly_ts &lt;- xts(data_final_monthly[, -which(names(data_final_monthly) == &quot;Date_t&quot;)] , order.by = as.Date(as.POSIXct(data_final_monthly$Date_t))) This is just great! Now, we are able to retrieve tons of data quickly from multiple sources, change periodicity to the same levels and combine the data to get one large dataframe in which we can conduct all the operations for factor and variable creation we need. "],["risk-and-return.html", "Chapter 3 Risk and Return 3.1 The time-series of Returns - Transformations in R 3.2 Security Returns 3.3 Portfolio Returns 3.4 Individual Security Risk 3.5 Rolling Risk Characteristics 3.6 Portfolio Risk 3.7 Risk in Extremes and Alternative Risk Measures 3.8 Market Efficiency and independence of risk and return", " Chapter 3 Risk and Return The second week covers Risk and Return, one of the fundamental aspects of asset pricing. Therein, we cover the theoretical aspect behind portfolio theory, what risk metrics are, how to use performance measures, as well as how to visualise asset returns over time. Besides presenting application steps using pre-defined functions, we also cover how to calculate the metrics considered manually using matrix algebra to enhance your understanding of the respective empirical steps. 3.1 The time-series of Returns - Transformations in R Before we dig into the actual analysis, we need to clarify a common set-up for analysing time-serie returns. Note that security analysis is always given within a time-series format. As we want to calculate returns and risk structures, we need to incorporate time-series since we want to compare security or portfolio behaviour over time. That is, we want to have a frame of reference and state how a security develops compared to that frame (e.g. the return from 2015 to now to assess what profit a portfolio has delivered). In order to use time-series in securitiy analysis, we first need to modify the data set into a time-series format. We do so by using the package xts. Therein, we follow these steps to transform and use the concept of time-series within R: # Load in the dataset with your path A1 &lt;- read.csv(&quot;/Users/nikolasanic/Desktop/Master UZH/Data/A1_dataset_01_Ex_Session.txt&quot;, header = T, sep = &quot;\\t&quot;, dec = &#39;.&#39;) # Look the data set to get a better overview head(A1) ## Date ABB Actelion Adecco Alcon Alusuisse Baloise Ciba_SC Ciba_Geigy_I Ciba_Geigy_PS Clariant Compagnie_Financiere_Richemont ## 1 1988-06-30 5.294 NA 155.845 NA NA NA NA 668.55 305.75 NA NA ## 2 1988-07-29 4.982 NA 157.232 NA NA NA NA 649.82 301.81 NA NA ## 3 1988-08-31 5.049 NA 162.319 NA NA NA NA 648.83 315.62 NA NA ## 4 1988-09-30 5.527 NA 171.476 NA NA NA NA 668.55 327.45 NA NA ## 5 1988-10-31 5.928 NA 175.730 NA NA NA NA 685.31 353.10 NA NA ## 6 1988-11-30 6.161 NA 171.080 NA NA NA NA 534.45 398.47 NA NA ## Credit_Suisse_Group Credit_Suisse_Holding Elektrowatt EMS_Chemie_Holding Geberit Georg_Fischer Givaudan Holcim Jacobs_Suchard Julius_Baer_Group ## 1 NA 76.45 296.70 NA NA NA NA 98.75 7890 NA ## 2 NA 77.05 294.23 NA NA NA NA 97.82 7700 NA ## 3 NA 75.54 286.81 NA NA NA NA 97.36 7500 NA ## 4 NA 80.83 279.89 NA NA 184.400 NA 98.29 7750 NA ## 5 NA 87.03 292.75 NA NA 179.771 NA 99.77 7680 NA ## 6 NA 81.28 274.94 NA NA 169.741 NA 95.04 6850 NA ## Kudelski LafargeHolcim Lonza_Group Merck_Serono Nestle_I Nestle_PS Nobel_Biocare_Holding Novartis_I Novartis_N OC_Oerlikon_Corporation ## 1 NA NA NA NA 834.87 4.103 NA NA NA 17.294 ## 2 NA NA NA NA 817.19 4.006 NA NA NA 16.325 ## 3 NA NA NA NA 816.69 4.123 NA NA NA 19.715 ## 4 NA NA NA NA 859.42 4.133 NA NA NA 21.444 ## 5 NA NA NA NA 873.17 4.264 NA NA NA 20.960 ## 6 NA NA NA NA 675.26 5.829 NA NA NA 21.790 ## Pargesa_Holding Partners_Group Roche_Holding SAirGroup Sandoz_PS Sandoz_N Schweizerische_Volksbank_StN Schweizerische_Volksbank_ST SGS Sika ## 1 30.672 NA 11.166 NA 467.50 NA 158.67 NA NA NA ## 2 31.068 NA 11.329 NA 491.85 NA 155.73 NA NA NA ## 3 31.661 NA 11.119 NA 478.21 NA 154.75 NA NA NA ## 4 33.541 NA 11.935 NA 481.13 NA 152.79 NA NA NA ## 5 32.057 NA 11.842 NA 479.19 NA NA NA NA NA ## 6 30.276 NA 11.655 NA 411.98 NA NA NA NA NA ## Societe_Internationale_Pirelli Sulzer Swiss_Bank_I Swiss_Bank_PS Swiss_Bank_N Swiss_Life_Holding_I Swiss_Life_Holding_N Swiss_Re Swissair ## 1 222.89 NA 351 292 NA NA NA 16.554 1068.24 ## 2 217.00 NA 380 299 NA NA NA 16.637 1125.22 ## 3 212.80 NA 357 291 NA NA NA 16.802 1082.49 ## 4 207.75 NA 375 303 NA NA NA 16.940 1101.48 ## 5 224.57 NA 391 313 NA NA NA 17.518 1125.22 ## 6 209.43 NA 363 296 NA NA NA 19.006 1035.01 ## Swisscom Syngenta Synthes The_Swatch_Group_I The_Swatch_Group_N Transocean UBS_N UBS_PS UBS_I Winterthur Zurich_Insurance_Group_I ## 1 NA NA NA NA NA NA NA 112.5 610.87 955.02 1406.93 ## 2 NA NA NA NA NA NA NA 116.0 648.19 972.96 1397.30 ## 3 NA NA NA NA NA NA NA 113.5 624.61 937.09 1319.00 ## 4 NA NA NA NA NA NA NA 114.5 639.35 1008.82 1373.21 ## 5 NA NA NA NA NA NA NA 123.0 675.68 998.96 1409.34 ## 6 NA NA NA NA NA NA NA 114.0 613.81 824.99 1144.34 ## Zurich_Insurance_Group_N ## 1 NA ## 2 NA ## 3 NA ## 4 NA ## 5 NA ## 6 NA # Define the date column inthe dataset as as.Date() date = as.Date(A1[,1]) # Here, we first assign a date format to the date variable, otherwise the xts package cannot read it. # Other forms of transformation (as.POSIXct etc.) would certainly also work. A1ts &lt;- xts(x = A1[,-1], order.by = date) head(A1ts) # Print the Returns ## ABB Actelion Adecco Alcon Alusuisse Baloise Ciba_SC Ciba_Geigy_I Ciba_Geigy_PS Clariant Compagnie_Financiere_Richemont ## 1988-06-30 5.294 NA 155.845 NA NA NA NA 668.55 305.75 NA NA ## 1988-07-29 4.982 NA 157.232 NA NA NA NA 649.82 301.81 NA NA ## 1988-08-31 5.049 NA 162.319 NA NA NA NA 648.83 315.62 NA NA ## 1988-09-30 5.527 NA 171.476 NA NA NA NA 668.55 327.45 NA NA ## 1988-10-31 5.928 NA 175.730 NA NA NA NA 685.31 353.10 NA NA ## 1988-11-30 6.161 NA 171.080 NA NA NA NA 534.45 398.47 NA NA ## Credit_Suisse_Group Credit_Suisse_Holding Elektrowatt EMS_Chemie_Holding Geberit Georg_Fischer Givaudan Holcim Jacobs_Suchard ## 1988-06-30 NA 76.45 296.70 NA NA NA NA 98.75 7890 ## 1988-07-29 NA 77.05 294.23 NA NA NA NA 97.82 7700 ## 1988-08-31 NA 75.54 286.81 NA NA NA NA 97.36 7500 ## 1988-09-30 NA 80.83 279.89 NA NA 184.400 NA 98.29 7750 ## 1988-10-31 NA 87.03 292.75 NA NA 179.771 NA 99.77 7680 ## 1988-11-30 NA 81.28 274.94 NA NA 169.741 NA 95.04 6850 ## Julius_Baer_Group Kudelski LafargeHolcim Lonza_Group Merck_Serono Nestle_I Nestle_PS Nobel_Biocare_Holding Novartis_I Novartis_N ## 1988-06-30 NA NA NA NA NA 834.87 4.103 NA NA NA ## 1988-07-29 NA NA NA NA NA 817.19 4.006 NA NA NA ## 1988-08-31 NA NA NA NA NA 816.69 4.123 NA NA NA ## 1988-09-30 NA NA NA NA NA 859.42 4.133 NA NA NA ## 1988-10-31 NA NA NA NA NA 873.17 4.264 NA NA NA ## 1988-11-30 NA NA NA NA NA 675.26 5.829 NA NA NA ## OC_Oerlikon_Corporation Pargesa_Holding Partners_Group Roche_Holding SAirGroup Sandoz_PS Sandoz_N Schweizerische_Volksbank_StN ## 1988-06-30 17.294 30.672 NA 11.166 NA 467.50 NA 158.67 ## 1988-07-29 16.325 31.068 NA 11.329 NA 491.85 NA 155.73 ## 1988-08-31 19.715 31.661 NA 11.119 NA 478.21 NA 154.75 ## 1988-09-30 21.444 33.541 NA 11.935 NA 481.13 NA 152.79 ## 1988-10-31 20.960 32.057 NA 11.842 NA 479.19 NA NA ## 1988-11-30 21.790 30.276 NA 11.655 NA 411.98 NA NA ## Schweizerische_Volksbank_ST SGS Sika Societe_Internationale_Pirelli Sulzer Swiss_Bank_I Swiss_Bank_PS Swiss_Bank_N Swiss_Life_Holding_I ## 1988-06-30 NA NA NA 222.89 NA 351 292 NA NA ## 1988-07-29 NA NA NA 217.00 NA 380 299 NA NA ## 1988-08-31 NA NA NA 212.80 NA 357 291 NA NA ## 1988-09-30 NA NA NA 207.75 NA 375 303 NA NA ## 1988-10-31 NA NA NA 224.57 NA 391 313 NA NA ## 1988-11-30 NA NA NA 209.43 NA 363 296 NA NA ## Swiss_Life_Holding_N Swiss_Re Swissair Swisscom Syngenta Synthes The_Swatch_Group_I The_Swatch_Group_N Transocean UBS_N UBS_PS UBS_I ## 1988-06-30 NA 16.554 1068.24 NA NA NA NA NA NA NA 112.5 610.87 ## 1988-07-29 NA 16.637 1125.22 NA NA NA NA NA NA NA 116.0 648.19 ## 1988-08-31 NA 16.802 1082.49 NA NA NA NA NA NA NA 113.5 624.61 ## 1988-09-30 NA 16.940 1101.48 NA NA NA NA NA NA NA 114.5 639.35 ## 1988-10-31 NA 17.518 1125.22 NA NA NA NA NA NA NA 123.0 675.68 ## 1988-11-30 NA 19.006 1035.01 NA NA NA NA NA NA NA 114.0 613.81 ## Winterthur Zurich_Insurance_Group_I Zurich_Insurance_Group_N ## 1988-06-30 955.02 1406.93 NA ## 1988-07-29 972.96 1397.30 NA ## 1988-08-31 937.09 1319.00 NA ## 1988-09-30 1008.82 1373.21 NA ## 1988-10-31 998.96 1409.34 NA ## 1988-11-30 824.99 1144.34 NA As you can see, the date column is now an index. We need this to be an index to be better able to calculate through it. Small Primer into using lag() lag() is one of the most commonly used functions when working with time-series. You can use lag(1) for instance to define a variable that is lagged by 1 period. Like that, you can calculate returns manually. However, there is an important caveat when using lag(). That is: if the package dplyr is loaded, lag won’t work when trying to render or knit the document. This is b/c the lag function is available in both the stats and dplyr package. Thus, when we do ont specify from which package we should take the lag function, it will take the function automatically from a “higher-order” package, in this case dplyr. However, since the dplyr lag function cannot be rendered, we need to specify that we want to take the lag function explicitly from the stats package. That is, we need to write: stats::lag() to use the function. 3.2 Security Returns In this chapter, we focus on the percentage changes in the price of a security. This is called the security’s return. From an investments perspective, the return of a security is sometimes a more important measure than the dollar change in the price of our investment. From an investments perspective, the return of a security is sometimes a more important measure than the dollar change in the price of our investment. To see why, suppose someone told you they made 500.- in 1 year from investing in some stock. That information alone does not tell us whether 500.- is a good return on their investment because we would need to know how much the person invested to make the 500.-. If the initial investment was 1000.-, making 500.- is exceptional as that equals to a 50% return in 1 year. However, if the initial investment was 100,000.-, making 500.- is only equal to a 0.5% return. 3.2.1 Simple Returns The daily price return is the percentage change in the price of a security today relative to its price yesterday. That is: \\[ R_{i,t} = \\frac{P_t - P_{i,t-1}}{P_{i,t-1}} = \\frac{P_{i,t}}{P_{i,t-1}} - 1 \\] Where \\(R_{t}\\) is the return of security i at time t, \\(P_{i,t}\\) is the price of this security at t and \\(P_{i,t-1}\\) is the price of the security from one period earlier, that is t-1. To calculate our first return, load the return_bigfour csv file into R. # Read big four dataset in A1_bigfour &lt;- read.csv(&quot;/Users/nikolasanic/Desktop/Advanced_Empirical_Finance_Documents/Data/Risk_Return/A1_bigfour_Ex_real.csv&quot;, header = T, sep = &quot;,&quot;, dec = &#39;.&#39;) # Turn it to an xts file date_bf = as.Date(dmy(A1_bigfour$Date)) # Here, we first assign a date format to the date variable, otherwise the xts package cannot read it. # Other forms of transformation (as.POSIXct etc.) would certainly also work. A1ts_bf &lt;- xts(x = A1_bigfour[,-1], order.by = date_bf) An important feature of xts objects is the date configuration ability. That is, by writing the dates of interest in brackets next to the xts object, we can define the exact period of observation: A1ts_bf[&#39;1988-06-30/1989-12-29&#39;] ## Nestle_PS Novartis_N Roche_Holding UBS_N ## 1988-06-30 4.103 NA 11.166 NA ## 1988-07-29 4.006 NA 11.329 NA ## 1988-08-31 4.123 NA 11.119 NA ## 1988-09-30 4.133 NA 11.935 NA ## 1988-10-31 4.264 NA 11.842 NA ## 1988-11-30 5.829 NA 11.655 NA ## 1988-12-30 6.521 NA 11.795 NA ## 1989-01-31 6.618 NA 12.844 NA ## 1989-02-28 6.233 NA 12.727 NA ## 1989-03-31 6.452 NA 13.753 NA ## 1989-04-28 6.560 NA 15.152 NA ## 1989-05-31 6.209 NA 15.105 NA ## 1989-06-30 7.260 NA 15.781 NA ## 1989-07-31 8.002 NA 17.879 NA ## 1989-08-31 8.338 NA 20.031 NA ## 1989-09-29 8.279 NA 18.043 NA ## 1989-10-31 7.997 NA 16.660 NA ## 1989-11-30 8.338 NA 17.897 NA ## 1989-12-29 8.586 NA 17.412 NA Now, we can calculate the returns for each security. We do this in two ways: First, with the formula above, then with the function ReturnCalculate() # Calculate Returns for Roche: ## ALWAYS USE stats::lag(). THIS IS B/C DPLYR PACKAGE ASSUMES AN INCORRECT LAG WHEN BEING CALLED AND THUS THE MARKDOWN CANNOT BE RENDERED. ## Manually A1_return_Roche_manual &lt;- as.data.frame(A1ts_bf$Roche_Holding / stats::lag(A1ts_bf$Roche_Holding, 1) - 1) A1_return_Roche_manual_ts &lt;- xts(A1_return_Roche_manual, order.by = date_bf) ## With function: Here I combine the return calculate and xts function into one line A1_return_Roche_function_ts &lt;- xts(Return.calculate(A1ts_bf$Roche_Holding, method = &#39;discrete&#39;), order.by = date_bf) # Get the sum to ensure they are the same. We do this by an ifelse statement, stating that if both sums are identical, it prints a 1 ifelse(mean(A1_return_Roche_function_ts, na.rm = TRUE) == mean(A1_return_Roche_manual_ts, na.rm = TRUE), &quot;Both calculations are identical&quot;, 0) ## [1] &quot;Both calculations are identical&quot; A small note here: As you may have noticed, the first return will always be a NA value, since there is no lagged parameter for the first price. As such, every time we work with data that has missing or NA values, we need to modify the calculus by using the argument na.rm = TRUE in a specific function. For that, look at the sum that we calculated. The function is sum(A1_return_Roche_function, na.rm = TRUE). This tells R to simply skip or disregard the empty cells. 3.2.2 Logarithmic Returns Log returns are often used as an approximation to normal returns. Given the Taylor approximation, we understand that log returns are approximately equal to simple returns if the volatility structure is relatively low, implying no large jumps between the periods. We know that volatility is generally higher for riskier assets and longer horizons. Log returns are computed as follows: \\[ R_{i,t} = log(P_{i,t}) - log(P_{i,t-1}) \\] A large benefit of logarithmic returns is their additive behaviour. That is, we can state that the sum of normally distributed variables is normally distributed (if they are uncorrelated). As opposed to this, the product of normally distributed variables is not normally distributed. Consequently, using log returns, we can still rely on many statistical tests that require normality assumptions (such as p values and confidence intervals). Another advantage is their computational simplicity. Note that with log returns, we just need additive properties, but not multiplicative, which is relatively less expensive to compute. # Let&#39;s calculate the log returns for Roche: ## Manually A1_return_Roche_manual_log_ts &lt;- xts(log(A1ts_bf$Roche_Holding) - log(stats::lag(A1ts_bf$Roche_Holding, 1)), order.by = date_bf) ## With function A1_return_Roche_function_log_ts &lt;- xts(Return.calculate(A1ts_bf$Roche_Holding, method = &#39;log&#39;), order.by = date_bf) # Get the sum to ensure they are the same. We do this by an ifelse statement, stating that if both sums are identical, it prints a 1 ifelse(mean(A1_return_Roche_manual_log_ts, na.rm = TRUE) == mean(A1_return_Roche_function_log_ts, na.rm = TRUE), &quot;Both calculations are identical&quot;, 0) ## [1] &quot;Both calculations are identical&quot; # We can also compare both simple and log returns: Roche_log_simpel = as.data.frame(rbind(mean(A1_return_Roche_manual_log_ts, na.rm = TRUE), mean(A1_return_Roche_manual_ts, na.rm = TRUE))) # Define colnames and rownames colnames(Roche_log_simpel) = c(&quot;Return Roche&quot;) rownames(Roche_log_simpel) = c(&quot;Mean Returns Log&quot;, &quot;Mean Returns Simpel&quot;) Roche_log_simpel ## Return Roche ## Mean Returns Log 0.008377684 ## Mean Returns Simpel 0.009988026 3.2.3 Accounting for Dividends: Total Returns The return to investors from holding shares of stock is not limited to changes in the price of the security. For companies that pay dividends, the shareholders holding shares prior to the ex-dividend date receive cash payments that they are able to reinvest. Reinvesting dividends implies we get compounded returns on the dividends. Returns that include dividend reinvestment are known as holding period returns (HPR) or total returns: \\[ R_{i,t}^{Tot} = \\frac{P_t - P_{i,t-1} + CF_{i,t}}{P_{i,t-1}} = \\underbrace{\\left[\\frac{P_{i,t}}{P_{i,t-1}} - 1\\right]}_{\\text{Capital Appreciation}} + \\underbrace{\\frac{CF_{i,t}}{P_{i,t-1}}}_{\\text{CF Yield}} \\] For a daily total return calculation, the dividend yield is zero on non-ex dividend dates. This has two implications. On most days, the price return and the total return are the same because we only have the changes in the capital appreciation on those dates For a non-dividend paying stock, the price return and total return are the same However, on the ex-dividend dates, we would expect the price return and total return to deviate. The more ex-dividend dates there are during our investment horizon, the larger the deviation between cumulative price returns and cumulative total returns will be. To account for this, most large data providers report a variable that incorporates an adjustment for dividends. This variable is labeled the adjusted close price, which is the security’s close price adjusted for both stock splits and dividends (compared to solely close price, which is the non-adjusted price we take for return calculations). 3.2.4 Truncating the data Financial data are sometimes subject to extreme values or outliers. If we determine that these outliers are more noise than information, keeping the outliers in the data can unduly influence the outcome of the analysis and/or create problems in the interpretation of the results. The most common approach is to use truncation techniques. Truncation replaces the values greater than the i-th percentile and less than the (1 − i-th) percentile and removes them from the dataset. This is done using the quantile() and subset() function in R. For example, if we want to winsorize at the 0.5% level, we can use this function to determine what is the cut-off value and then set values higher than the upper cut-off (i.e., 99.5%) to the value of the upper cut-off and set values lower than the lower cut-off (i.e., 0.5%) to the value of the lower cut-off. Let’s do this for Roche: # We first calculate upper and lower cut-offs with the quantile() function upper_co &lt;- as.numeric(quantile(A1_return_Roche_function_ts, 0.995, na.rm = T)) lower_co &lt;- as.numeric(quantile(A1_return_Roche_function_ts, 0.005, na.rm = T)) # We then take the ifelse statement to replace the most extreme values with the upper and lower cut-off scores A1_return_Roche_function_co &lt;- subset(A1_return_Roche_function_ts, A1_return_Roche_function_ts &lt;= upper_co &amp; A1_return_Roche_function_ts &gt;= lower_co) 3.2.5 Arithmetic vs. Geometric Returns Now, we looked at how to calculate periodic returns. If we want to have a summary of all returns to understand an average behaviour of our data, we can use two return calculation methods. These are called Arithmetic and Geometric returns. First, arithmetic returns: \\[ R_{i,T}^{A} = \\frac{1}{T}\\sum_{t=1}^T R_{i,t} - 1 \\] Then, Geometric returns: \\[ R_{i,T}^{G} = [(1+R_{i,1}*)(1+R_{i,2})*\\dots*(1+R_{i,T})]^{1/T} = \\left[\\prod_{t=1}^T R_{i,t} \\right]^{1/T} - 1 \\] There is an important difference as to when we need which returns: Arithmetic Returns: Use this when you want the expected return for a one period investment Geometric Returns: Use this when you want the cumulative return over an entire investment period Consequently, we can get quite different results based on the decision of the return calculation method. 3.2.6 Cumulative Returns When evaluating investments, we are typically concerned with how our investment has performed over a particular time horizon. Put differently, we are interested in cumulative multi-period returns. We could be interested in knowing the returns of our investment over the past week or over the past month or over the past year. To fully capture the effects of being able to reinvest dividends, we should calculate daily returns and string those returns together for longer periods. As we mentioned, we use the geometric return calculation method when we want to assess returns over an entire investment horizon. As such, we calculate the period return with the formula above and then string each return together to obtain the dynamic investment return. We can do this either manually or use the function cumprod() given in the PerformanceAnalytics package. # First, we need to cap the data set such that the first observation is no longer part of it (as this is a NA). A1_return_Roche_ts_cap &lt;- A1_return_Roche_function_ts[-1,] # Then, let&#39;s calculate the cumulative returns. To do so, we can use a handy feature of xts objects. By inserting the dates in brackets, we can define from when to when we want to have the analysis (thereby pinning-down a period of interest) Roche_Prod &lt;- cumprod(1+A1_return_Roche_ts_cap[&#39;1988-07-29/2000-12-29&#39;]) Note that this package also has three other handy features: cumsum(), cummin() and cummax. The first calculates the sum of the subsequent returns up to a given date. The latter two calculate the maximum and minimum value of a period up to a certain point. Let’s try this out: # Let&#39;s calculate the sum, min and max values Roche_Sum &lt;- cumsum(1+A1_return_Roche_ts_cap[&#39;1988-07-29/2000-12-29&#39;]) Roche_Min &lt;- cummin(1+A1_return_Roche_ts_cap[&#39;1988-07-29/2000-12-29&#39;]) Roche_Max &lt;- cummax(1+A1_return_Roche_ts_cap[&#39;1988-07-29/2000-12-29&#39;]) # Now, let&#39;s bind all of them together with the merge.xts function, and create a dataframe and name the columns: cum_ret_all &lt;- merge.xts(Roche_Prod, Roche_Sum, Roche_Min, Roche_Max) colnames(cum_ret_all) &lt;- c(&quot;Roche Cum Prod&quot;, &quot;Roche Cum Sum&quot;, &quot;Roche Cum Min&quot;, &quot;Roche Cum Max&quot;) Importantly, we use the cumprod() function to draw returns over time. That is, each share chart of an investment return you’ve seen is exactly constructed according to the cumulative function we just explored. To see this: tidy(Roche_Prod) %&gt;% ggplot(aes(x=index,y= value, color=series)) + geom_line() + ylab(&quot;Cumulative Returns&quot;) + xlab(&quot;Time&quot;) + ggtitle(&quot;Cumulative Return of Roche from 1988 to 2000&quot;) + labs(color=&#39;Cumulative Return&#39;) + theme(plot.title= element_text(size=14, color=&quot;grey26&quot;, hjust=0.3,lineheight=2.4, margin=margin(15,0,15,45)), panel.background = element_rect(fill=&quot;#f7f7f7&quot;), panel.grid.major.y = element_line(size = 0.5, linetype = &quot;solid&quot;, color = &quot;grey&quot;), panel.grid.minor = element_blank(), panel.grid.major.x = element_blank(), plot.background = element_rect(fill=&quot;#f7f7f7&quot;, color = &quot;#f7f7f7&quot;), axis.title.y = element_text(color=&quot;grey26&quot;, size=12, margin=margin(0,10,0,10)), axis.title.x = element_text(color=&quot;grey26&quot;, size=12, margin=margin(10,0,10,0)), axis.line = element_line(color = &quot;grey&quot;)) 3.2.7 Periodic Transformation of Returns Another important tool that we can use is the transformation functions that xts offers. That is, we can convert time series data to an OHLC series. In R, we do this by using the to.period(xts) package. In total, this package offers 12 time transformations, ranging from minutes to years. For us, the most important ones are to.weekly(), to.monthly(), to.quarterly() as well as to.yearly(). The terms of transformation are self-explanatory here. To see how we can transform this, use: # First, we need to transform the prices data set into different intervals. Note that we have daily prices. Roche_ts_daily &lt;- to.daily(A1ts_bf$Roche_Holding) Roche_ts_weekly &lt;- to.weekly(A1ts_bf$Roche_Holding) Roche_ts_monthly &lt;- A1ts_bf$Roche_Holding Roche_ts_quarterly &lt;- to.quarterly(A1ts_bf$Roche_Holding) Roche_ts_yearly &lt;- to.yearly(A1ts_bf$Roche_Holding) # Then, we can calculate the returns for each periodicity accoringly: Roche_ts_daily_ret &lt;- Return.calculate(Roche_ts_daily, method = &#39;discrete&#39;) Roche_ts_weekly_ret &lt;- Return.calculate(Roche_ts_weekly, method = &#39;discrete&#39;) Roche_ts_monthly_ret &lt;- Return.calculate(Roche_ts_monthly, method = &#39;discrete&#39;) Roche_ts_quarterly_ret &lt;- Return.calculate(Roche_ts_quarterly, method = &#39;discrete&#39;) Roche_ts_yearly_ret &lt;- Return.calculate(Roche_ts_yearly, method = &#39;discrete&#39;) 3.2.8 Annualising Returns One important feature we still need to consider is the annualisation of returns. Instead of simply transforming the time-series into different intervals, we here take periodic returns and calculate what their value when shifted to a different periodicity would be. The daily to yearly formula for this is: \\[ R_{i,t}^{Annual} = (1+R_{i,t})^{365} -1 \\] Under the assumption that \\(R_{i,t}\\) are daily returns. In general, if we have different periods, we can shift the periodicity by simply taking the n’th power of the underlying return metric, whereas n defines the product of the different periodicities. As such, we get: Daily to Weekly: n = 7 Daily to Monthly: n = 30 Daily to Annual: n = 252 (trading days) Weekly to Monthly: n = 4 Weekly to Annual: n = 52 Monthly to Annual: n = 12 Also, if you need to convert it into higher frequency, you take the 1/n’th power of the underlying return metric. As such: Annual to Monthly: 1/n = 1/12 (the rest is analogous) # Get the mean monthly return for Roche: Roche_mean &lt;- mean(A1_return_Roche_ts_cap[&#39;1988-07-29/2000-12-29&#39;]) # Annualise it: Roche_mean_ann &lt;- (1+Roche_mean)^12 - 1 # Put together Roche_average_returns &lt;- as.data.frame(cbind(Roche_mean, Roche_mean_ann)) colnames(Roche_average_returns) &lt;- c(&quot;Roche Monthly Mean&quot;, &quot;Roche Monthly Mean Annualised&quot;) # Print it round(Roche_average_returns,4) ## Roche Monthly Mean Roche Monthly Mean Annualised ## 1 0.0198 0.2659 3.3 Portfolio Returns A key lesson in finance is diversification, which means we should not put all our eggs in one basket. From the perspective of portfolio management, this means that we should have more than one stock in our portfolio. In the last chapter, we looked at returns of individual securities. In this chapter, we extend the returns calculation when you have more than one security in your portfolio. Here, we will look at return characteristics of multiple securities thus that are stacked into one portfolio. Given the matrix algebra and calculus theory from week 1, we can easily apply the derived notions in a practical setting. For the calculation of the portfolios, we will use the bigfour data frame previously introduced. But instead of only assessing Roche, we now introduce Nestle, Novartis and UBS into the setting. 3.3.1 Equal-weighted Returns In general, there are two ways to calculate portfolio returns. The first is called Equal-weighted Returns. This implies that we invest equally in all stocks of a given portfolio. Thereby, we invest 1/N into each asset. In terms of portfolio metrics calculations, this is equivalent as simply taking the mean of all security returns: \\[ R_{p,t} = \\frac{1}{N}\\sum_{i=1}^N R_{i,t} \\] Where \\(R_{p,t}\\) is the portfolio return at time t, \\(R_{i,t}\\) is the security return at time t and N is the number of securities of the portfolio. If we use this approach, we can easily calculate the mean return with the rowMeans() function: # First, the non matrix algebra solution. ## We calculate the returns of all variables A1_bigfour_ret_ts &lt;- xts(Return.calculate(A1ts_bf), order.by = date_bf) ## We delete the first row as this is NA A1_bigfour_ret_ts &lt;- A1_bigfour_ret_ts[-1,] ## We take the row to get the portfolio return at each period bigfour_EW_ret_ts &lt;- rowMeans(A1_bigfour_ret_ts, na.rm = T) ## Calculate annualized return for arithmetic value bigfour_EW_ret_ts_mean_annual &lt;- (1 + mean(bigfour_EW_ret_ts, na.rm = T))**12 -1 bigfour_EW_ret_ts_mean_annual ## [1] 0.1027313 A1_bigfour_ret_ts ## Nestle_PS Novartis_N Roche_Holding UBS_N ## 1988-07-29 -0.0236412381 NA 0.0145978864 NA ## 1988-08-31 0.0292061907 NA -0.0185364992 NA ## 1988-09-30 0.0024254184 NA 0.0733878946 NA ## 1988-10-31 0.0316961045 NA -0.0077922078 NA ## 1988-11-30 0.3670262664 NA -0.0157912515 NA ## 1988-12-30 0.1187167610 NA 0.0120120120 NA ## 1989-01-31 0.0148750192 NA 0.0889359898 NA ## 1989-02-28 -0.0581746751 NA -0.0091093117 NA ## 1989-03-31 0.0351355687 NA 0.0806160132 NA ## 1989-04-28 0.0167389957 NA 0.1017232604 NA ## 1989-05-31 -0.0535060976 NA -0.0031019007 NA ## 1989-06-30 0.1692704139 NA 0.0447533929 NA ## 1989-07-31 0.1022038567 NA 0.1329446803 NA ## 1989-08-31 0.0419895026 NA 0.1203646736 NA ## 1989-09-29 -0.0070760374 NA -0.0992461684 NA ## 1989-10-31 -0.0340620848 NA -0.0766502245 NA ## 1989-11-30 0.0426409904 NA 0.0742496999 NA ## 1989-12-29 0.0297433437 NA -0.0270995139 NA ## 1990-01-31 -0.0230607966 NA 0.0000000000 NA ## 1990-02-28 0.0106103958 NA 0.0278543533 NA ## 1990-03-30 -0.0023593252 NA -0.0270995139 NA ## 1990-04-30 -0.0672815419 NA -0.0139558925 NA ## 1990-05-31 0.0940669371 NA 0.1553963539 NA ## 1990-06-29 -0.0400926999 NA 0.0684579321 NA ## 1990-07-31 -0.0113471753 NA -0.0480301958 NA ## 1990-08-31 -0.0941391941 NA -0.1298012589 NA ## 1990-09-28 -0.1147054859 NA -0.1022326005 NA ## 1990-10-31 0.1204323995 NA 0.1446425173 NA ## 1990-11-30 -0.0591112923 NA -0.0080917807 NA ## 1990-12-31 -0.0014442519 NA 0.0216796111 NA ## 1991-01-31 0.0114260920 NA 0.0026797922 NA ## 1991-02-28 0.0863720864 NA 0.0846514672 NA ## 1991-03-29 0.0806897460 NA 0.0634114452 NA ## 1991-04-30 0.0108404385 NA 0.0435522769 NA ## 1991-05-31 0.0488010604 NA 0.0703280769 NA ## 1991-06-28 -0.0453814338 NA -0.0307790008 NA ## 1991-07-31 0.0249127452 NA 0.0466081335 NA ## 1991-08-30 -0.0219586660 NA 0.0222454090 NA ## 1991-09-30 -0.0499459719 NA -0.0257216348 NA ## 1991-10-31 0.0387969165 NA 0.0833088882 NA ## 1991-11-29 -0.0193430657 NA -0.0139259603 NA ## 1991-12-31 0.0553281231 NA -0.0076105292 NA ## 1992-01-31 0.0523098625 NA 0.0230066806 NA ## 1992-02-28 0.0375335121 NA 0.0898798253 NA ## 1992-03-31 0.0085055986 NA 0.0549902500 NA ## 1992-04-30 0.0358706096 NA 0.0293050141 NA ## 1992-05-29 -0.0019581573 NA 0.0696421575 NA ## 1992-06-30 -0.0061957869 NA -0.0059216752 NA ## 1992-07-31 -0.0420822943 NA -0.0297540455 NA ## 1992-08-31 -0.0193079510 NA 0.0153174252 NA ## 1992-09-30 0.0809644951 NA 0.0845957235 NA ## 1992-10-30 0.0374501177 NA 0.0250603518 NA ## 1992-11-30 0.0389584772 NA 0.0598015027 NA ## 1992-12-31 0.0892348586 NA 0.0692309727 NA ## 1993-01-29 -0.0732961478 NA -0.0167994458 NA ## 1993-02-26 0.0232295683 NA -0.0365887415 NA ## 1993-03-31 0.0818933824 NA 0.0557137260 NA ## 1993-04-30 -0.1008410500 NA 0.0287743085 NA ## 1993-05-31 0.0513983371 NA 0.1002380895 NA ## 1993-06-30 0.0253414809 NA 0.0582526394 NA ## 1993-07-30 -0.0806310254 NA 0.0110092122 NA ## 1993-08-31 0.0533841754 NA 0.0217786585 NA ## 1993-09-30 -0.0135746606 NA 0.0319916822 NA ## 1993-10-29 0.0889908257 NA 0.0798054754 NA ## 1993-11-30 -0.0235888795 NA 0.0460956004 NA ## 1993-12-31 0.1069887834 NA 0.0490386100 NA ## 1994-01-31 0.1114575214 NA 0.1315402224 NA ## 1994-02-28 -0.1002805049 NA -0.0196084098 NA ## 1994-03-31 -0.0756040530 NA 0.0014296663 NA ## 1994-04-29 -0.0109612142 NA -0.0413716977 NA ## 1994-05-31 -0.0221653879 NA -0.0014892376 NA ## 1994-06-30 -0.0209241500 NA -0.0476959269 NA ## 1994-07-29 0.0445235975 NA -0.1674497457 NA ## 1994-08-31 0.0460358056 NA 0.1466139167 0.0400111400 ## 1994-09-30 -0.0472697637 NA -0.0516372370 -0.0288761939 ## 1994-10-31 0.0042771600 NA -0.0345633215 -0.1188014155 ## 1994-11-30 0.0494037479 NA 0.0456468328 -0.0374465422 ## 1994-12-30 0.0121753247 NA 0.0847643276 -0.0272540095 ## 1995-01-31 -0.0593424218 NA 0.0071007459 -0.0440037877 ## 1995-02-28 0.0179028133 NA 0.0728839872 0.0335023015 ## 1995-03-31 -0.0745393635 NA -0.0445549778 -0.0040590822 ## 1995-04-28 0.0126696833 NA 0.0527536323 0.0000000000 ## 1995-05-31 0.0527256479 NA 0.0435766412 0.0162458961 ## 1995-06-30 0.0178268251 NA 0.0327106159 0.0160418871 ## 1995-07-31 -0.0208507089 NA 0.0559240256 -0.0354695466 ## 1995-08-31 0.0383304940 NA 0.0299969713 -0.0408093668 ## 1995-09-29 -0.0295324036 NA 0.0111481865 0.0680848542 ## 1995-10-31 0.0059171598 NA 0.0110252747 0.0358391123 ## 1995-11-30 0.0529411765 NA 0.0763728224 0.0077124953 ## 1995-12-29 0.0183559457 NA 0.0275822005 0.0000000000 ## 1996-01-31 -0.0007836991 NA -0.0290353218 0.0343343077 ## 1996-02-29 0.0290196078 NA 0.0519120592 0.0147988284 ## 1996-03-29 0.0228658537 NA 0.0600881174 0.0908906780 ## 1996-04-30 0.0290611028 NA -0.0116330068 -0.0999814333 ## 1996-05-31 0.0209992759 NA -0.0169047418 -0.0629706034 ## 1996-06-28 0.0134751773 NA -0.0057282263 0.0473884088 ## 1996-07-31 -0.0433869839 NA -0.0749067719 -0.0811350499 ## 1996-08-30 0.0277980980 NA 0.0356721388 -0.0050897861 ## 1996-09-30 -0.0049822064 NA 0.0098377671 0.0041386446 ## 1996-10-31 -0.0178826896 NA 0.0352027706 0.0030339458 ## 1996-11-29 0.0305899490 NA 0.0481108557 0.0225430887 ## 1996-12-31 0.0155477032 NA 0.0394258590 -0.0560919797 ## 1997-01-31 0.0744606820 0.062630182 0.1997067797 0.0106433302 ## 1997-02-28 0.0401554404 0.096611353 -0.0063992470 0.1302363679 ## 1997-03-31 0.0491905355 0.059304603 0.0024182892 -0.0353556269 ## 1997-04-30 0.0623145401 0.087351486 0.0003979308 0.0809766568 ## 1997-05-30 -0.0167597765 -0.011330541 0.0104415274 0.1051429706 ## 1997-06-30 0.0943181818 0.215640320 0.0496834301 0.0774413799 ## 1997-07-31 -0.0036344756 0.039832322 0.1075318384 0.0044609355 ## 1997-08-29 -0.0984887962 -0.131436153 -0.1418805951 -0.1206989582 ## 1997-09-30 0.1710982659 0.057870824 0.0278935556 0.1525135709 ## 1997-10-31 -0.0261599210 -0.016587371 -0.0461234544 -0.0499672346 ## 1997-11-28 0.0633552965 0.038760566 0.0369758609 0.1191584756 ## 1997-12-31 0.0433746425 0.040395071 0.1367513766 0.1645993837 ## 1998-01-30 0.0758337140 0.067498127 0.0575645494 -0.0035391790 ## 1998-02-27 0.0912951168 0.058106520 0.1199531890 0.0858062803 ## 1998-03-31 0.1334630350 0.007849487 -0.0395813065 0.0922014001 ## 1998-04-30 -0.0010298661 -0.080806072 -0.0784858092 -0.0321605508 ## 1998-05-29 0.0903780069 0.011286268 0.0019745547 0.0342703453 ## 1998-06-30 0.0230066183 0.006386726 -0.0223206425 0.1325392165 ## 1998-07-31 -0.0462107209 -0.003958194 0.0775438110 0.1471495939 ## 1998-08-31 -0.1337209302 -0.105407491 -0.0675987529 -0.2781998580 ## 1998-09-30 0.0268456376 -0.013344836 -0.0043433598 -0.4218326028 ## 1998-10-30 0.0457516340 0.099588845 0.0604006398 0.3759154203 ## 1998-11-30 0.0069444444 0.073768273 0.0379712946 0.1305521196 ## 1998-12-31 0.0310344828 0.030536997 0.0219556232 0.0047410649 ## 1999-01-29 -0.1321070234 -0.016299196 0.1032213584 0.0876753011 ## 1999-02-26 0.0539499037 -0.042915320 -0.0075719108 -0.0174139919 ## 1999-03-31 -0.0168190128 -0.055477060 -0.0163501220 0.0310300111 ## 1999-04-30 0.0498326515 -0.069963376 -0.0060931695 0.1139759830 ## 1999-05-31 -0.0141693234 0.013884267 -0.0830553702 -0.1312669695 ## 1999-06-30 0.0064678405 0.002644007 -0.0285683822 0.0310991459 ## 1999-07-30 0.0467690111 -0.051103917 0.0319135886 -0.0204675730 ## 1999-08-31 0.0197817190 0.012074517 0.0624601139 -0.0594074573 ## 1999-09-30 -0.0568561873 0.021096088 -0.0088450236 -0.0105211726 ## 1999-10-29 0.0425531915 0.024276706 0.0538408066 0.0484577147 ## 1999-11-30 -0.0265306122 0.087272991 0.0491844869 -0.0191528776 ## 1999-12-31 0.0192173305 -0.056886876 -0.0156262594 -0.0114920452 ## 2000-01-31 -0.0723345903 -0.132149100 -0.0582021846 -0.1046632124 ## 2000-02-29 0.0395417591 0.046816822 0.0101143589 0.0532407407 ## 2000-03-31 0.0593672236 0.070612245 0.0044470712 0.0776785714 ## 2000-04-28 0.0201342282 0.054535882 -0.0066439299 -0.0343190364 ## 2000-05-31 0.0592105263 0.038356259 -0.0078040533 0.0793928395 ## 2000-06-30 0.0139751553 0.037751803 -0.0795643580 0.0494023417 ## 2000-07-31 0.0649310873 -0.002700026 -0.0125944584 0.0062632913 ## 2000-08-31 0.0796663791 0.022107236 -0.0051020408 0.0540501419 ## 2000-09-29 -0.0410229089 0.006081219 -0.0256410256 -0.0926968607 ## 2000-10-31 0.0347222222 0.029053917 0.0802631579 0.0826118545 ## 2000-11-30 0.0120805369 0.065762383 0.0475030451 -0.0361546850 ## 2000-12-29 0.0026525199 0.017400568 -0.0401162791 0.1020887728 ## 2001-01-31 -0.0780423280 -0.027923211 -0.0860084797 0.0964226487 ## 2001-02-28 0.0516499283 0.016157989 -0.0457256461 -0.0827571305 ## 2001-03-30 -0.0130968622 -0.042756184 -0.1319444444 -0.0620337652 ## 2001-04-30 -0.0069118054 -0.004798819 -0.0032000000 0.0580996233 ## 2001-05-31 0.0339643653 0.012611276 0.0874799358 0.0151647018 ## 2001-06-29 0.0285406570 -0.046886447 -0.0442804428 -0.0391769718 ## 2001-07-31 -0.0314136126 -0.076095311 0.0154440154 -0.0871728315 ## 2001-08-31 -0.0486486486 0.012479201 -0.0912547529 0.0389218009 ## 2001-09-28 -0.0198863636 0.040262942 -0.0292887029 -0.0712493585 ## 2001-10-31 -0.0173913043 -0.033965245 -0.0237068966 0.0046354566 ## 2001-11-30 0.0014749263 -0.054783320 0.0397350993 0.0763613029 ## 2001-12-31 0.0427098675 0.038062284 0.0063694268 0.0250674237 ## 2002-01-31 0.0480225989 -0.022500000 -0.0400843882 -0.0751910934 ## 2002-02-28 0.0148247978 0.104006820 0.0527472527 0.0174288024 ## 2002-03-29 -0.0066401062 0.021621622 0.0918580376 0.0500956586 ## 2002-04-30 0.0240641711 0.027210884 -0.0611854685 -0.0567592567 ## 2002-05-31 -0.0065274151 -0.009565857 0.0244399185 0.0492987044 ## 2002-06-28 -0.0880420499 -0.027488856 -0.1053677932 -0.0866302285 ## 2002-07-31 -0.0821325648 -0.077922078 -0.0622222222 -0.1037455042 ## 2002-08-30 0.0109890110 0.009113505 0.0213270142 0.0826817962 ## 2002-09-30 0.0000000000 -0.043513957 -0.0751740139 -0.1329562883 ## 2002-10-31 -0.0170807453 -0.033476395 0.0486703462 0.1476322093 ## 2002-11-29 -0.0521327014 -0.019538188 0.0095693780 0.0625541890 ## 2002-12-31 -0.0233333333 -0.086050725 -0.0867298578 -0.1010003324 ## 2003-01-31 -0.0307167235 -0.008919722 -0.0259470680 -0.1212895418 ## 2003-02-28 -0.0387323944 -0.004000000 -0.1321257326 -0.0364206741 ## 2003-03-31 -0.0201465201 0.005020080 -0.0067526090 0.0105610037 ## 2003-04-30 0.0336448598 0.068931069 0.0667490729 0.1191215181 ## 2003-05-30 -0.0126582278 -0.045794393 0.1506373117 0.0909250483 ## 2003-06-30 0.0238095238 0.049951028 0.0699899295 0.0733386967 ## 2003-07-31 -0.0196779964 -0.012126866 0.0776470588 0.0670384362 ## 2003-08-29 0.1131386861 -0.027384325 -0.0633187773 -0.0603259343 ## 2003-09-30 -0.0016393443 -0.007766990 0.0209790210 -0.0191968424 ## 2003-10-31 -0.0344827586 -0.003913894 0.0091324201 0.1066126033 ## 2003-11-28 0.0221088435 0.072691552 0.0542986425 0.0152349992 ## 2003-12-31 0.0282861897 0.028388278 0.0708154506 0.0174215082 ## 2004-01-30 0.0744336570 0.008904720 0.0220440882 0.0655322327 ## 2004-02-27 0.0090361446 -0.012356575 0.0274509804 0.0337922403 ## 2004-03-31 -0.0358208955 -0.038427167 -0.0553435115 0.0085714286 ## 2004-04-30 0.0154798762 0.074349442 0.0989898990 -0.0207182984 ## 2004-05-31 -0.0060975610 -0.027681661 -0.0294117647 -0.0244171508 ## 2004-06-30 0.0245398773 -0.016903915 -0.0606060606 -0.0183440131 ## 2004-07-30 -0.0209580838 0.036199095 0.0201612903 -0.0300524766 ## 2004-08-31 -0.0840978593 0.022707424 -0.0296442688 -0.0081549708 ## 2004-09-30 -0.0450751252 -0.005977797 0.0509164969 0.0353360651 ## 2004-10-29 -0.0096153846 -0.018900344 -0.0511627907 -0.0182215369 ## 2004-11-30 0.0326566637 -0.043782837 -0.0196078431 0.0672233711 ## 2004-12-31 0.0170940171 0.049450549 0.0908333333 0.0352718978 ## 2005-01-31 0.0487394958 -0.005235602 -0.0328495034 0.0115383704 ## 2005-02-28 0.0336538462 0.021929825 -0.0323854660 0.0461423151 ## 2005-03-31 0.0147286822 -0.042060086 0.0465306122 0.0009851334 ## 2005-04-29 -0.0435446906 0.037634409 0.1201248050 -0.0574170171 ## 2005-05-31 0.0495207668 0.052677029 0.0961002786 0.0110343846 ## 2005-06-30 -0.0015220700 0.001640689 0.0304955527 0.0389616486 ## 2005-07-29 0.0777439024 0.027027027 0.0801479655 0.0589843221 ## 2005-08-31 -0.0042432815 -0.030303030 -0.0114155251 -0.0311453378 ## 2005-09-30 0.0767045455 0.079769737 0.0381062356 0.0721095625 ## 2005-10-31 0.0131926121 0.056359482 0.0711902113 -0.0045387333 ## 2005-11-30 0.0156250000 -0.007209805 0.0259605400 0.1077551526 ## 2005-12-30 0.0076923077 0.002904866 -0.0015182186 0.0313256602 ## 2006-01-31 -0.0458015267 0.016654598 0.0238215915 0.1111131176 ## 2006-02-28 0.0286666667 0.005698006 -0.0400990099 0.0021615823 ## 2006-03-31 0.0032404407 0.026912181 0.0010314595 0.0280075249 ## 2006-04-28 -0.0226098191 -0.018620690 -0.0175167439 0.0265345722 ## 2006-05-31 -0.0389953734 -0.050597330 -0.0052438385 -0.0625931675 ## 2006-06-30 0.0563961486 -0.019985196 0.0653663679 -0.0275746746 ## 2006-07-31 0.0501302083 0.055891239 0.0836219693 0.0037932430 ## 2006-08-31 0.0495970242 0.005007153 0.0360730594 0.0403419492 ## 2006-09-29 0.0301240402 0.039145907 -0.0471573380 0.0739389439 ## 2006-10-31 -0.0252293578 0.034246575 0.0069380204 -0.0066894157 ## 2006-11-30 -0.0047058824 -0.074834437 -0.0055121727 -0.0302823935 ## 2006-12-29 0.0236406619 0.005726557 0.0092378753 0.0277634721 ## 2007-01-31 0.0536951501 0.018505338 0.0695652174 0.0506407920 ## 2007-02-28 -0.0049315068 -0.051712089 -0.0697475396 -0.0713387579 ## 2007-03-30 0.0424008811 0.027266028 -0.0110395584 -0.0007003346 ## 2007-04-30 0.0137348125 0.010760402 0.0613953488 0.0962622644 ## 2007-05-31 -0.0057321522 -0.022711143 -0.0148992112 0.0094756432 ## 2007-06-29 -0.0230607966 0.002178649 -0.0329181495 -0.0788510794 ## 2007-07-31 -0.0021459227 -0.053623188 -0.0128794848 -0.0821938737 ## 2007-08-31 0.1301075269 -0.023736600 -0.0214352283 -0.0673657928 ## 2007-09-28 -0.0047573739 0.007843137 0.0052380952 -0.0063361177 ## 2007-10-31 0.0219885277 -0.041245136 -0.0639507342 -0.0103820523 ## 2007-11-30 0.0159027128 0.044642857 0.0905870445 -0.0766857247 ## 2007-12-31 -0.0423572744 -0.034965035 -0.0923433875 -0.0839197956 ## 2008-01-31 -0.0711538462 -0.122383253 0.0010224949 -0.1541812407 ## 2008-02-29 0.0341614907 -0.055963303 0.0464759959 -0.2247310737 ## 2008-03-31 -0.0065065065 -0.010689990 -0.0878477306 -0.1600890111 ## 2008-04-30 0.0015113350 0.035363458 -0.0754414125 0.2733967116 ## 2008-05-30 0.0311871227 0.038899431 0.0399305556 -0.2320166447 ## 2008-06-30 -0.0993170732 0.027397260 0.0239287702 -0.1458167331 ## 2008-07-31 -0.0017331023 0.110222222 0.0559782609 -0.0494402985 ## 2008-08-29 0.0546875000 -0.014411529 -0.0427174472 0.1844946026 ## 2008-09-30 -0.0082304527 -0.048740861 -0.0618279570 -0.2352941176 ## 2008-10-31 -0.0663900415 -0.002561913 0.0131805158 0.0482123510 ## 2008-11-28 -0.0235555556 -0.032534247 -0.0384615385 -0.2170542636 ## 2008-12-31 -0.0532544379 -0.067256637 -0.0441176471 -0.0204620462 ## 2009-01-30 -0.0341346154 -0.087666034 0.0055384615 -0.0134770889 ## 2009-02-27 -0.0457939273 -0.112312812 -0.1860465116 -0.2445355191 ## 2009-03-31 0.0036515389 0.009372071 0.1744360902 -0.0325497288 ## 2009-04-30 -0.0291060291 0.007428041 -0.0749039693 0.4906542056 ## [ reached getOption(&quot;max.print&quot;) -- omitted 142 rows ] We can derive the same result and be a little faster by using matrix algebra as we have shown in Chapter 1. That is: \\[ R_p = \\textbf{w}^T\\textbf{R} \\] That is, the overall portfolio return is the matrix multiplication of the weight vector with the individual security returns. To do so, we just use the following: # First we define the average return vector of the securities: return_vector &lt;- rowMeans(A1_bigfour_ret_ts, na.rm = T) # Then, we can create a vector consisting of average weights. Here, we have seven securities, so each return will be multiplied with 1/7. weight_vector &lt;- rep(1/length(return_vector),each=length(return_vector)) # Now calculate the matrix product: mean_return_bigfour &lt;- return_vector %*% weight_vector # Annualise it: bigfour_EW_ret_ts_mean_annual_matmul &lt;- (1+mean_return_bigfour)^12 -1 # See if it is the same: ifelse(bigfour_EW_ret_ts_mean_annual == bigfour_EW_ret_ts_mean_annual_matmul, &quot;Both calculations are identical&quot;, 0) ## [,1] ## [1,] &quot;Both calculations are identical&quot; This was important to show that portfolio returns are nothing else than matrix multiplication steps we saw in the Linear Algebra part. 3.3.2 Value-weighted Returns As opposed to weighting each security with the same factor, we can also account for company size when evaluating portfolio performance. A value-weighted (VW) portfolio invests capital in proportion to the market capitalization of the securities in the portfolio. In a VW portfolio, returns of larger firms are given more weight. As such, you weight the individual security returns with the inverse of their respective market capitalisation, proportional to the overall market capitalisation of the portfolio. Importantly, when using VW strategies, we must use past weights. This follows an underlying logic. To calculate the day t market capitalization, we would need to know the return on day t for that security. So if we use the market capitalization on day t as the weights for day t return, we are assuming perfect foresight as to what the end of day return would be. This is not realistic. This phenomenon is also called the look-ahead bias. To combat this issue, a more appropriate approach is to take the market capitalization the day prior to the rebalancing date, as this information would have been known to investors on the rebalancing date. The market capitalisation is calculated using: \\[ MC_{i,t} = P_{i,t} * SHROUT_{i,t} \\] Whereas \\(P_{i,t}\\) is the Price of security i at time t, and \\(SHROUT_{i,t}\\) are the outstanding shares of security i at time t. As such, once we have the data on Outstanding Shares, we can easily compute VW portfolio returns. # Load in the Market Cap Dataset A4_bigfour &lt;- read.csv(&quot;/Users/nikolasanic/Desktop/Advanced_Empirical_Finance_Documents/Data/Risk_Return/A4_bifgour.csv&quot;, header = TRUE, sep = &quot;,&quot;) # Get the dataset into an xts object (since the length of the data frame is the same, we can use the date_bf as column to order the dates after) A4ts_bf &lt;- xts(A4_bigfour, order.by = date_bf) # Now we can create the weights. There are multiple options to do so, but I show just one: ## First, we take the sum of all the market capitalisations for each date A4_marketcap &lt;- rowSums(A4ts_bf, na.rm = TRUE) ## Then we calculate the individual weights according to the overall market capitalisaiton A4_weights &lt;- A4ts_bf / A4_marketcap ## Lag the weights for one time period to avoid the look-ahead bias A4_weights_lagged &lt;- lag.xts(A4_weights, 1) ## Calculate the returns, given by the returns we calculated in A1ts_bf (discrete) times the lagged weights for each share A4_returns &lt;- A1_bigfour_ret_ts * A4_weights_lagged ## Now we sum this across all stock to get the portfolio return (rowSums()) since we already have the proportionally weighted returns, now their sum will deliver the actual VW return Returns_VW_PF_A4 &lt;- rowSums(A4_returns, na.rm = TRUE) ## Define another date object date_bf_A4 = as.Date(dmy(A1_bigfour[-1,1])) ## Create xts object with correct number of dimensions bigfour_VW_ret_ts &lt;- xts(Returns_VW_PF_A4, order.by = date_bf_A4) To see the comparison of equal and value weighted returns, let’s look at the cumulative return structure of both: # First, transform the variable &quot;bigfour_EW_ret_ts&quot; into an xts object bigfour_EW_ret_ts &lt;- xts(bigfour_EW_ret_ts, order.by = date_bf_A4) # Calculate the cumprod for value weighted and equal weighted returns for a given time-frame Bigfour_cp_VW &lt;- cumprod(1+bigfour_VW_ret_ts[&#39;1988-07-29/2000-12-29&#39;]) Bigfour_cp_EW &lt;- cumprod(1+bigfour_EW_ret_ts[&#39;1988-07-29/2000-12-29&#39;]) # Get the data merged: cum_ret_EW_VW_bigfour &lt;- merge.xts(Bigfour_cp_VW, Bigfour_cp_EW) # Change the names of the columns colnames(cum_ret_EW_VW_bigfour) &lt;- c(&quot;Value-Weighted Return&quot;, &quot;Equal-weighted Return&quot;) # Plot the relationship tidy(cum_ret_EW_VW_bigfour) %&gt;% ggplot(aes(x=index,y= value, color=series)) + geom_line() + scale_color_manual(values=c(&quot;goldenrod&quot;, &quot;dodgerblue4&quot;, &quot;dodgerblue1&quot;, &quot;darkorchid4&quot;)) + ylab(&quot;Cumulative Returns&quot;) + xlab(&quot;Time&quot;) + ggtitle(&quot;Cumulative Return of the Big Four from 1988 to 2000&quot;) + labs(color=&#39;Cumulative Return&#39;) + theme(plot.title= element_text(size=14, color=&quot;grey26&quot;, hjust=0.3,lineheight=2.4, margin=margin(15,0,15,45)), panel.background = element_rect(fill=&quot;#f7f7f7&quot;), panel.grid.major.y = element_line(size = 0.5, linetype = &quot;solid&quot;, color = &quot;grey&quot;), panel.grid.minor = element_blank(), panel.grid.major.x = element_blank(), plot.background = element_rect(fill=&quot;#f7f7f7&quot;, color = &quot;#f7f7f7&quot;), axis.title.y = element_text(color=&quot;grey26&quot;, size=12, margin=margin(0,10,0,10)), axis.title.x = element_text(color=&quot;grey26&quot;, size=12, margin=margin(10,0,10,0)), axis.line = element_line(color = &quot;grey&quot;)) 3.3.3 Timing of Returns An interesting property of returns is their timing character. That is, depending on when you invest, it may take you longer to derive a positive profit from your investment than in other times. To see this, we can easily consult the cumulative return metrics again. Note from theory that we understand a positive profit to be once the cumulative return is larger than 1 for a given time period. For instance, we may ask how long it takes us to derive a positive return if we had invested right before the financial crisis, in May 2007. # Let&#39;s create the cum return series for both value- and equal-weighted portfolios of the big four. cum_ret_EW_Financial &lt;- cumprod(1 + bigfour_EW_ret_ts[&#39;2007-06-01/2021-02-26&#39;]) cum_ret_VW_Financial &lt;- cumprod(1 + bigfour_VW_ret_ts[&#39;2007-06-01/2021-02-26&#39;]) # This command reports all values which were in this time above 1, so higher than the initial return on the beginning of the financial crisis EW_Positive_After_Financial &lt;- subset(cum_ret_EW_Financial, cum_ret_EW_Financial &gt;= 1) VW_Positive_After_Financial &lt;- subset(cum_ret_VW_Financial, cum_ret_EW_Financial &gt;= 1) # This command returns the first of said values EW_First_Positive_After_Financial &lt;- EW_Positive_After_Financial[c(1:2),1] VW_First_Positive_After_Financial &lt;- VW_Positive_After_Financial[c(1:2),1] # Then, we get the dates for both returns print(paste0(&quot;When investing just before the Financial Crisis, the EW Big Four portfolio reaches its first positive return on &quot;, as.Date(as.POSIXct(EW_Positive_After_Financial[1,0])), &quot;. When investing just before the Financial Crisis, the VW Big Four portfolio reaches its first positive return on &quot;, as.Date(as.POSIXct(VW_Positive_After_Financial[1,0])))) ## [1] &quot;When investing just before the Financial Crisis, the EW Big Four portfolio reaches its first positive return on 2014-09-30. When investing just before the Financial Crisis, the VW Big Four portfolio reaches its first positive return on 2014-09-30&quot; 3.3.4 Nominal vs. Real Returns The concept of inflation plays an important role in asset management. Inflation is regarded as a hidden cost variable, since it indirectly reduces the overall return by adding a factor for price developments. In other words, inflation decides how much worth your money has, to put it bluntly. If inflation is positive, then prices increase and, as such, for CHF 1.- you can consume less. On the other hand, if inflation is negative, then CHF 1.- can get you a higher consumption and thus is worth more. As inflation is not measurable directly, we need to use some proxies that we believe adequately represent the pricing adjustments. One such method is to use available Consumer Price Index data. The CPI measures the change in prices of goods and services which are representative of the private households’ consumption in Switzerland. It indicates how much consumers have to increase or to decrease their expenditure to maintain the same volume of consumption, despite the variations in prices. Consequently, by analysing the CPI, we can proxy inflation levels for Switzerland. To do so, we need to calculate the returns of the CPI. That is: \\[ Inflation_{i,t} = \\frac{CPI_{i,t}}{CPI_{i,t-1}} - 1 \\] Whereas \\(Inflation_{i,t}\\) is the inflation of country i at time t and \\(CPI_{i,t}\\) is the Consumer Price Index of country i at time t. Using CPI-adjustments is important to understand the difference between nominal returns and real returns. The nominal return on an investment is the money made without factoring expenses, such as inflation, taxes, and fees. The real return on an investment is the return made on an investment after subtracting costs, such as inflation, taxes, and fees. In general, we derive the real return by subtracting non-avoidable expenses that mitigate the actual return of our investment from the pure financial return we would hypothetically have achieved if no other costs had occurred. Real and Nominal Returns have the followinf mathematical relationship: \\[ R_{p,t}^{Real} = \\frac{1+R_{p,t}}{1+Inflation_{i,t}} - 1 \\] Whereas \\(R_{p,t}\\) is the nominal portfolio return of portfolio p at time t, \\(Inflation_{i,t}\\) is the inflation of country i at time t and \\(R_{p,t}^{Real}\\) is the corresponding real return. To understand how inflation works in our asset management setting, it is important to calculate and visualize the different portfolio returns. # First, load the consumer price index data CPI_data &lt;- read.csv(&quot;/Users/nikolasanic/Desktop/Master UZH/Data/A1_dataset_05_Ex_Session.txt&quot;, header = T, sep = &quot;\\t&quot;) # Create a time-series object CPI_date &lt;- as.Date(CPI_data[,1]) CPI_data_ts &lt;- xts(CPI_data[,-1], order.by = CPI_date) # Calculate the returns of the CPI (to calculate the inflation yearly as (CPI_t - CPI_{t-1})/ CPI_{t-1}) Inflation &lt;- Return.calculate(CPI_data_ts, method = &quot;discrete&quot;) # Only take the specific column we are interested in and delete the first row Inflation &lt;- Inflation[-1,1] # Calculate the real return, inflation adjusted Inflation_Adjusted_bigfour_EW_ret_ts &lt;- (1 + bigfour_EW_ret_ts[&#39;1988-07-29/2000-12-29&#39;]) / (1 + Inflation[&#39;1988-07-29/2000-12-29&#39;]) -1 # Get the cumulative product again Bigfour_cp_EW_real &lt;- cumprod(1+Inflation_Adjusted_bigfour_EW_ret_ts[&#39;1988-07-29/2000-12-29&#39;]) # Now, we can plot the underlying relationship ## First, get the cumprod() of the inflation to understand the behaviour over time inflation_cp &lt;- cumprod(1+Inflation[&#39;1988-07-29/2000-12-29&#39;]) ## Then merge the dataframes and assign correct names inflation_data &lt;- merge.xts(Bigfour_cp_EW[&#39;1988-07-29/2000-12-29&#39;], Bigfour_cp_EW_real[&#39;1988-07-29/2000-12-29&#39;], inflation_cp[&#39;1988-07-29/2000-12-29&#39;]) colnames(inflation_data) &lt;- c(&quot;Nominal Return&quot;, &quot;Real Return&quot;, &quot;Inflation&quot;) ## Plot the relationship tidy(inflation_data) %&gt;% ggplot(aes(x=index,y= value, color=series)) + geom_line() + scale_color_manual(values=c(&quot;goldenrod&quot;, &quot;dodgerblue1&quot;, &quot;darkorchid4&quot;)) + ylab(&quot;Cumulative Returns&quot;) + xlab(&quot;Time&quot;) + ggtitle(&quot;Cumulative Return of the Big Four from 1988 to 2000, Real and Nominal&quot;) + labs(color=&#39;Cumulative Return&#39;) + theme(plot.title= element_text(size=14, color=&quot;grey26&quot;, hjust=0.3,lineheight=2.4, margin=margin(15,0,15,45)), panel.background = element_rect(fill=&quot;#f7f7f7&quot;), panel.grid.major.y = element_line(size = 0.5, linetype = &quot;solid&quot;, color = &quot;grey&quot;), panel.grid.minor = element_blank(), panel.grid.major.x = element_blank(), plot.background = element_rect(fill=&quot;#f7f7f7&quot;, color = &quot;#f7f7f7&quot;), axis.title.y = element_text(color=&quot;grey26&quot;, size=12, margin=margin(0,10,0,10)), axis.title.x = element_text(color=&quot;grey26&quot;, size=12, margin=margin(10,0,10,0)), axis.line = element_line(color = &quot;grey&quot;)) As we can see, the increase in the difference between nominal and real return is explained by the increase in Inflation for the period from approximately 1992 on. 3.4 Individual Security Risk Risk is the other major concept in Asset Management. In essence, risk defines the extent to which the exact level of our return is uncertain. This is also known as level of volatility that individual assets have. Most investments inherently contain some level of risk. As regular individual investors, we are unlikely to take advantage of any mispricing in the market and create arbitrage profits. Consequently, there is no financial instrument with a non-zero risk component. The closest instrument to a truly risk-free security is a short maturity US Treasury security. This is because the likelihood of the US government defaulting on very short-term obligations is extremely low and other risks, such as inflation risk and liquidity risk, are also negligible for very short maturities. 3.4.1 Standard Deviation and Variance Although we can fairly easily understand the notion of risk, it is quite impossible to find a truly unbiased metric to quantify it. As such, a measure that quantifies all attributes of risk still eludes us. Thus, the measures of risk we use today, including those we discuss in this chapter, are all imperfect measures of risk. Despite this, two of the most important metrics in quantifying the risk of an asset that comes from its underlying volatility are Variance and Standard Deviation. We already discusses both metrics in Chapter 1, so we will only look at their application in a portfolio context here. Remember that the volatility is calculated as the average, squared difference between individual security returns and its mean return. As such, it is the average distance that each datapoint has to its corresponding “center”. We square the distance because we say that both positive and negative deviations from the expected value are considered risk (although sometimes we only care for negative deviations). The Variance is calculated as: \\[ \\sigma_i^2 = \\frac{1}{T-1}\\sum_{t=1}^T (R_{i,t} - \\bar{R_i})^2 \\] And the Standard Deviation is defined as its square root, \\(\\sigma = \\sqrt{\\sigma^2}\\) In order to calculate the variance and standard deviation, we just use the functions var() and sd() # Let&#39;s take the Roche example again variance_roche &lt;- var(A1_return_Roche_function_ts, na.rm = T) sd_roche &lt;- sd(A1_return_Roche_function_ts, na.rm = T) As with returns, it is important to understand how we work with different periodicities of the underlying risk variables. To recap what we have seen in the lecture related to different time horizons, we understand that as number of periods (T) increases the variance of the total return increases linearly and Standard deviation increases by \\(\\sqrt{T}\\). This is due to the additivity property we have seen. This implies that if we have identically (the bets are the same) and independent returns (the outcome of the bet today has no impact on the outcome of the bet in the next period) over time, then the overall variance \\(\\sum_{i=1}^T R_{i}\\) of total return will increase in periods: \\[ \\begin{align} Var(\\sum_{t=1}^T R_{i}) &amp;= \\sum_{i=1}^T \\sum_{j=1}^T \\sigma_{i,j} \\\\ &amp;= \\sum_{t=1}^T\\sigma_i^2 &amp;&amp; \\text{Independence Property: whenever i is not equal to j, then } \\sigma_{ij} = 0 \\text{ (as no covariation)}\\\\ &amp;= T\\sigma_i^2 &amp;&amp; \\text{Identically Distributed Property: } \\sigma_i = \\sigma_j \\end{align} \\] Given this notion, we understand that, in case of IID settings, we can annualise the standard deviation from daily as is: \\[ \\sigma_i^{Annual} = \\sigma_i*\\sqrt{365} \\] 3.5 Rolling Risk Characteristics Another important property of risk and return is dynamic factorisation of the underlying metrics. Although this sounds difficult, it actually is quite straight-forward. In essence, we use rolling measures of risk if we have a time series and want the standard deviation for a moving window. Transforming properties into a moving window roots in the idea of time-series dependence of returns. In general, we need to account for the fact that future returns are somehow related to past returns. However, we do not know how strong this relation is and how long this relation goes. As the market characteristics change constantly, we cannot ensure that returns from five years ago, when measured daily, have a material effect on current returns. Consequently, we need to account for the fact that time-series dependencies are limited and that returns that are too distant in the past may pose no real explanatory potential any longer. Exactly this idea is what then motivates us to use a rolling, or moving, approach when calculating portfolio metrics. In essence, when calculating a metric on a rolling basis, we define the time horizon, or width, of the function. Then, instead of taking the entire time-series up to the current date to calculate the metric, we just take a time-series of width T. Like that, we only take the T most recent observations to calculate the current measure. Then, for the next day, we shift by one day and calculate the metric for the new day accordingly. As such, a rolling risk metric is nothing else than a risk metric that is defined based on a shorter past time horizon. To use something in a rolling fashion implies to use it more dynamically than we would when considering the entire time horizon. We implement rolling risk metrics with the roll() package. This is a package that provides fast and efficient computation of rolling and expanding statistics for time-series data. # To show the roll function, let&#39;s take the Big Four data again: bigfour_EW_ret_ts_sd &lt;- roll_sd(bigfour_EW_ret_ts, width = 6, na_restore = T) head(as.data.frame(bigfour_EW_ret_ts_sd), 10) ## V1 ## 1988-07-29 NA ## 1988-08-31 NA ## 1988-09-30 NA ## 1988-10-31 NA ## 1988-11-30 NA ## 1988-12-30 0.06717991 ## 1989-01-31 0.06200490 ## 1989-02-28 0.07013633 ## 1989-03-31 0.06983444 ## 1989-04-28 0.06661992 What appears here is that the first five observations are all NA. This is intuitive, as the standard deviation consists of the five most recent returns only and then is updated each period accordingly. As a consequence, we only have a sufficient amount of observations from period 6 on. Thereby, R has not enough observations in Periods one to five and thus assigns a NA output. As a side note, you can use the package to calculate any type of rolling metric, such as roll_mean(), roll_var() roll_cov(), roll_lm() or roll_quantile(), to name a few. In order to get the entire options of the package, visit . 3.6 Portfolio Risk The previous section discusses how to calculate risk for an individual security. However, most investments are done in the context of a portfolio of securities. To understand portfolio risk, it is important to comprehend the fundamental notion of portfolio covariation based on the dependence structure of the underlying assets. Recapitulate quickly Chapter 3. In the Matrix Algebra section, we have seen that the covariance is important when assessing risk in the context of a portfolio. We stated that, unless assets are independent of each other, the covariance will contribute a certain part to the overall portfolio risk. In Chapter 3.2.6.2 we derived how to calculate the variance of a linear combination of random vectors. Then, in Chapter 3.2.7 we saw how to apply this linear combination within a portfolio setting. 3.6.1 Portfolio Risk of two securities To apply the theory from Chapter 3 to the portfolio setting, it is easiest if we start with the two asset case. As we derived, the risk of a portfolio with two assets is: \\[ \\sigma_p^2 = w_1^2\\sigma_1^2 + w_2^2\\sigma_2^2 + 2w_1w_2\\sigma_{12} \\] Whereas \\(\\sigma_{12}\\) is the covariance coefficient between both assets and is calculated as: \\(\\sigma_{12} = \\rho_{12}\\sigma_1\\sigma_2\\). Further \\(w_i\\) is the respective portfolio weight and \\(\\sigma_i^2\\) is the respective variance of the security. As with the return, we can calculate the risk characteristic in two different ways: With summation or with Matrix Algebra. We start again with the summation case: # First we take the Big Four PF but we only take the Roche_Holding and the Nestle_PS columns NR_df &lt;- as.data.frame(A1_bigfour_ret_ts[,c(1,3)]) # Now, we calculate the respective standard deviation and covariance metrics NR_df_N_sd &lt;- sd(NR_df$Nestle_PS) NR_df_R_sd &lt;- sd(NR_df$Roche_Holding) NR_df_cov &lt;- cov(NR_df$Nestle_PS, NR_df$Roche_Holding) # Then, we calculate the PF risk according to the formula above NR_PF_Risk_EW &lt;- NR_df_N_sd^2*0.5^2 + NR_df_R_sd^2*0.5^2 + 2*0.5*0.5*NR_df_cov # Lastly, we annualise it from monthly observations NR_PF_Risk_EW_ann &lt;- NR_PF_Risk_EW*sqrt(12) We can also calculate this in Matrix Form. To recapitulate, we derived that the general portfolio variance is calculated as: \\[ \\begin{align} var(\\textbf{w}&#39;\\textbf{R}) &amp;= w&#39;\\Sigma w \\end{align} \\] Note that we deliberately use w instead of x to acknowledge that this is the weighted portfolio risk formula. # First we take the Big Four PF but we only take the Roche_Holding and the Nestle_PS columns NR_df &lt;- as.matrix(A1_bigfour_ret_ts[,c(1,3)]) # Now, we calculate the respective covariance metrics NR_df_cov &lt;- cov(NR_df) # Then, we can create a vector consisting of average weights. Here, we have two securities, so each return will be multiplied with 1/2. weight_vector &lt;- matrix(rep(1/length(NR_df[1,]),each=length(NR_df[1,])), 1) # Lastly, we take the formula above and calculate the portfolio risk NR_PF_Risk_EW_matmul &lt;- weight_vector %*% NR_df_cov %*% t(weight_vector) # Now, let&#39;s see again if they are identical: ifelse(round(NR_PF_Risk_EW_matmul,7) == round(NR_PF_Risk_EW,7), &quot;Both calculations are identical&quot;, 0) ## [,1] ## [1,] &quot;Both calculations are identical&quot; As we can see, both ways to derive the portfolio risk are identical. 3.6.2 Portfolio Risk for Multiple Assets We can easily extend the idea behind portfolio risk for the general case of N variables by simply using the exactly same formulas as for the two assets case. However, when we start introducing a larger portfolio, we may notice something interesting. In particular, when we add assets that are not perfectly correlated with the securities in our portfolio, the overall risk of the portfolio decreases.This evidence was first introduced by Markowitz in 1950 and is known as Diversification. It constitutes one of the fundamental theorems of Portfolio Theory. Consequently, the risk of a well-diversified portfolio is lower than that of its individual counterparts. To understand the logic behind this, it is handy to look at the mathematical formulation. Let’s start again with the portfolio variance in the general case of N (not perfectly correlated) assets: \\[ \\begin{align} var(\\textbf{w}&#39;\\textbf{R}) &amp;= w&#39;\\Sigma w \\\\ &amp;= \\underbrace{\\sum_{i=1}^N w_i\\sigma_i^2}_{\\text{sum of the weighted variances}} + \\underbrace{\\sum_{i=1}^N\\sum_{j = 1, i \\neq j}^N w_iw_j\\sigma_{ij}}_{\\text{sum of the weighted covariances}} \\end{align} \\] As we understand, the variance of a portfolio consists of the weighted sum of its constituents as well as its covariance, given the assumption that the movements of the stocks are not independent from each other. Rephrasing of the Equation above results in Markowitz’s (1950) famous formula for portfolio risk. \\[ \\begin{align} var(\\textbf{w}&#39;\\textbf{R}) &amp;= \\frac{1}{n}\\bar{\\sigma_i}^2 + \\frac{n-1}{n}\\bar{\\sigma}_{ij} \\\\ &amp;= \\underbrace{\\frac{1}{n}\\bar{\\sigma_i}^2}_{\\text{Average Variance}} + \\underbrace{\\left(1 - \\frac{1}{n}\\right)\\bar{\\sigma}_{ij}}_{\\text{Average scaled covariance}} \\end{align} \\] Here, we can see the Average Variance as well as the Average scaled Covariance. In general notion, these two terms are the Idiosyncratic as well as the Systematic part of portfolio risk. To understand why both parts are called accordingly, it is important to define the portfolio risk regarding its dependence on the portfolio size N. As we can see, both parts depend on the number of securities within a portfolio. Consequently, once we increase N, (=as N approaches infinity), the first term gradually approaches zero, whereas the latter increases monotonically. As such, the Average Variance will approach zero and is thus called the Idiosyncratic Part of the portfolio risk, since it can be diversified away with an increasing portfolio size. However, the latter part will, in the extreme case of \\(n \\rightarrow \\infty\\), be exactly the average covariance between the assets. This is thus called the Systematic Part of the portfolio risk, since it cannot be diversified away with increasing portfolio size. Thereby, this latter term only depends on portfolio selection abilities, as a well-diversified portfolio of the manager, in theory, aims at reducing the co-dependence of its constituents to decrease overall volatility. To see this principle in action, let’s take the Big Four portfolio again. Here, we will see a small example that enables you to understand how an increasing portfolio size leads to a lower portfolio risk. Note that I will make use of for loops here as there is much repetition in the code otherwise. But don’t worry, I’ll explain every step. # First, we need to delete the first column as this has so few observations that we cannot coerce it with any other values. A4_returns_cap &lt;- A4_returns[,-1] # We start by defining the following for loop: ## As we know the usual form of a loop is given by: for(i in ... ) { ## some conditions ## } for (i in 1:ncol(A4_returns_cap)){ # Here, we say we go through each column of the A4_returns_cap data frame. That is, we tell R to loop through each column of that data frame to get the PF risk for each column. weight_vector &lt;- matrix(rep(1/length(A4_returns_cap[1, c(1:i)]),each=length(A4_returns_cap[1, c(1:i)])), 1) # Remember we want to calculate the PF risk according to the matrix algebra formula from above. To do so, we first need the weight vector. This is done as we did it before. # That is: ## 1) We create a matrix by entering repeated fractions (e.g. if we have two, then we get 1/length(A4_returns_cap[1, c(1:2)]) = 1/2 and repeat this fraction: length(A4_returns_cap[1, c(1:2)])) = 2 times --&gt; giving the vector [0.5, 0.5]) ## 2) Since we want to see how the PF risk changes when adding more stocks, we need to do this for an increasing stock combination. That is what [1, c(1:i)] does. The c(1:i) defines how many columns should be used to calculate the weight vector. Since we defined i as being each number of column elements (in our case: 1,2,3,4,5), we can easily loop over each column and thereby include one column more in each loop. cov_matrix &lt;- cov(na.omit(A4_returns_cap[,c(1:i)])) # Here, we copy the logic from above but just calculate the covariance matrix. Note that, in order not to get a matrix full of NA&#39;s, you need to use na.omit() before defining the frame you want to calculate your covariance matrix on. df_temp_var &lt;- weight_vector %*% cov_matrix %*% t(weight_vector) # Here, we just calculate the respective PF Risk consisting of the Variance of the Portfolio (we do not take the square root here). col_name_pf_risk &lt;- paste0(i, &quot;_Fold_PF&quot;) # This is to define the column names for the dataframe in which we combine all the results from all the loops. # Now, we defined all the functions and are able to create them through the loop structure. We must now just define how R should output all results (preferably in a data frame with defined column names to see the progress of the loop structure). if (colnames(A4_returns_cap)[i] == &quot;Nestle_PS&quot;) { df_final_temp_var &lt;- df_temp_var col_name_pf_risk_final &lt;- col_name_pf_risk # This here is where we create the output structure. Since we are operating in a loop, we tell R to do the same thing for just other combinations of data. In order to summarise and output everything on the same page, we need to define how R should handle this output. # Let&#39;s start with the first output. This is the first column the loop is running through. By defining if (colnames(A4_returns_cap)[i] == &quot;Nestle_PS&quot;) ..., we state that if the column name is Nestle_PS (which is the column name of the FIRST column), then R should assign the PF Risk quantity of the first PF as well as its respective column name to two distinct lists. That is the first element of the lists then. } else { df_final_temp_var &lt;- cbind(df_final_temp_var, df_temp_var) col_name_pf_risk_final &lt;- cbind(col_name_pf_risk_final, col_name_pf_risk) # Now, we define what to do with all other elements in the list (e.g. what to do with the output of the second loop onward). There, we just say that we need to column bind (cbind) the output of PF risk as well as its respective column name. That is, we create a list of columns to which then the outputs of each loop are subsequently added. # Like that, we create an output structure for our loop. } } # Here, we then define the dataframe and assign column as well as row name(s) to the final, looped output. df_final_temp_var &lt;- as.data.frame(df_final_temp_var) colnames(df_final_temp_var) &lt;- col_name_pf_risk_final rownames(df_final_temp_var) &lt;- &quot;PF Risk (Variance)&quot; # Lastly, we define an output of annualised PF Risk. df_final_temp_var_annual &lt;- df_final_temp_var*12 df_final_temp_var_annual ## 1_Fold_PF 2_Fold_PF 3_Fold_PF 4_Fold_PF 5_Fold_PF 6_Fold_PF 1_Fold_PF 2_Fold_PF 3_Fold_PF ## PF Risk (Variance) 0.002602892 0.0008820035 0.002068171 0.001554444 0.001044176 0.001259599 0.003183536 0.002034153 0.001410742 As is applicable, we can see that the portfolio risk, given the chosen metric, is decreasing with increasing portfolio constituents. As such, it can be shown that, as N approaches infinity, overall risk (given as portfolio variance) gradually declines until it only depends on the covariance of the individual securities (only systematic risk components). By adding more stocks, the portfolio risk in terms of variance will decline, as it no longer depends on idiosyncratic, but solely systematic risk components, which is a good sign for its accuracy. However, factors which cannot be diminished by diversification are thus not accurately accounted for. Especially, since the variance or standard deviation are a mean, it masks extreme situations, understating systemic events. As a consequence, this form of risk is a necessary, but not fully sufficient measure of overall portfolio risk. 3.7 Risk in Extremes and Alternative Risk Measures We already talked in the lecture about the insufficiencies of variances and standard deviations and stressed the need for different assessment tools. Note that we just mentioned that, since both metrics are a mean, they automatically mask extreme situations and understate systematic events. As such, although they may be able to quantify volatility in normal times, risk in extremes should be quantified by other measures. 3.7.1 Value at Risk (VaR) A popular measure of the risk of loss in a portfolio is Value-at-Risk or VaR. VaR measures the loss in our portfolio over a pre-specified time horizon, assuming some level of probability. That is, it states what the maximum loss of a portfolio is expected with a certain probability. Although we are free to decide which level of probability we want to choose, the usual terms are 1% as well as 5% levels. Therein, we will use the market convention of representing VaR as a positive number and using the significance level. In order to apply the VaR measures, we follow two strategies: Gaussian VaR and Historical VaR. The former assumes that the data follow a normal or Gaussian distribution, while the latter uses the distributional properties of the actual data. As such, Historical VaR requires more data to implement and Gaussian VaR requires less data to implement 3.7.1.1 Gaussian VaR One of the simplest approaches to estimate VaR is to assume that the portfolio returns follow a normal distribution. Hence, this approach is called Gaussian VaR. Because of the distributional assumption, we only need at least 1 year of daily returns data, which is approximately 252 observations. Then, we compute the the VaR at a given level of \\(\\alpha\\) by following standard test statistic calculations: \\[ VaR_\\alpha = -(\\mu - \\sigma Z_\\alpha)I \\] whereas \\(\\alpha\\) is the significance level of the VaR, \\(\\mu, \\sigma\\) are the portfolio return and standard deviation, respectively, \\(Z_\\alpha\\) is the z-score based on the VaR significance level, and I is the current portfolio value. The Z-score is calculated using the function qnorm(). You may recall this from the Reading of Week 1. Note that there were the functions for distribution calculations (d, p, q, r). Therein, we stated that q defines the value of a certain quantile, given the underlying distribution and property assumptions. This is the function that returns the inverse values of the CDF. It takes thus an argument at the desired level of significance and gives you the number which corresponds to this quantile level in the CDF, thereby telling you which value incorporates exactly \\(\\alpha\\) percent of the probability mass. # Let&#39;s use the equal-weighted return of the big four here: bigfour_EW_ret_ts # We first calculate mean and standard deviation bf_mean &lt;- mean(bigfour_EW_ret_ts, na.rm = T) bf_sd &lt;- sd(bigfour_EW_ret_ts, na.rm = T) # Also assume that our PF had a value of 1&#39;000&#39;000 I = 1000000 # Then, we just use the formula above to get the respective VaR at given levels of alpha VaR_5 &lt;- abs(round((bf_mean + qnorm(0.05)*bf_sd)*I,1)) VaR_1 &lt;- abs(round((bf_mean + qnorm(0.01)*bf_sd)*I,1)) # Let&#39;s print it to see the maximum loss at a given level of confidence, given a Normal Distribution assumptions cat(&quot;The VaR at an alpha level of 5% is: &quot;, VaR_5, &quot;\\nThe VaR at an alpha level of 1% is: &quot;, VaR_1) ## The VaR at an alpha level of 5% is: 67490.9 ## The VaR at an alpha level of 1% is: 98843.9 3.7.1.2 Historical VaR Historical VaR uses a mix of current weights in the portfolio and a simulation of historical returns of the securities in the portfolio to construct a simulated series of portfolio profits and losses. Given its use of historical returns data, Historical VaR works better when we have lots of data. Typically, 3–5 years of data is recommended for this approach. The main difference in it is that, instead of assuming a given distribution of the underlying returns of the portfolio, we derive the return assumptions by looking at the historical distirbution of the returns and assuming that the future distribution will be similar. This is done using the quantile() function. # Also assume that our PF had a value of 1&#39;000&#39;000 I = 1000000 # Calculate the appropriate metrics VaR_1_hist &lt;- round((quantile(bigfour_EW_ret_ts,0.99))*I,1) VaR_5_hist &lt;- round((quantile(bigfour_EW_ret_ts,0.95))*I,1) # Let&#39;s print it to see the maximum loss at a given level of confidence, given a Normal Distribution assumptions cat(&quot;The VaR at an alpha level of 5% is: &quot;, VaR_5_hist, &quot;\\nThe VaR at an alpha level of 1% is: &quot;, VaR_1_hist) ## The VaR at an alpha level of 5% is: 83508.2 ## The VaR at an alpha level of 1% is: 125733.2 3.7.1.3 Plotting VaR Sometimes it may be easier for us to visualize the data by plotting (i) the density of the simulated portfolio profits and losses, (ii) the normal distribution of profits and losses based on the mean and standard deviation of the simulated portfolio profits and losses, and (iii) our estimates of the 1 and 5% 1-Day Historical VaR. This gives us a better overview of # First define the density of the simulated portfolio profits and losses density_bf &lt;- bigfour_EW_ret_ts*I # Then, define the normal distribution of the portfolio values based on the mean and standard deviation of the simulated portfolio profits and losses. norm_bf &lt;- rnorm(392, mean = bf_mean*I, sd = bf_sd*I) VaR_dist &lt;- merge.xts(cbind(density_bf, norm_bf)) var_dist_melt &lt;- melt(data.frame(VaR_dist)) ## No id variables; using all as measure variables var_dist_melt %&gt;% ggplot(aes(x=value, fill = variable)) + geom_density(alpha = 0.2) + scale_color_manual(values=c(&quot;goldenrod&quot;, &quot;dodgerblue1&quot;, &quot;darkorchid4&quot;)) + scale_fill_manual( values = c(&quot;goldenrod&quot;, &quot;dodgerblue1&quot;)) + ylab(&quot;Density&quot;) + xlab(&quot;Portfolio Profit&quot;) + ggtitle(&quot;Density of Simulated Portfolio Profit and 1% and 5% 1−Day Historical (VaR)&quot;) + geom_vline(xintercept = -quantile(-VaR_dist$density_bf, 0.99), col = &quot;dodgerblue1&quot;, linetype = &quot;dashed&quot;) + geom_vline(xintercept = -quantile(-VaR_dist$density_bf, 0.95), col = &quot;darkorchid4&quot;, linetype = &quot;dashed&quot;) + labs(color=&#39;Cumulative Return&#39;) + theme(plot.title= element_text(size=14, color=&quot;grey26&quot;, hjust=0.3,lineheight=2.4, margin=margin(15,0,15,45)), panel.background = element_rect(fill=&quot;#f7f7f7&quot;), panel.grid.major.y = element_line(size = 0.5, linetype = &quot;solid&quot;, color = &quot;grey&quot;), panel.grid.minor = element_blank(), panel.grid.major.x = element_blank(), plot.background = element_rect(fill=&quot;#f7f7f7&quot;, color = &quot;#f7f7f7&quot;), axis.title.y = element_text(color=&quot;grey26&quot;, size=12, margin=margin(0,10,0,10)), axis.title.x = element_text(color=&quot;grey26&quot;, size=12, margin=margin(10,0,10,0)), axis.line = element_line(color = &quot;grey&quot;)) 3.8 Market Efficiency and independence of risk and return We have seen in the lecture that we primarily assume that returns are IID. That is, they are independent of each other and identically distributed. The first argument thus implies that the return of this period should not influence returns of the next periods. Consequently, the outcome of the second bet is unaffected by the outcome of the first. This is also an important notion of the efficient markets hypothesis (EMH). In it, we state that it is very difficult to develop a consistently profitable trading strategy. The EMH says that market prices fully reflect available information and prices should be unpredictable. To make this clear, this does not imply that the EMH states prices can never be predicted. What the EMH means is that any predictability will be competed away quickly by traders and any actionable mispricing will disappear. In order to understand whether returns are indeed independent of each other, it is therefore necessary to conduct a test of independence. There are numerous tests to depict such independence, but we’ll cover the following. 3.8.1 Variance Ratio Test One such test is based on the examination of variances of different horizons. This is also known as Variance Ratio test and was first introduced by Poterba and Summers (1988). They create tests for serial correlation in stock returns using variance ratios. By assessing different ratios, they can understand whether the variances of different time horizons are proportional to each other. Especially, they test: \\[ VR(k) = \\frac{Var(R_t^k)}{k} / \\frac{Var(R_t^{12})}{12} \\] whereas \\(\\frac{Var(R_t^k)}{k}\\) is the variance of a k month return, scaled by the months of the return. They state that, if the EMH holds and returns are independent of each other, then the ratio of the variance between two periods should be not significantly different from each other (it should be equal to one). The logic behind this ratio follows suit the idea we elaborated on previously, when we assessed how risk behaves under our IID assumptions over time. Let’s further explore the idea behind the VR. We start by assuming a random walk theory process. That is, the stock price of securities follows a random walk that depends on an IID error term that is normally distributed with mean zero and variance \\(\\sigma^2\\): \\(N(0, \\sigma^2)\\). This is the underlying idea of independence, and thus market efficiency. Now remember the IID properties and what they imply for calculating t-period risk. If we assume independence of returns, we define that the covariance between returns is zero (no serial correlation). Further assuming that the variances are identically distributed, we obtain an linear increase of the variance of total returns with the number of observational periods (k), implying that: \\(Var(R_t^k) = \\frac{k}{1}Var(R_t^1)\\). This is just what we derived earlier. Now, let’s transfer this observation to the VR. Looking at the ratio, we first could write \\(Var(R_t^k)\\) in terms of \\(Var(R_t^12)\\). Under the assumption of independence, the k’th period variance is nothing else than the 12-period variance scaled by a factor of k/12: \\(Var(R_t^k) = \\frac{k}{12}Var(R_t^{12})\\). This is also why we wrote \\(\\frac{k}{1}\\) in the formula above to show you the proportion of going from 1 period to 12 periods. Now, if we add this relation between the k’th and 12’th period variance under the assumption of independence into our formula, then we realize that it will cancel each other out. \\[ \\begin{align} VR(k) &amp;= \\frac{Var(R_t^k)}{k} / \\frac{Var(R_t^{12})}{12} \\\\ &amp;= \\frac{\\frac{k}{12}Var(R_t^{12})}{k} / \\frac{Var(R_t^{12})}{12} &amp;&amp; \\text{Assumption of Independence = Proportionality to k}\\\\ &amp;= \\frac{Var(R_t^{12})}{12} / \\frac{Var(R_t^{12})}{12} &amp;&amp; \\text{k cancels each other out}\\\\ &amp;= 1 \\end{align} \\] As such, we found that, under independence assumption, the variances of different periods are proportional by a factor of k. If this factor is indeed k, then the VR components will cancel each other out and result in a VR equal to 1 (or at least not materially different from 1). However, in a mean-reverting case, the previous returns inversely impact subsequent ones, implying an under-proportional spread. As such, k period variance is smaller than the k/12 one year variance, implying a remaining factor in the denominator. Thus, mean reversion will render the VR below one. Consequently, we were able to show mathematically that the VR output is able to determine, or test, the independence assumption. Let’s understand how we can transform this idea into our empirical setting. This is done by using the rollapply() function in combination with the var() function. # For this, we will calculate the Variance Ratios for four different periods. I will follow a similar approach by looping. Just now, it is up to you to understand what the loop does in which parts. NR_df &lt;- as.data.frame(A1_bigfour_ret_ts[,c(1,3)]) # To understand the VR implications, let&#39;s take the return of Nestle and Roche again ## We define the annual variance through &quot;roll applying&quot; the cumulative product of returns throughout a period of 12, then take the variance of t_i. That is, we: ## 1) apply a rolling function on the cumulative return of 12 periods to get 1-year rolling returns ## 2) take the variance of the respective 1-year rolling returns to get the 12-period horizon variance. EW_Var_12_N &lt;- var(rollapply(NR_df$Nestle_PS ,12,function(x) prod(x+1)-1), na.rm = T) EW_Var_12_R &lt;- var(rollapply(NR_df$Roche_Holding ,12,function(x) prod(x+1)-1), na.rm = T) ## Then, we define different time horizons (k) to set the 12-period variance into proportion to the k-period variances. times &lt;- c(1,24,36,48) # NOTE: We do not comment the loop as precisely as above, since the loop technique and syntax is the same as for the loops before. As such, only the input function(s) change, which we comment accordingly. for (i in times){ # Applying the VR function in a loop for the respective time periods. EW_Var_N &lt;- var(rollapply(NR_df$Nestle_PS ,i,function(x) prod(x+1)-1), na.rm = T) EW_VR_N &lt;- (EW_Var_N/i)/(EW_Var_12_N/12) EW_Var_R &lt;- var(rollapply(NR_df$Roche_Holding ,i,function(x) prod(x+1)-1), na.rm = T) EW_VR_R &lt;- (EW_Var_R/i)/(EW_Var_12_R/12) col_name_EW &lt;- paste0(i, &quot;_Month&quot;) if (i == 1){ # Define the condition(s) to create a new dataframe and bind the columns EW_VR_final_N &lt;- EW_VR_N EW_VR_final_R &lt;- EW_VR_R col_name_EW_final &lt;- col_name_EW } else { EW_VR_final_N &lt;- cbind(EW_VR_final_N,EW_VR_N) # Conditions to bind the remaining columns instead of overwriting the loop EW_VR_final_R &lt;- cbind(EW_VR_final_R,EW_VR_R) col_name_EW_final &lt;- cbind(col_name_EW_final, col_name_EW) } } # Create dataframe data_frame_ratios &lt;- as.data.frame(rbind(EW_VR_final_N, EW_VR_final_R)) Annual_Var &lt;- as.data.frame(rbind(EW_Var_12_N, EW_Var_12_R)) data_frame_ratios &lt;- as.data.frame(cbind(Annual_Var, data_frame_ratios)) col_name_EW_final &lt;- cbind(&quot;Annual Variance&quot;, col_name_EW_final) # Define the according names for the dataframe (columns and rows) names(data_frame_ratios) = col_name_EW_final rownames(data_frame_ratios) &lt;- c(&quot;Nestle&quot;, &quot;Roche&quot;) data_frame_ratios ## Annual Variance 1_Month 24_Month 36_Month 48_Month ## Nestle 0.03691215 0.7809401 0.9262487 1.084813 1.214529 ## Roche 0.05396368 0.7069857 1.5340426 2.458771 3.417456 "],["beta.html", "Chapter 4 Beta 4.1 Application of Regression Analysis: \\(\\beta\\) analysis in Financial Market Settings", " Chapter 4 Beta 4.1 Application of Regression Analysis: \\(\\beta\\) analysis in Financial Market Settings We now covered the theoretical fundamentals of both the statistical and algebraic properties and defined a method to apply these properties into a coherent cause-and-effect analysis. We also understand the idea of co-movements and variation within a this framework and can depict how to account for the pure covariation of two variables within a multi-dimensional setting. Lastly, we do understand the sample properties of the estimators at hand and derive how to check for both the assumptions made within the regression settings as well as the model fit. Now, we are eager to apply our knowledge within a financial domain. In essence, we understand that the term \\(\\beta\\), in statistics, is simply the coefficient of an explanatory variable \\(x_i\\) within a regression setting. It states the average change in the dependent variable when, keeping all other variables constant, the explanatory variable changes by one unit, whereas unit is a measurement indicator (such as percentage points, price in dollars, years etc.). This holds true for many applications within financial econometric settings, which combines financial and economic theory with statistical quantification. However, within financial economics, we usually think of \\(\\beta\\) in terms of the sensitivity of the security return relative to variations in the market return. Most famously, we associate \\(\\beta\\) with the following formula: \\[ \\hat{\\beta} = \\frac{cov(r_i,r_m)}{var(r_m)} \\] Mathematically speaking, in financial economics, we usually define \\(\\beta\\) as the covariance of the individual security return with the market return relative to the variance of the market return (= the covariance of the market return with itself). We can see that this is solely a specification of the general \\(\\beta\\) factor that associates co-movements of any dependent and explanatory variable with the variation of the explanatory variable itself. Within the general OLS setting, we simply defined the former as \\(x_i\\) and the latter as \\(y_i\\). As such, the general regression coefficient states nothing else than the variation of a variable based on the relational variation with a different variable. In the case of financial economics, we derive from economic theory that a covariation between individual returns and underlying market returns may be likely. However, from an econometric point of view, this is identical to the derivation of the general regression coefficient within a simple OLS framework. Consequently, the method of finding a \\(\\beta\\) factor that explains security returns is coherent with the general regression framework technique. In this chapter, we start exploring how researchers came to this result, what it implies and whether it actually reflects market settings. Further, we obtain a more econometric setting and want to understand how we can use econometric functions through regression techniques based on financial economic relationships in R. 4.1.1 The Single Index Model The relationship portrayed above is the key factor of one of the most fundamental asset pricing models: The Single-Index Model. The Single-Index model (SIM) is a simple asset pricing model to measure both the risk and the return of a stock as well as their interconnections. The model has been developed by William Sharpe in 1963. In essence, it assumes that variations of individual securities are linearly related to variations of a common underlying market structure, or factor. In this case, market structure is defined as an index to which the security belongs. The SIM thus builds on the previous notions of risk we encountered when assessing the security variance as tool to measure risk. Remember, in Chapter 4.4.4 as well as in the Linear Algebra repetition we have shown that the portfolio variance in the general case of N (not perfectly correlated) assets is: \\[ var(w&#39;\\textbf{R}) = \\frac{1}{n}\\bar{\\sigma}^2_i + (1-\\frac{1}{n})\\bar{\\sigma}_{ij} \\] That is, the risk of a portfolio depends on both the variation of the individual securities as well as their covariation with the underlying assets. In general notion, these two terms are the Idiosyncratic as well as the Systematic part of portfolio risk. As we showed, the variance of a portfolio decreases with an increasing amount of imperfectly correlated assets. This process, known as diversification, cancels out the idiosyncratic part of risk, leaving only the systematic part of risk left within a portfolio. As such, each well-diversified portfolio solely depends on the covariation between the underlying assets. In order to measure said systematic risk component, we need to capture the common components of the covariation properties of the underlying assets. In essence, we need to quantify a common component on which the set of security returns jointly depends and which captures the required compensation to risk within a portfolio setting. In order to do so, the SIM explains the covariance and correlation structure among asset returns as resulting from common exposures to an underlying market index. That is, it quantifies the security’s risk as its covariation properties relative to the underlying, common variation of a market factor. In other words, it measures risk by the co-movements of a security and market return relative to the variation of the underlying market. This relative co-dependence to a common factor is measured by the \\(\\beta\\) factor we have encountered before. 4.1.1.1 Assessing the co-dependence of security returns and a common factor To understand the intuition behind the SIM, we make use of the co-dependence of the security returns to a common, underlying factor. To see why we hypothesize this relationship, let’s look at the prices and returns of the Big Four companies and a common index, the Swiss Market Index (SM) # Read in the datasets stocks &lt;- read.csv(&quot;~/Desktop/Master UZH/Data/A2_dataset_01_Ex_Session.txt&quot;, header = T, sep = &quot;\\t&quot;) SMI &lt;- read.csv(&quot;~/Desktop/Master UZH/Data/A2_dataset_03_Ex_Session.txt&quot;, header = T, sep = &quot;\\t&quot;) rf &lt;- read.csv(&quot;~/Desktop/Master UZH/Data/A2_dataset_02_Ex_Session.txt&quot;, header = T, sep = &quot;\\t&quot;) # Select only the columns of interest stocks &lt;- stocks %&gt;% select(c(Date, Nestle_PS, Roche_Holding, Novartis_N, UBS_N)) %&gt;% filter(Date &gt; &quot;1997-08-29&quot;) SMI &lt;- SMI %&gt;% select(c(Date, SMI)) %&gt;% filter(Date &gt; &quot;1997-08-29&quot;) # Bind both together stocks_SMI &lt;- cbind(stocks, SMI) stocks_SMI &lt;- subset(stocks_SMI[, -6]) # Transform all into xts objects stocks_SMI_ts &lt;- xts(stocks_SMI[,-1], order.by = as.Date(stocks_SMI[,1])) stocks_SMI_ts_ret &lt;- Return.calculate(stocks_SMI_ts, method = &quot;discrete&quot;)[-1,] First, let’s look at the returns of the companies and index tidy(stocks_SMI_ts_ret) %&gt;% ggplot(aes(x=index,y= value, color=series)) + geom_line() + ylab(&quot;Returns&quot;) + xlab(&quot;Time&quot;) + ggtitle(&quot;Return of the Big Four and SMI from 1988 to 2000&quot;) + labs(color=&#39;Cumulative Return&#39;) + theme(plot.title= element_text(size=14, color=&quot;grey26&quot;, hjust=0.3,lineheight=2.4, margin=margin(15,0,15,45)), panel.background = element_rect(fill=&quot;#f7f7f7&quot;), panel.grid.major.y = element_line(size = 0.5, linetype = &quot;solid&quot;, color = &quot;grey&quot;), panel.grid.minor = element_blank(), panel.grid.major.x = element_blank(), plot.background = element_rect(fill=&quot;#f7f7f7&quot;, color = &quot;#f7f7f7&quot;), axis.title.y = element_text(color=&quot;grey26&quot;, size=12, margin=margin(0,10,0,10)), axis.title.x = element_text(color=&quot;grey26&quot;, size=12, margin=margin(10,0,10,0)), axis.line = element_line(color = &quot;grey&quot;)) The time plots of returns show some common movements among the stocks that are similar to movements of the SMI. The figure shows further that individual stock returns are more volatile than the SMI returns, and that the movement in stock returns tends to follow the movements in the SMI returns indicating positive covariance and correlations. Let’s examine this more thoroughly. # Calculate and display the codependence structure vcov_bigfour_smi &lt;- cov(stocks_SMI_ts_ret) corr_bigfour_smi &lt;- cov2cor(vcov_bigfour_smi) corrplot.mixed(corr_bigfour_smi, upper=&quot;ellipse&quot;) All returns are positively correlated and each stock return has the highest positive correlation with the SMI. Further, we can plot the relation between both the individual security return and the SMI and assess to what extent it is linear. # Define a list with names of the companies names = list(&quot;Nestle&quot;, &quot;Novartis&quot;, &quot;Roche&quot;, &quot;UBS&quot;) # Redefine a data frame stocks_SMI_df_ret &lt;- data.frame(stocks_SMI_ts_ret) colnames(stocks_SMI_df_ret) &lt;- c(&quot;Nestle&quot;, &quot;Novartis&quot;, &quot;Roche&quot;, &quot;UBS&quot;, &quot;SMI&quot;) # Create another for loop to plot multiple relationships for (i in 1:(ncol(stocks_SMI_df_ret)-1)){ assign(paste(&quot;figure_&quot;, names[i], sep = &quot;&quot;), stocks_SMI_df_ret %&gt;% ggplot(aes_string(x = &quot;SMI&quot;, y = names[[i]])) + geom_point(width = 0.2, alpha = 0.6, color = &quot;deepskyblue4&quot;) + geom_smooth(method = &quot;lm&quot;, se = F, col = &quot;deeppink4&quot;, linetype = &quot;dashed&quot;) + ylab(paste(&quot;Returns&quot;, names[i], sep = &quot; &quot;)) + xlab(&quot;SMI Returns&quot;) + # Title string # ggtitle(&quot;Relationship of SMI and Big Four Company Return&quot;) + labs(color=&#39;Factor Portfolios&#39;) + theme( # Title Elements plot.title= element_text(size=14, color=&quot;grey26&quot;, hjust=0.3,lineheight=0.4, margin=margin(15,0,15,0)), axis.title.y = element_text(color=&quot;grey26&quot;, size=12, margin=margin(0,10,0,10)), axis.title.x = element_text(color=&quot;grey26&quot;, size=12, margin=margin(10,0,10,0)), # Background colour and fill panel.background = element_rect(fill=&quot;#f7f7f7&quot;), plot.background = element_rect(fill=&quot;#f7f7f7&quot;, color = &quot;#f7f7f7&quot;), # Major Panel Grids panel.grid.major.x = element_blank(), panel.grid.major.y = element_line(size = 0.5, linetype = &quot;solid&quot;, color = &quot;grey&quot;), # Minor Panel Grids panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank(), # Line colour of the x and y axis axis.line = element_line(color = &quot;grey&quot;))) } # Take them together in a multi-plot Rmisc::multiplot(figure_Nestle, figure_Novartis, figure_Roche, figure_UBS, cols = 2) The scatterplots show that as the market return increases, the returns on each stock increase in a linear way. 4.1.1.2 Sharpe’s Single-Index Model Based on the observations made above, William Sharpe hypothesized that a single security’s return is described by systematic (macroeconomic) uncertainty component (which is assumed to be well represented by a single index of stock returns) unique (microeconomic) uncertainty component (which is represented by a security-specific random component) And that these securities tend to move together driven by the same the same forces, which can be approximated by a single factor, or index. For that, let \\(R_{it}\\) denote the simple return on asset i over the investment horizon between times t-1 and t. Further, let \\(R_{mt}\\) denote the simple return on a well diversified market index portfolio (e.g. the SMI). The SIM has the form: \\[ R_{it} = \\alpha_i + \\beta_iR_{mt} + \\epsilon_{it} \\] Whereas: \\[ \\begin{align} R_{mt} &amp;\\sim iid N(0,\\sigma_m^2) \\\\ \\epsilon_{it} &amp;\\sim N(0, \\sigma_{\\epsilon,i}^2) \\\\ cov(R_{mt}, \\epsilon_{is}) &amp;= 0 &amp;&amp; \\text{no co-dependence of any security&#39;s error term w/ market } \\\\ cov(\\epsilon_{it}, \\epsilon_{is}) &amp;= 0 &amp;&amp; \\text{no co-dependence of any security&#39;s error with others} \\end{align} \\] The SIM assumes that all individual asset returns, \\(R_{it}\\), are covariance stationary and are a linear function of the market index return (systematic uncertainty), \\(R_{mt}\\) and an independent error term, \\(\\epsilon_{it}\\) (unique uncertainty). 4.1.1.3 Economic Interpretation of the variables First, let’s look at \\(\\beta_i\\). It represents the slope coefficient in the linear relationship between \\(R_{it}\\) and \\(R_{mt}\\). It is written as the partial derivative of the SIM w.r.t. the Market Index Return: \\[ \\frac{\\delta SIM}{\\delta R_{mt}} = \\beta_i \\] That, is \\(\\beta_i\\) is the average effect on the security return, \\(R_{it}\\), when the market return, \\(R_{mt}\\), changes by one unit. The larger (smaller) \\(\\beta_i\\), the more extreme (less extreme) is the variation of asset i relative to the variation of the market return. Further, we regard the coefficient in terms of portfolio risk budgeting. Given the formula for \\(\\hat{\\beta}\\), the covariance is a relative quantity proportional to the overall variation of the underlying index. Consequently, \\(\\hat{\\beta}\\) is proportional to the marginal volatility contribution of asset i to the volatility of the market portfolio. Therefore, assets with large (small) values of \\(\\hat{\\beta}\\) contribute more (less) heavily to the overall volatility of the market portfolio. From this point of view, the coefficient of the SIM can be interpreted as measure of portfolio risk. More allocations to assets with high (low) values will increase (decrease) portfolio risk (as measured by portfolio volatility) due to the relative volatility principle. The derivation of the coefficient is rather straightforward and displays clearly the covariation properties: \\[ \\begin{align} cov(R_{it}, R_{mt}) &amp;= cov(\\alpha_i + \\beta_iR_{mt} + \\epsilon_{it}, R_{mt}) \\\\ &amp;= \\underbrace{cov(\\alpha_i, R_{mt})}_{=0} + \\beta_i\\underbrace{ cov(R_{mt}, R_{mt})}_{=var(R_{mt})} + \\underbrace{cov(\\epsilon_{it}, R_{mt})}_{=0} \\\\ &amp;\\rightarrow \\beta_i = \\frac{cov(R_{it}, R_{mt})}{var(R_{mt})} \\end{align} \\] In addition, we can think of \\(R_{mt}\\) as capturing “market-wide” news at time t that is common to all assets and \\(\\beta_i\\) captures the sensitivity or exposure of asset i to this market-wide news. Further, we can think of \\(\\epsilon_{it}\\) as capturing specific news to asset i that is unrelated to market news or to specific news to any other asset j. Lastly, \\(\\alpha_i\\) can be interpreted as the expected return if the expected market return is exactly zero. Consequently, we see that: \\(\\epsilon_{it}\\): part of the return independent of the index \\(\\beta_iR_{mt}\\): part of the return due to index fluctuations 4.1.1.4 Statistical Properties of the SIM Single-Asset properties The following statistical properties hold for the SIM: \\[ \\begin{align} E[R_{it}] &amp;= \\alpha_i + \\beta_iR_{mt}\\\\ var[R_{it}] &amp;= \\beta_i^2var(R_{mt}^2) + \\sigma_{\\epsilon,i}^2 \\\\ cov[R_{it}, R_{jt}] &amp;= \\beta_i\\beta_j\\sigma_m^2 \\end{align} \\] Looking more thoroughly at the overall security variance, we see that it consists of two parts: \\[ var[R_{it}] = \\underbrace{\\beta_i^2\\sigma_{m}^2}_{\\text{market variance (sytematic part)}} + \\underbrace{\\sigma_{\\epsilon,i}^2}_{\\text{asset specific variance (unique part)}} \\] Portfolio properties A nice feature of the SI model for asset returns is that it also holds for a portfolio of asset returns. This property follows because asset returns are a linear function of the market return. To illustrate this, let’s look at a portfolio with two assets: \\[ \\begin{align} R_{1t} &amp;= \\alpha_1 + \\beta_1R_{mt} + \\epsilon_{1t} \\\\ R_{2t} &amp;= \\alpha_2 + \\beta_2R_{mt} + \\epsilon_{2t} \\end{align} \\] Then, the portfolio return with weights \\(x_1\\) and \\(x_2\\) is: \\[ \\begin{align} R_{pt} &amp;= x_1(\\alpha_1 + \\beta_1R_{mt} + \\epsilon_{1t}) + x_2(R_{2t} + \\alpha_2 + \\beta_2R_{mt} + \\epsilon_{2t}) \\\\ &amp;= (x_1\\alpha_1)x_2\\alpha_2) + (x_1\\beta_1 + x_2\\beta_2)R_{mt} + (x_1\\epsilon_{1t} + x_2\\epsilon_{2t}) \\\\ &amp;= \\alpha_{p} + \\beta_pR_{mt} + \\epsilon_{p,t} \\end{align} \\] whereas \\(\\alpha_{p} = (x_1\\alpha_1)x_2\\alpha_2)\\), \\(\\beta_p = (x_1\\beta_1 + x_2\\beta_2)\\) and \\(\\epsilon_{p,t} = (x_1\\epsilon_{1t} + x_2\\epsilon_{2t})\\). Consequently, we see that the portfolio returns follow the same linear dependence as the single asset model returns. If we increase this to N assets, we obtain the following expected return: \\[ \\begin{align} E[r_{it}] &amp;= \\sum_{i=1}^nx_iR_{it} \\\\ &amp;= \\sum_{i=1}^n x_i(\\alpha_i + \\beta_iR_{mt} + \\epsilon_{it})\\\\ &amp;= \\frac{1}{n}\\sum_{i=1}^n \\alpha_i + R_{mt}\\frac{1}{n}\\sum_{i=1}^n\\beta_i + \\frac{1}{n}\\sum_{i=1}^n \\epsilon_{it} \\\\ &amp;= \\bar{\\alpha} + \\bar{\\beta}R_{mt} + \\bar{\\epsilon_t} \\end{align} \\] Which implies the return of the portfolio depends on the sensitivity of the portfolio to the market, given by \\(\\frac{1}{n}\\sum_{i=1}^n\\beta_i\\) sensitivity to a non-market component, given by \\(\\frac{1}{n}\\sum_{i=1}^n \\alpha_i\\) error component, given by \\(\\frac{1}{n}\\sum_{i=1}^n \\epsilon_{it}\\) That said, the expected portfolio return depends on the same constituents as the single asset return. We can also draw the properties for the variance of the portfolio returns. The portfolio variance follows the same properties as we have already seen when considering the diversification properties earlier. That is, when increasing the portfolio by imperfectly correlated assets, the systematic component will stay but the idiosyncratic component will be diversified away: \\[ var(\\bar{\\epsilon_t}) = var(\\frac{1}{n}\\sum_{i=1}^n \\epsilon_{it}) = \\frac{1}{n^2}\\sum_{i=1}^n var(\\epsilon_{it}) = \\frac{1}{n}(\\frac{1}{n}\\sum_{i=1}^n var(\\epsilon_{it})) = \\frac{1}{n}\\sigma_{\\epsilon,i}^2 \\] Now, as n increases, the idiosyncratic volatility part will approach zero (LLN property). Consequently, we get that: \\[ \\begin{align} var[R_{pt}] &amp;= \\bar{\\beta}^2\\sigma^2_{m} \\\\ SD[R_{pt}] &amp;= |\\bar{\\beta}|\\sigma_{m} \\end{align} \\] Hence, the portfolio volatility is proportional to market volatility where the factor of proportionality is the absolute value of portfolio beta. This helps us to understand the type of risk that gets diversified away and the type of risk that remains when we form diversified portfolios: Asset specific risk, which is uncorrelated across assets, gets diversified away Market risk, which is common to all assets, does not get diversified away 4.1.1.5 The SIM in Matrix Form Because we have always considered portfolios in matrix notation so far, let’s stick to this principle and try to understand the implications from it. For i = 1,…,n assets, stacking multiple returns provides us with the SIM in matrix notation form: \\[ \\begin{bmatrix} R_{1t} \\\\ R_{2t} \\\\ \\vdots \\\\ R_{nt} \\end{bmatrix} = \\begin{bmatrix} \\alpha_1 \\\\ \\alpha_2 \\\\ \\vdots \\\\ \\alpha_n \\end{bmatrix} + \\begin{bmatrix} \\beta_{1} \\\\ \\beta_{2} \\\\ \\vdots \\\\ \\beta_{n} \\end{bmatrix} R_{mt} + \\begin{bmatrix} \\epsilon_{1t} \\\\ \\epsilon_{2t} \\\\ \\vdots \\\\ \\epsilon_{nt} \\end{bmatrix} \\] Therein, we can re-write the statistical properties defined before in matrix notation as: \\[ \\begin{align} E[\\textbf{R}_t] &amp;= \\alpha + \\beta\\mu_m \\\\ var[\\textbf{R}_t] &amp;= \\Sigma = \\beta var(R_{mt})\\beta&#39; + \\textbf{D} = \\sigma_m^2\\beta\\beta&#39; + \\textbf{D} \\end{align} \\] whereas \\(\\textbf{D}\\): \\[ \\textbf{D} = var(\\epsilon_t) = \\begin{bmatrix} \\sigma_{\\epsilon,1}^2 &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; \\sigma_{\\epsilon,2}^2 &amp; \\dots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; \\sigma_{\\epsilon,n}^2 \\end{bmatrix} \\] Note that, from linear algebra, we know that \\(var(w\\textbf{X}) = w\\textbf{X}w&#39;\\). The \\(\\textbf{D}\\) uses the assumption that the Market Return is uncorrelated with all asset-individual error terms. This implies that there is no covariance between the error terms of the individual asset returns, as they do not depend on any common factor. To comprehend this behaviour and the subsequent portfolio variance better, it is handy to look at the case with n = 3. In this case: \\[ \\begin{align} \\Sigma &amp;= \\begin{bmatrix} var(R_{1t}) &amp; cov(R_{1t}, R_{2t}) &amp; cov(R_{1t}, R_{3t}) \\\\ cov(R_{2t}, R_{1t}) &amp; var(R_{2t}) &amp; cov(R_{2t}, R_{3t}) \\\\ cov(R_{3t}, R_{1t}) &amp; cov(R_{3t}, R_{2t}) &amp; var(R_{3t}) \\end{bmatrix} \\\\ &amp;= \\begin{bmatrix} \\sigma_1^2 &amp; \\sigma_{12} &amp; \\sigma_{13} \\\\ \\sigma_{21} &amp; \\sigma_2^2 &amp; \\sigma_{23} \\\\ \\sigma_{31} &amp; \\sigma_{32} &amp; \\sigma_3^2 \\end{bmatrix} \\\\ &amp;= \\begin{bmatrix} \\beta_1^2\\sigma_m^2 + \\sigma_{\\epsilon, 1}^2 &amp; \\sigma_m\\beta_1\\beta_2 &amp; \\sigma_m^2\\beta_1\\beta_3 \\\\ \\sigma_m\\beta_2\\beta_1 &amp; \\beta_2^2\\sigma_m^2 + \\sigma_{\\epsilon, 2}^2 &amp; \\sigma_m^2\\beta_1\\beta_3 \\\\ \\sigma_m\\beta_3\\beta_1 &amp; \\sigma_m\\beta_3\\beta_2 &amp; \\beta_3^2\\sigma_m^2 + \\sigma_{\\epsilon, 3}^2 \\end{bmatrix} \\\\ &amp;= \\sigma_m^2 \\begin{bmatrix} \\beta_1^2 &amp; \\beta_1\\beta_2 &amp; 2\\beta_1\\beta_3 \\\\ \\beta_2\\beta_1 &amp; \\beta_2^2 &amp; \\beta_1\\beta_3 \\\\ \\beta_3\\beta_1 &amp; \\beta_3\\beta_2 &amp; \\beta_3^2 \\end{bmatrix} + \\begin{bmatrix} \\sigma_{\\epsilon,1}^2 &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma_{\\epsilon,2}^2 &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma_{\\epsilon,n}^2 \\end{bmatrix} \\end{align} \\] The first matrix shows the return variance and covariance contributions due to the market returns, and the second matrix shows the contributions due to the asset specific errors. As such, we again retrieved that the portfolio variance depends on both a systematic and an idiosyncratic part, which, due to its independence properties, will approach zero in case of imperfectly correlated portfolios. 4.1.2 Estimation of the SIM In order to retrieve the SIM coefficients, we will use the least squares estimate procedure as well as the manual computation principle. Although we expect you to be familiar with regression-type analysis, we still made a chapter on it available within the part on Inductive Statistics. You can find the respective information and applications in the sub-chapter theoretical Introduction to Regression Analysis. 4.1.2.1 Ordinary Least Squares (OLS) procedure Now, let’s look at the SIM. For that, we have time-series data of the big four companies in Switzerland from 2013-01-31 to 2017-12-31. Let’s dig into the regression setting. We start first by regressing the individual security returns on the market return. Note that we do not yet incorporate any risk-free return. As we know from the lecture, the traditional textbook recipe is: Take 60 months of total return (including dividends) data of the stock (i) and the market (m) Run following time-series regression: \\(R_{it} - r_f = \\alpha_i + \\beta_i(R_{mt} - r_f) + \\epsilon_{it}\\) Apply a ”Bloomberg” adjustment – shrink the beta towards 1: \\(\\beta_{BA} = (1-AF)\\hat{\\beta} + AF\\) # Since we work with monthly observations, we need to change the risk-free rate accordingly. rf_ts &lt;- xts(rf[,-1], order.by = as.Date(rf[,1])) rf_ts_yearly &lt;- rf_ts$SWISS.CONFEDERATION.BOND.1.YEAR...RED..YIELD / 100 rf_ts_monthly &lt;- ((1 + rf_ts_yearly)^(1/12) - 1) In R, the usual command for conducting regression is the following: # First, define a different name for the rf colnames(rf_ts_monthly) &lt;- &quot;rf_1_year&quot; # Let&#39;s run the simple OLS reg_Nestle &lt;- lm(stocks_SMI_ts_ret$Nestle_PS[&#39;2013-01-31/2017-12-31&#39;] - rf_ts_monthly$rf_1_year[&#39;2013-01-31/2017-12-31&#39;] ~ stocks_SMI_ts_ret$SMI[&#39;2013-01-31/2017-12-31&#39;] - rf_ts_monthly$rf_1_year[&#39;2013-01-31/2017-12-31&#39;]) reg_Novartis &lt;- lm(stocks_SMI_ts_ret$Novartis_N[&#39;2013-01-31/2017-12-31&#39;] - rf_ts_monthly$rf_1_year[&#39;2013-01-31/2017-12-31&#39;] ~ stocks_SMI_ts_ret$SMI[&#39;2013-01-31/2017-12-31&#39;] - rf_ts_monthly$rf_1_year[&#39;2013-01-31/2017-12-31&#39;]) reg_Roche &lt;- lm(stocks_SMI_ts_ret$Roche_Holding[&#39;2013-01-31/2017-12-31&#39;] - rf_ts_monthly$rf_1_year[&#39;2013-01-31/2017-12-31&#39;] ~ stocks_SMI_ts_ret$SMI[&#39;2013-01-31/2017-12-31&#39;] - rf_ts_monthly$rf_1_year[&#39;2013-01-31/2017-12-31&#39;]) reg_UBS &lt;- lm(stocks_SMI_ts_ret$UBS_N[&#39;2013-01-31/2017-12-31&#39;] - rf_ts_monthly$rf_1_year[&#39;2013-01-31/2017-12-31&#39;] ~ stocks_SMI_ts_ret$SMI[&#39;2013-01-31/2017-12-31&#39;] - rf_ts_monthly$rf_1_year[&#39;2013-01-31/2017-12-31&#39;]) The ~sign is also called tilde. It is used to define the regression property. In essence it states for regress the dependent variable on the explanatory variable(s). Note we also have to define the data argument, which is the indication from which dataset we take the variables. It is good to understand what this command actually provides us with. To do so, we can add a $ sign after the created variable: coefficients: Displays the coefficients of the intercept and the explanatory variables residuals: Displays the residual values of the regression \\((\\hat{y}_i - y_i)\\) fitted.values: Displays the predicted y values df.residual: Displays the DOF of the residuals model: Displays the input parameters of the model We will need especially the residuals and the fitted values command to comprehend the computing behavior of the model. For us most important is the summary() argument. This is used to see the regression output. summary(reg_Nestle) ## ## Call: ## lm(formula = stocks_SMI_ts_ret$Nestle_PS[&quot;2013-01-31/2017-12-31&quot;] - ## rf_ts_monthly$rf_1_year[&quot;2013-01-31/2017-12-31&quot;] ~ stocks_SMI_ts_ret$SMI[&quot;2013-01-31/2017-12-31&quot;] - ## rf_ts_monthly$rf_1_year[&quot;2013-01-31/2017-12-31&quot;]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.052956 -0.016822 -0.000448 0.016847 0.058989 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.002192 0.003247 0.675 0.502 ## stocks_SMI_ts_ret$SMI[&quot;2013-01-31/2017-12-31&quot;] 0.762084 0.102810 7.413 5.94e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.02472 on 58 degrees of freedom ## Multiple R-squared: 0.4865, Adjusted R-squared: 0.4776 ## F-statistic: 54.95 on 1 and 58 DF, p-value: 5.938e-10 It computes the following terms: Distribution characters of Residuals The Coefficient, Standard Errors, T-Values and P-Values of the Estimates Residual Standard Errors and the DOF of the Residuals R2 and Adjusted R2 F Statistic of the model As such, we obtain everything we need from this output. In order to get it, we can use different specifications: # Define the summary for nestle sum_1 &lt;- summary(reg_Nestle) If we use the $ sign, we can retrieve some important options from the summary command: coefficients: Returns a k \\(\\times\\) n matrix with the coefficients, standard errors, t-values and p-values cov.unscaled: Returns the k \\(\\times\\) k variance-covariance matrix of the regressors df: Returns the DOF r.squared: Returns the R2 value adj.r.squared: Returns the adjusted R2 value fstatistic: Returns the F-Statistic of the model # The coefficient matrix looks like this: sum_1$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.002192443 0.003246622 0.6752999 5.021687e-01 ## stocks_SMI_ts_ret$SMI[&quot;2013-01-31/2017-12-31&quot;] 0.762084340 0.102810047 7.4125473 5.938184e-10 # We can easily retrieve the coefficients, standard erros, t-values and p-values of the regression output. To do so, we just need to slice the matrix ## Get the coefficient for beta of the regression: coef_SMI &lt;- sum_1$coefficients[2,1] # Get the standard errors se_SMI &lt;- sum_1$coefficients[2,2] # Get the t value t_SMI &lt;- sum_1$coefficients[2,3] # Get the p-value p_SMI &lt;- sum_1$coefficients[2,4] Accordingly, we can try to replicate all the results from the lecture: # First, define the individual dependent variables y_Ne &lt;- stocks_SMI_ts_ret$Nestle_PS[&#39;2013-01-31/2017-12-31&#39;] - rf_ts_monthly$rf_1_year[&#39;2013-01-31/2017-12-31&#39;] y_No &lt;- stocks_SMI_ts_ret$Novartis_N[&#39;2013-01-31/2017-12-31&#39;] - rf_ts_monthly$rf_1_year[&#39;2013-01-31/2017-12-31&#39;] y_Ro &lt;- stocks_SMI_ts_ret$Roche_Holding[&#39;2013-01-31/2017-12-31&#39;] - rf_ts_monthly$rf_1_year[&#39;2013-01-31/2017-12-31&#39;] y_U &lt;- stocks_SMI_ts_ret$UBS_N[&#39;2013-01-31/2017-12-31&#39;] - rf_ts_monthly$rf_1_year[&#39;2013-01-31/2017-12-31&#39;] stocks_SMI_ts_ret_new &lt;- merge.xts(y_Ne, y_No, y_Ro, y_U) # Then, define the explanatory variable and add a constant term. We need to add a constant in order to add the alpha term x &lt;- stocks_SMI_ts_ret$SMI[&#39;2013-01-31/2017-12-31&#39;] - rf_ts_monthly$rf_1_year[&#39;2013-01-31/2017-12-31&#39;] # And the names of the stocks Stonks &lt;- c(&quot;Nestle_PS&quot;, &quot;Novartis_N&quot;, &quot;Roche_Holding&quot;, &quot;UBS_N&quot;) # This is the same loop structure as was already used during the first assignment for (i in Stonks){ # We first create a column name variable col_name &lt;- paste0(i) # Then, we fit the stock excess return on the market excess return fit &lt;- summary(lm(stocks_SMI_ts_ret_new[&#39;2013-01-31/2017-12-31&#39;, i] ~ x$SMI[&#39;2013-01-31/2017-12-31&#39;])) # We calculate the respective values based on the fit variable created for each company of Stonks beta &lt;- fit$coefficients[2,1] alpha &lt;- fit$coefficients[1,1] R2 &lt;- fit$r.squared std_res &lt;- sd(fit$residuals, na.rm = T) beta_std_error &lt;- fit$coefficients[2,2] alpha_std_error &lt;- fit$coefficients[1,2] adjusted_BA &lt;- fit$coefficients[2,1]*2/3 + 1/3 t_alpha &lt;- fit$coefficients[1,1] / fit$coefficients[1,2] t_beta &lt;- fit$coefficients[2,1] / fit$coefficients[2,2] # Use these lines of code to create temporary data frames which are being merged with each other in a subsequent step if (i == &quot;Nestle_PS&quot;){ df_final_beta &lt;- beta df_final_alpha &lt;- alpha df_final_R2 &lt;- R2 df_final_std_res &lt;- std_res df_final_beta_std_error &lt;- beta_std_error df_final_alpha_std_error &lt;- alpha_std_error df_final_adjusted_BA &lt;- adjusted_BA df_final_t_alpha &lt;- t_alpha df_final_t_beta &lt;- t_beta col_name_final &lt;- col_name } # This subsequent step is undertaken here else { df_final_beta &lt;- cbind(df_final_beta, beta) df_final_alpha &lt;- cbind(df_final_alpha, alpha) df_final_R2 &lt;- cbind(df_final_R2, R2) df_final_std_res &lt;- cbind(df_final_std_res, std_res) df_final_beta_std_error &lt;- cbind(df_final_beta_std_error, beta_std_error) df_final_alpha_std_error &lt;- cbind(df_final_alpha_std_error, alpha_std_error) df_final_adjusted_BA &lt;- cbind(df_final_adjusted_BA, adjusted_BA) df_final_t_alpha &lt;- cbind(df_final_t_alpha, t_alpha) df_final_t_beta &lt;- cbind(df_final_t_beta, t_beta) col_name_final &lt;- cbind(col_name_final, col_name) } } # Bind all values together for each company of Stonks and obtain a data frame with all required variables data_frame_ratios &lt;- as.data.frame(rbind(df_final_beta, df_final_alpha, df_final_R2, df_final_std_res, df_final_beta_std_error, df_final_alpha_std_error, df_final_adjusted_BA, df_final_t_alpha, df_final_t_beta)) # Define the according names for the dataframe (columns and rows) names(data_frame_ratios) = col_name_final rownames(data_frame_ratios) &lt;- c(&quot;Beta&quot;, &quot;Alpha&quot;, &quot;R2&quot;, &quot;Res.Std.Dev&quot;, &quot;Std. Error Beta&quot;, &quot;Std. Error Alpha&quot;, &quot;Adj. Beta&quot;, &quot;T Alpha&quot;, &quot;T Beta&quot;) t(data_frame_ratios) ## Beta Alpha R2 Res.Std.Dev Std. Error Beta Std. Error Alpha Adj. Beta T Alpha T Beta ## Nestle_PS 0.7644739 1.913296e-03 0.4874485 0.02448622 0.1029326 0.003250561 0.8429826 0.58860488 7.426935 ## Novartis_N 1.0579713 7.321110e-04 0.6228580 0.02571500 0.1080980 0.003413682 1.0386475 0.21446373 9.787146 ## Roche_Holding 0.9805971 8.590951e-05 0.5230519 0.02924882 0.1229532 0.003882800 0.9870647 0.02212566 7.975370 ## UBS_N 1.2929343 -1.273317e-03 0.3222448 0.05856991 0.2462101 0.007775193 1.1952895 -0.16376668 5.251345 4.1.2.1.1 Manually computing the estimates When running the regression, we do nothing more than applying the function of the covariance / variance we defined above. Consequently, we should be able to easily replicate the regression results. Let’s do this # Here, define the explanatory variable and add a constant term. We need to add a constant in order to add the alpha term X &lt;- cbind(1, x) # Let&#39;s calculate the beta coefficients manually ## Either with the formula for cov-var reg_Nestle_coef_manual &lt;- cov(y_Ne, x) / var(x) reg_Novartis_coef_manual &lt;- cov(y_No, x) / var(x) reg_Roche_coef_manual &lt;- cov(y_Ro, x) / var(x) reg_UBS_coef_manual &lt;- cov(y_U, x) / var(x) ## Or with the formula we retrieved from the chapter of linear algebra betaHat_Ne = solve(t(X) %*%X)%*%t(X)%*%y_Ne betaHat_No = solve(t(X) %*%X)%*%t(X)%*%y_No betaHat_Ro = solve(t(X) %*%X)%*%t(X)%*%y_Ro betaHat_U = solve(t(X) %*%X)%*%t(X)%*%y_U # Now, calculate the beta standard errors manually ## First, we calculate the variance-covariance matrix (vcov) whereas: ### var = sum((y_Ne - X%*%betaHat_Ne)^2) ### 1/n = 1/(nrow(X)-ncol(X)) ### (X&#39;X)^-1 = chol2inv(chol(t(X)%*%X)) vcov_Nestle &lt;- sum((y_Ne - X%*%betaHat_Ne)^2)/(nrow(X)-ncol(X))*chol2inv(chol(t(X)%*%X)) ## Then, we take the square root of the second element from the diagonal matrix (the diagonal elements are the variances in a vcov matrix), because the Standard Deviation of the estimates divided by the square root of n (we already incorporated n in the formula above) ### SE(beta) = SD(beta) /sqrt(N) = sqrt(var(N)) / sqrt(N) se_Nestle &lt;- sqrt(diag(vcov_Nestle))[2] ## Let&#39;s do this for all of the estimates vcov_Novartis &lt;- sum((y_No - X%*%betaHat_No)^2)/(nrow(X)-ncol(X))*chol2inv(chol(t(X)%*%X)) se_Novartis &lt;- sqrt(diag(vcov_Novartis))[2] vcov_Roche &lt;- sum((y_Ro - X%*%betaHat_Ro)^2)/(nrow(X)-ncol(X))*chol2inv(chol(t(X)%*%X)) se_Roche &lt;- sqrt(diag(vcov_Roche))[2] vcov_UBS &lt;- sum((y_U - X%*%betaHat_U)^2)/(nrow(X)-ncol(X))*chol2inv(chol(t(X)%*%X)) se_UBS &lt;- sqrt(diag(vcov_UBS))[2] # Now, we can create some dataframes coefs_compare &lt;- cbind(data_frame_ratios[1,1], reg_Nestle_coef_manual, data_frame_ratios[1,2], reg_Novartis_coef_manual, data_frame_ratios[1,3], reg_Roche_coef_manual, data_frame_ratios[1,4], reg_UBS_coef_manual) se_compare &lt;- cbind(data_frame_ratios[5,1], se_Nestle, data_frame_ratios[5,2], se_Novartis, data_frame_ratios[5,3], se_Roche, data_frame_ratios[5,4], se_UBS) coefs_se &lt;- rbind(coefs_compare, se_compare) colnames(coefs_se) &lt;- c(&quot;Nestle&quot;, &quot;Nestle Manual&quot;, &quot;Novartis&quot;, &quot;Novartis Manual&quot;, &quot;Roche&quot;, &quot;Roche Manual&quot;, &quot;UBS&quot;, &quot;UBS Manual&quot;) rownames(coefs_se) &lt;- c(&quot;Estimates&quot;, &quot;Standard Errors&quot;) # Display the output round(coefs_se, 4) ## Nestle Nestle Manual Novartis Novartis Manual Roche Roche Manual UBS UBS Manual ## Estimates 0.7645 0.7645 1.0580 1.0580 0.9806 0.9806 1.2929 1.2929 ## Standard Errors 0.1029 0.1029 0.1081 0.1081 0.1230 0.1230 0.2462 0.2462 4.1.2.2 Predicting Betas Instead of applying an arbitrary adjustment factor AF we could estimate a stock specific adjustment factor by running the following regression: \\[ \\beta_{i,t} = a_i + b\\hat{\\beta}_{i,t-1} + e_{i,t} \\] in this case, the estimated factors \\(\\hat{a}\\) and \\(\\hat{b}\\) are used to forecast betas. The forecasted beta is then: \\[ \\beta_{i,t+1} = \\hat{a} + \\hat{b}\\hat{\\beta}_{i,t+1} \\] This is also known as forecasting \\(\\beta\\) We do this to understand whether, and to what extent, current betas are associated with movements in future \\(\\beta\\). That is, we attempt to understand the co-existence of our estimates throughout time, which is substantially linked to the discussions on independence we had in the previous chapter. Consequently, we will run two distinct regressions. The exact way to do this is the following: # Create a new dataframe for the enitre time series y_Ne_tot &lt;- stocks_SMI_ts_ret$Nestle_PS- rf_ts_monthly$rf_1_year y_No_tot &lt;- stocks_SMI_ts_ret$Novartis_N - rf_ts_monthly$rf_1_year y_Ro_tot &lt;- stocks_SMI_ts_ret$Roche_Holding - rf_ts_monthly$rf_1_year y_U_tot &lt;- stocks_SMI_ts_ret$UBS_N - rf_ts_monthly$rf_1_year x_tot &lt;- stocks_SMI_ts_ret$SMI - rf_ts_monthly$rf_1_year stocks_SMI_ts_ret_new_tot &lt;- merge.xts(y_Ne_tot, y_No_tot, y_Ro_tot, y_U_tot) # Merge the SMI into the existing time-series object stocks_SMI_ts_ret_new_merged &lt;- merge.xts(stocks_SMI_ts_ret_new_tot, x_tot) for (i in Stonks){ # Column names values col_name &lt;- paste0(i) # run the rolling regression fit_roll &lt;- roll_regres(stocks_SMI_ts_ret_new_merged[,i] ~ stocks_SMI_ts_ret_new_merged$SMI, width = 60) # Extract the coefficients ## Note that you only use the observations from 60 onwards, because for the first 59 you get NA values (as your width is 60) fit_roll_coefs &lt;- fit_roll$coefs[60:dim(stocks_SMI_ts_ret_new_merged)[1],] # Calculate the respective alpha and beta hat values predicted_alpha &lt;- summary(lm(fit_roll_coefs[,2] ~ lag(fit_roll_coefs[,2])))$coefficients[1,1] predicted_beta &lt;- summary(lm(fit_roll_coefs[,2] ~ lag(fit_roll_coefs[,2])))$coefficients[2,1] # Calculate RMSE predicted_rmse &lt;- sqrt(mean(summary(lm(fit_roll_coefs[,2] ~ lag(fit_roll_coefs[,2])))$residuals^2)) # Structure for creating the data frames if (i == &quot;Nestle_PS&quot;){ predicted_alpha_final &lt;- predicted_alpha predicted_beta_final &lt;- predicted_beta predicted_rmse_final &lt;- predicted_rmse } else { predicted_alpha_final &lt;- cbind(predicted_alpha_final, predicted_alpha) predicted_beta_final &lt;- cbind(predicted_beta_final, predicted_beta) predicted_rmse_final &lt;- cbind(predicted_rmse_final, predicted_rmse) } } # Merge all dataframes ## rowbind command pred_alpha_beta &lt;- rbind(predicted_alpha_final, predicted_beta_final) ## Add the Beta_t variable (The ultimate Beta estimate from the previous exercise) all_pred_alpha_beta &lt;- as.data.frame(rbind(pred_alpha_beta, df_final_beta)) ## Define names names(all_pred_alpha_beta) &lt;- c(&quot;Nestle&quot;, &quot;Novartis&quot;, &quot;Roche&quot;, &quot;UBS&quot;) rownames(all_pred_alpha_beta) &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;beta_t&quot;) ## Transpose the data frame all_pred_alpha_beta &lt;- t(all_pred_alpha_beta) ## Calculate the Beta forecast value beta_forecast &lt;- all_pred_alpha_beta[,1] + all_pred_alpha_beta[,2] * all_pred_alpha_beta[,3] ## Bind the Beta forecast variable with all other variables predicting_beta_df &lt;- cbind(all_pred_alpha_beta, beta_forecast) predicting_beta_df ## a b beta_t beta_forecast ## Nestle 0.011128874 0.9835823 0.7644739 0.7630519 ## Novartis 0.001777507 1.0023047 1.0579713 1.0621871 ## Roche 0.038515121 0.9558142 0.9805971 0.9757837 ## UBS 0.021480237 0.9854612 1.2929343 1.2956168 We can use this procedure for many applications. For instance, we can take the current Stock Excess Returns and regress them on the \\(\\beta\\) values we just retrieved (instead of the \\(\\beta\\) values): \\[ R_{i,t} = c_i + d\\hat{\\beta}_{i,t-1} + e_{i,t} \\] Ultimately, we can also calculate prediction metrics like the Root-Mean-Squared-Error (RMSE): # Create the dataframe with Root-Mean-Squared-Errors all_RMSE &lt;- as.data.frame(predicted_rmse_final) colnames(all_RMSE) &lt;- c(&quot;Nestle&quot;, &quot;Novartis&quot;, &quot;Roche&quot;, &quot;UBS&quot;) rownames(all_RMSE) &lt;- &quot;RMSE&quot; all_RMSE ## Nestle Novartis Roche UBS ## RMSE 0.02164547 0.02589957 0.03309323 0.05612155 In essence, the predicting \\(\\beta\\) approach is nothing else than a Two-Step Regression procedure. This implies that we run two separate regressions: In the first, we estimate the \\(\\beta\\) coefficients in the usual way In the second, we take the \\(\\beta\\) coefficients as explanatory variables and estimate their factors. The coefficients, or “betas”, we estimate in the second regression are also known as Factor Loadings. They represent the extent to which the movements of the factor of interest vary with the dependent variable. We use this procedure to get the average variation of the dependent variable that is being represented by our \\(\\beta\\) coefficients. Usually, we have some economic intuition or hypothesis about the absolute magnitude of these loadings. For instance, in CAPM settings, we assume a direct proportion of the factor loadings on the respective excess returns (with a slope of the Capital Market Line of 1). We will cover factor loadings in more detail in the next chapter. 4.1.2.3 Beta as a risk measure We have seen in the last chapter that there appears to be a connection between the return of an underlying security and its associated risk. We also showed that the basic formulation of risk through variance or standard deviation terms does not adequately represent the overall risk associated in portfolio settings. This is because the risk we consider is defined as variation which depends on idiosyncratic as well as systematic components. While properties such as the standard deviation or the variance do not incorporate the fact that idiosyncratic risk can, and should, be diversified away, they are likely to incorrectly quantify the extent to which associated risk should be compensated through the underlying return structure. Further, we learned that \\(\\beta\\) is able to counteract this adversarial property to some extent. As such, we could argue that \\(\\beta\\), which relies on co-movements with a general, underlying index factor, should be better equipped in quantifying the associated risk as it incorporates diversification potential. To understand whether this is indeed the case, one can create portfolios based on \\(\\beta\\) values and observe whether portfolios associated with higher average \\(\\beta\\) values also have higher cumulative returns. We can do this with the following code: # Read in the data A4 &lt;- read.csv(&quot;~/Desktop/Master UZH/Data/A2_dataset_04_Ex_Session.txt&quot;, header = T, sep = &quot;\\t&quot;) A5 &lt;- read.csv(&quot;~/Desktop/Master UZH/Data/A2_dataset_05_Ex_Session.txt&quot;, header = T, sep = &quot;\\t&quot;) # Create an xts Time-Series object A4_ts &lt;- xts(A4[,-1], order.by = as.Date(A4[,1])) A5_ts &lt;- xts(A5[,-1], order.by = as.Date(A5[,1])) # Create a Return object A4_ts_ret &lt;- Return.calculate(A4_ts, method = &quot;discrete&quot;)[-1,] ## Here: we need to adjust the time frames always to t-1 as we did for the standard deviation Beta_daily &lt;- as.matrix(A5_ts[&#39;1994-04-29/2017-12-28&#39;]) Ret_daily &lt;- as.matrix(A4_ts_ret[&#39;1994-05-02/2017-12-29&#39;]) # Create Portfolios based on Beta for each period for (i in 1:dim(Beta_daily)[1]){ # Get only the respective row for each iteration B_daily &lt;- Beta_daily[i,] # Calculate the respective quantile values ## For each time period i, we create lists with securities and their respective beta values for each quintile. That is, each Q will consist of all ## securities that are within a given quintile. We do this by assigning a subset command, in which we subset at each time period the dataframe ## such that only securities are put into a given list which are in this quintile. For instance, Q1 comprises of all securities which have a lower ## beta value than the 20&#39;th percentile Q1 &lt;- subset(B_daily, B_daily &lt; quantile(B_daily, c(0.2), na.rm = T)) Q2 &lt;- subset(B_daily, B_daily &gt;= quantile(B_daily, c(0.2), na.rm = T) &amp; B_daily &lt; quantile(B_daily, c(0.4), na.rm = T)) Q3 &lt;- subset(B_daily, B_daily &gt;= quantile(B_daily, c(0.4), na.rm = T) &amp; B_daily &lt; quantile(B_daily, c(0.6), na.rm = T)) Q4 &lt;- subset(B_daily, B_daily &gt;= quantile(B_daily, c(0.6), na.rm = T) &amp; B_daily &lt; quantile(B_daily, c(0.8), na.rm = T)) Q5 &lt;- subset(B_daily, B_daily &gt;= quantile(B_daily, c(0.8), na.rm = T)) # Calculate the respective daily mean returns ## Here, we take the mean value from the daily returns. The command [i, names(Q1)] selects in each iteration i only the companies whose names are ## in the first quintile portfolio (Then for the second analogously). Returns_Q1 &lt;- mean(Ret_daily[i,names(Q1)],na.rm=TRUE) Returns_Q2 &lt;- mean(Ret_daily[i,names(Q2)],na.rm=TRUE) Returns_Q3 &lt;- mean(Ret_daily[i,names(Q3)],na.rm=TRUE) Returns_Q4 &lt;- mean(Ret_daily[i,names(Q4)],na.rm=TRUE) Returns_Q5 &lt;- mean(Ret_daily[i,names(Q5)],na.rm=TRUE) ## As such, we &quot;map&quot; the names of the companies that belong to each defined quintile onto the return dataset for each period (e.g. if at time ## period i Credit Suisse, Zurich, Swiss Re and ABB have Beta values within the highest quintile, then Ret_daily[i,names(Q5)] will only select ## these four companies. Consequently, we only perform the mean return calculation on these four companies, which then amounts to the mean return ## of the highest quintile PF at time i. ) # Create dataframes if (i == 1){ Returns_Q1_final &lt;- Returns_Q1 Returns_Q2_final &lt;- Returns_Q2 Returns_Q3_final &lt;- Returns_Q3 Returns_Q4_final &lt;- Returns_Q4 Returns_Q5_final &lt;- Returns_Q5 } else{ Returns_Q1_final &lt;- rbind(Returns_Q1_final, Returns_Q1) Returns_Q2_final &lt;- rbind(Returns_Q2_final, Returns_Q2) Returns_Q3_final &lt;- rbind(Returns_Q3_final, Returns_Q3) Returns_Q4_final &lt;- rbind(Returns_Q4_final, Returns_Q4) Returns_Q5_final &lt;- rbind(Returns_Q5_final, Returns_Q5) } } We can now create a dataframe with the mean annualised return and standard deviation (the code is intentionally not shown): ## PF Q1 PF Q2 PF Q3 PF Q4 PF Q5 ## Mean Returns 0.070158 0.090542 0.10774 0.155226 0.101657 ## Std of Returns 9.865200 22.366272 35.99285 54.563412 130.832437 We can also plot the cumulative return of each quintile portfolio: 4.1.2.4 Working with robust standard errors We have introduced the assumptions we make when working in OLS settings and stated the estimator properties if these assumptions hold. Further, we have introduced some tests that can detect whether some assumptions do hold. For instance, when looking at the variance properties, we have claimed that there is no heteroskedasticity and no serial correlation present. However, this may be violated, as we have seen. In case of violation of assumtpions 3 and 4, the model still is unbiased, as is given by the derivation of the estimator’s expected value. However, the model won’t be consistent any longer because the t-statistics computed do not follow a standard normal distribution, even in large samples. This is because, if these assumptions do not hold, then the residual term matrix is no longer \\(I_N\\sigma^2\\). Consequently, the variance of the estimator, which we derived as \\(\\sigma^2(X&#39;X)^{-1}\\) is incorrect if the assumptions do not hold. Based on this, also the standard errors will be incorrect, thereby deriving an inconsistent t-statistic. In order to address these concerns, we introduce the concept of robust standard errors. These are standard errors that incorporate the fact that either of these assumptions are violated and thus adjust the covariance matrix of the residuals accordingly. We will introduce R packages that create consistent standard errors even if these assumptions are violated. Note that a thorough discussion of the mathematical properties of the standard error adjustments would be too much for this course, but we refer you to The fundamental discussion on Heteroskedasticity and Autocorrelated Standard Error applications in R as well as the respective chapters of econometrics-with-R. Heteroskedasticity-Robust Standard Errors In order to compute solely Heteroskedasticity-Robust Standard Errors, we can use consistent standard error estimates referred to as Eicker-Huber-White standard errors, the most frequently cited paper on this is White (1980). In order to implement these standard errors, we make use of the function vcovHC() from the package sandwich. This package can compute four distinct types of Heteroskedasticity-Consistent (HC) Standard Errors, namely HC0, HC1, HC2, HC3 (HC1 is the default use in most statistics programs). We can quite easily use these standard errors with the following code: # Get the reg_Nestle output and use the following command vcovHC_Ne &lt;- vcovHC(reg_Nestle, type = &quot;HC1&quot;) vcovHC_Ne ## (Intercept) stocks_SMI_ts_ret$SMI[&quot;2013-01-31/2017-12-31&quot;] ## (Intercept) 1.161379e-05 -0.0001571461 ## stocks_SMI_ts_ret$SMI[&quot;2013-01-31/2017-12-31&quot;] -1.571461e-04 0.0117222085 This will deliver us the HC consistent variance-covariance matrix of the first regression. Note that is of the same format as when we calculated the standard errors manually and saved it in vcov_Nestle. Now, as before, we are only interested in the square root of the diagonal values because these will give us the standard errors, indicated as standard deviation divided by the square root of n. se_Nestle_robust &lt;- sqrt(diag(vcovHC_Ne))[2] se_Nestle_robust ## stocks_SMI_ts_ret$SMI[&quot;2013-01-31/2017-12-31&quot;] ## 0.1082691 In order to obtain the same summary output as for the non-robust estimates, we can use the coeftest() function. coeftest(reg_Nestle, vcov = vcovHC_Ne) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.0021924 0.0034079 0.6433 0.5225 ## stocks_SMI_ts_ret$SMI[&quot;2013-01-31/2017-12-31&quot;] 0.7620843 0.1082691 7.0388 2.523e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Let’s quickly compare the standard error estimates for all the big four firms: # Do the HC robust standard error formulas and obtain the standard errors se_Nestle_robust &lt;- sqrt(diag(vcovHC(reg_Nestle, type = &quot;HC1&quot;)))[2] se_Novartis_robust &lt;- sqrt(diag(vcovHC(reg_Novartis, type = &quot;HC1&quot;)))[2] se_Roche_robust &lt;- sqrt(diag(vcovHC(reg_Roche, type = &quot;HC1&quot;)))[2] se_UBS_robust &lt;- sqrt(diag(vcovHC(reg_UBS, type = &quot;HC1&quot;)))[2] # Now, we can create some dataframes coefs_compare &lt;- cbind(data_frame_ratios[1,1], reg_Nestle_coef_manual, data_frame_ratios[1,2], reg_Novartis_coef_manual, data_frame_ratios[1,3], reg_Roche_coef_manual, data_frame_ratios[1,4], reg_UBS_coef_manual) se_compare &lt;- cbind(data_frame_ratios[5,1], se_Nestle_robust, data_frame_ratios[5,2], se_Novartis_robust, data_frame_ratios[5,3], se_Roche_robust, data_frame_ratios[5,4], se_UBS_robust) coefs_se &lt;- rbind(coefs_compare, se_compare) colnames(coefs_se) &lt;- c(&quot;Nestle&quot;, &quot;Nestle HC Robust&quot;, &quot;Novartis&quot;, &quot;Novartis HC Robust&quot;, &quot;Roche&quot;, &quot;Roche HC Robust&quot;, &quot;UBS&quot;, &quot;UBS HC Robust&quot;) rownames(coefs_se) &lt;- c(&quot;Estimates&quot;, &quot;Standard Errors&quot;) # Display the output round(coefs_se, 5) ## Nestle Nestle HC Robust Novartis Novartis HC Robust Roche Roche HC Robust UBS UBS HC Robust ## Estimates 0.76447 0.76447 1.05797 1.05797 0.98060 0.98060 1.29293 1.29293 ## Standard Errors 0.10293 0.10827 0.10810 0.10709 0.12295 0.08626 0.24621 0.19482 As we can see, the standard errors increase to a certain extent. Heteroskedasticity and Autocorrelation robust Standard Errors Within time-series regressions, we often work with lagged data as was covered previously. The error term in the distributed lag model may be serially correlated due to serially correlated determinants of \\(Y_t\\) that are not included as regressors. When these factors are correlated with the regressors of the model, serially correlated errors violate the assumption of exogeneity. Autocorrelated standard errors render the usual homoskedasticity-only and heteroskedasticity-robust standard errors invalid and may cause misleading inference. HAC errors are a remedy. There are R functions like vcovHAC() or NeweyWest() from the package sandwich which are convenient for computation of such estimators. For further theoretical insight into how the transformations work, refer to the respective documentation here. The principle behind the HAC standard errors is pretty much the same as with the HC standard errors. In essence, we usually employ Newey-West standard errors and add a correction factor that adjusts for serially correlated errors and involves estimates of m-1 autocorrelation coefficients. Usually this correction estimate has the following formula: \\[ m = 0.75\\cdot T^{1/3} \\] # Calculate the adjustment factor m = floor(0.75*length(stocks_SMI_df_ret)^{1/3}) # Calculate the NeweyWest vcov matrix nw_vcov_Ne &lt;- NeweyWest(lm(y_Ne ~ X), lag = m - 1, prewhite = T, adjust = T) # Calculate the corrected Standard Error nw_se_Nestle = sqrt(diag(nw_vcov_Ne))[2] Let’s again do this for all big four companies and compare the standard errors # Calculate NW SE nw_se_Novartis = sqrt(diag(NeweyWest(lm(y_No ~ X), lag = m - 1, prewhite = T, adjust = T)))[2] nw_se_Roche = sqrt(diag(NeweyWest(lm(y_Ro ~ X), lag = m - 1, prewhite = T, adjust = T)))[2] nw_se_UBS = sqrt(diag(NeweyWest(lm(y_U ~ X), lag = m - 1, prewhite = T, adjust = T)))[2] # Create the comparing frame coefs_compare &lt;- cbind(data_frame_ratios[1,1], reg_Nestle_coef_manual, reg_Nestle_coef_manual, data_frame_ratios[1,2], reg_Novartis_coef_manual, reg_Nestle_coef_manual, data_frame_ratios[1,3], reg_Roche_coef_manual, reg_Roche_coef_manual, data_frame_ratios[1,4], reg_UBS_coef_manual, reg_UBS_coef_manual) se_compare &lt;- cbind(data_frame_ratios[5,1], se_Nestle_robust, nw_se_Nestle, data_frame_ratios[5,2], se_Novartis_robust, nw_se_Novartis, data_frame_ratios[5,3], se_Roche_robust, nw_se_Roche, data_frame_ratios[5,4], se_UBS_robust, nw_se_UBS) coefs_se &lt;- rbind(coefs_compare, se_compare) colnames(coefs_se) &lt;- c(&quot;Nestle&quot;, &quot;Nestle HC Robust&quot;, &quot;Nestle NW-HAC Robust&quot;, &quot;Novartis&quot;, &quot;Novartis HC Robust&quot;, &quot;Novartis NW-HAC Robust&quot;, &quot;Roche&quot;, &quot;Roche HC Robust&quot;, &quot;Roche NW-HAC Robust&quot;, &quot;UBS&quot;, &quot;UBS HC Robust&quot;, &quot;UBS NW-HAC Robust&quot;) rownames(coefs_se) &lt;- c(&quot;Estimates&quot;, &quot;Standard Errors&quot;) # Display the output round(data.frame(coefs_se), 5) ## Nestle Nestle.HC.Robust Nestle.NW.HAC.Robust Novartis Novartis.HC.Robust Novartis.NW.HAC.Robust Roche Roche.HC.Robust ## Estimates 0.76447 0.76447 0.76447 1.05797 1.05797 0.76447 0.98060 0.98060 ## Standard Errors 0.10293 0.10827 0.10372 0.10810 0.10709 0.10930 0.12295 0.08626 ## Roche.NW.HAC.Robust UBS UBS.HC.Robust UBS.NW.HAC.Robust ## Estimates 0.98060 1.29293 1.29293 1.29293 ## Standard Errors 0.08549 0.24621 0.19482 0.17797 As we can see, further correcting for serial correlation again increases the standard errors to some extent. 4.1.2.5 Improving table design with stargazer Now we have the basics to perform simple regression analysis. While we can learn more about running regressions only once we introduce more elaborate econometric tools (such as IV, RDD, Panel Data Methods etc.) we also want to be able to display the outputs we generated in a nice and easily readable fashion. To display charts and regression outputs in such a way, we make use of Marek Hlavac’s (2021) stargazer package. This package facilitates the use of nice looking outputs as we know them from academic journals. To understand how we can use the package, we will quickly introduce its main components: # Let&#39;s jump right into the application of stargazer and explain the individual commands list_regs &lt;- list(reg_Nestle, reg_Novartis, reg_Roche, reg_UBS) stargazer(reg_Nestle, reg_Novartis, reg_Roche, reg_UBS, type = &quot;text&quot;, report = &quot;vcs*&quot;, se=lapply(list_regs, function(x) sqrt(diag(vcovHC(x, type = &quot;HC1&quot;)))), # This is a very handy feature! # Note that the lapply applies the calculation of the robust standard errors to each of the regression outputs we defined title=&quot;How standardized fertility measure is affected by socio-economic indicators?&quot;, column.labels = c(&quot;Iter1&quot;, &quot;Iter-2&quot;, &quot;Iter-3&quot;, &quot;Iter-4&quot;, &quot;Iter-5&quot;), covariate.labels = c(&quot;Intercept&quot;, &quot;SMI&quot;), no.space = TRUE, header=F, single.row = FALSE, font.size = &quot;small&quot;, intercept.bottom = F, column.separate = c(1, 1, 1, 1, 1), digits = 2, float = TRUE, t.auto = F, p.auto = F, notes.align = &quot;l&quot;, notes = c(&quot;datasets::swiss&quot;, &quot;lm() function with Robust SE&quot;), notes.append = TRUE ) ## ## How standardized fertility measure is affected by socio-economic indicators? ## ========================================================================================================================================================================= ## Dependent variable: ## ------------------------------------------------------------------------------------------------------------------------------------------- ## rf_1_year[&quot;2013-01-31/2017-12-31&quot;] rf_1_year[&quot;2013-01-31/2017-12-31&quot;] rf_1_year[&quot;2013-01-31/2017-12-31&quot;] rf_1_year[&quot;2013-01-31/2017-12-31&quot;] ## Iter1 Iter-2 Iter-3 Iter-4 ## (1) (2) (3) (4) ## ------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ## Intercept 0.002 0.001 0.0004 -0.001 ## (0.003) (0.003) (0.004) (0.01) ## SMI 0.76 1.06 0.98 1.29 ## (0.11)*** (0.11)*** (0.09)*** (0.19)*** ## ------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ## Observations 60 60 60 60 ## R2 0.49 0.63 0.52 0.32 ## Adjusted R2 0.48 0.62 0.52 0.31 ## Residual Std. Error (df = 58) 0.02 0.03 0.03 0.06 ## F Statistic (df = 1; 58) 54.95*** 96.89*** 63.98*** 27.48*** ## ========================================================================================================================================================================= ## Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 ## datasets::swiss ## lm() function with Robust SE As we can see, we can report scientific regression outputs with ease. Let us now explore how we were able to create the output. First note that stargazer needs you to already have run the repsective regressions and assign them to a variable (such as reg_Nestle). Then, you just add the respective regression variables to your model. The function works with the following inputs: x: We define as x the regression output variables we created in a previous step type: This is the output type. You can choose between text, latex or html, depending on which program you want to show the output `report: A character string containing only elements of “v”, “c”, “s”,“t”, “p”, * that determines whether, and in which order, variable names (“v”), coefficients, (“c”), standard errors/confidence intervals (“s”), test statistics (“t”) and pvalues (“p”) should be reported in regression tables. If one of the aforementioned letters is followed by an asterisk (\"*\"), significance stars will be reported se: The reported standard errors. Here, we can add the HC or HAC (NW) standard errors to account for both heteroskedasticity and autocorrelation no.space / single row: No space between the individual entries / whether estimator and SE output should be printed on one row. This is needed to narrow down the outputs intercept.bottom: Indicates whether the intercept should be displayed as last covariate row or as first digits: Number of decimal digits / p.auto: Indicates whether the t and p values should be automatically displayed omit.stat: Indicates whether you want to omit a statistic. Potential values to omit are: “all”, “adj.rsq”, “aic”, “bic”, “chi2”, “f”, “ll” (log-likelihood), “n”, “mills”, “res.dev”, “rsq”, “ser” (standard error of the regression), “sigma” or “wald” The remaining arguments are quite self-explanatory. Most importantly, we can use the adjusted, robust standard error configurations straight within an argument of the function. For instance, the HC robust Standard Errors are computed as: se = lapply(list_regs, function(x) sqrt(diag(vcovHC(x, type = \"HC1\")))) se = lapply(list_regs, function(x) sqrt(diag(NeweyWest(x, lag = m - 1, prewhite = T, adjust = T)))) After deciding which HC robustness to take for the HC Robust Standard Errors or deciding what m should be in the HAC NW Robust Standard Errors, you can just use either of these commands and add them to the se argument of stargazer. "],["portfolio-theory-mean-variance-optimisation-and-the-capm.html", "Chapter 5 Portfolio Theory: Mean-Variance Optimisation and the CAPM 5.1 Markowitz Portfolio Theory with two Risky Assets 5.2 Markowitz Portfolio Theory with a Risky and a Risk-Free Asset 5.3 Markowitz Portfolio Theory with two Risky and a Risk-Free Asset 5.4 Markowitz Portfolio Theory with N Risky Assets 5.5 The Capital Asset Pricing Model", " Chapter 5 Portfolio Theory: Mean-Variance Optimisation and the CAPM In the previous chapters we focused on the introduction of risk and return properties for individual assets as well as portfolios. Further, we introduced the concept of co-dependence both between individual assets as well between an asset and its underlying market structure. Therein, we derived important notions on the statistical requirements to draw causal conclusions. Within these chapters, we focused exclusively on the relationship between risk and return. Naturally, one definition we made was that risk must be compensated for with an appropriate rate of return. Noted as risk parity hypothesis, we created measures that put both factors into relation to each other and compared these ratios throughout a number of asset classes. In this chapter, we now take the underlying model on asset returns, the ideas on risk and return characteristics of portfolios as well as the statistical properties introduced and combine them to form the basis of fempirical portfolio analysis frameworks. Specifically, we present the well-known Mean-Variance portfolio analysis concept for optimal asset allocations, first presented by Henry Markowitz (1959). This theory serves as fundament for most modern asset management classes. The framework uses concepts and assumptions we introduced in previous chapters. As such, it is assumed that the asset returns are normally distributed and investor preferences solely depend on risk and return characteristics. According to the risk parity hypothesis, additional risk must be compensated for by higher returns. Consequently, investors are favorable towards portfolios with high returns and unfavorable towards such with higher variances. As risk and return appear to be positively correlated, investors face a trade-off of these metrics between individual portfolios. Markowitz proposes a framework which quantifies this mean-variance trade off and creates a method to define the optimal portfolio choice. Henceforth, in this chapter we provide the theoretical foundations as well as the practical application to the Markowitz portfolio optimisation framework. Therein, we first cover the case of two risky and one risk-free asset. We will use this setting to facilitate the main notions as well as replicate the major results and implications of Markowitz’s ideas, both graphically as well with forms of matrix algebra. Later on, we will generalise the simple model into a setting with multiple risky assets. Building on this, we introduce the concept of short sales contracts and, eventually, give rise to the notion of portfolio risk budgeting. 5.1 Markowitz Portfolio Theory with two Risky Assets 5.1.1 The case of portfolios In chapter two, we already discussed the concept of risk and return. Therein, we defined what a portfolio return is, how we can calculate different forms of returns, what co-dependence properties are and how we can quantify them in order to calculate the appropriate amount of risk for a portfolio. These definitions led us to the concept of portfolio risk and return relationships, where we defined the following: \\[ \\begin{align*} \\mu_P = E[R_P] &amp;= x_AR_A + x_BR_B \\\\ \\sigma_P^2 &amp;= x_A^2\\sigma_A^2 + x_B^2\\sigma_B^2 + 2x_Ax_B\\sigma_{AB}\\\\ \\sigma_{AB} &amp;= \\rho_{AB}\\sigma_A\\sigma_B \\\\ x_A + x_B &amp;= 1 \\\\ R_P &amp;\\sim N(\\mu_P, \\sigma_P^2) \\end{align*} \\] where we derived each concept in the previous chapters. Regarding the co-dependence property, we stated that a non perfectly correlated portfolio will reduce overall portfolio risk, due to the concept of diversification. Thus finding assets with non-perfectly positively correlated returns can be beneficial when forming portfolios because risk, as measured by portfolio standard deviation, can be reduced. We also derived this property in the linear algebra introduction. This concepts proves that a risk reduction effect is evident in long-only, two and multiple asset cases when forming portfolios, with the exact amount of reduction quantified by the correlation coefficient, \\(\\rho_{AB}\\). 5.1.2 The set of attainable portfolios Looking at portfolios, we also introduced the concept of weights. We stated that, in order to create portfolios, we need to define which assets receive which weights and how this weight distribution, or allocation, can influence both risk and return characteristics. Naturally, putting more weight on assets with higher volatility and higher returns will increase both the expected return as well as the risk of a portfolio. Throughout, we specified two weighting concepts: Equally-Weighted as well as Value-Weighted portfolios, whereas the former uses a 1/N weighting ratio, while the latter uses the inverse of the market capitalisation ratio to define weights of individual assets. However, in theory, we could use any weighting scheme we want in order to form portfolio, as long as the sum of all weights add up to one. This brings us to the next theory. The set of all attainable, or feasible, portfolios is defined as all portfolios that can be created by changing the portfolio weights of the individual assets, as long as their sum adds up to one. This theorem creates the well-known, parabola shaped relationship of risk and return for a portfolio when varying the weights of its components. Without going too deep into the mathematics, let’s visualize this relationship for the case of a portfolio consisting of two assets. # First define some return and risk characteristics mu_A = 0.149 sigma_A = 0.265 mu_B = 0.067 sigma_B = 0.167 rho_AB = -0.135 sigma_AB = sigma_B*sigma_A*rho_AB # Then, we also define a sequence of 30 portfolio weights for A and B x_A = seq(from=-0.8, to=2.2, by=0.1) x_B = 1 - x_A # Create the expected return as well as the variance and standard deviation of each portfolios mu_AB = x_A*mu_A + x_B*mu_B var_AB = x_A^2*sigma_A^2 + x_B^2*sigma_B^2 + 2*x_A*x_B*sigma_AB sd_AB = sqrt(var_AB) # Create a data frame for the relationship risk_return_df &lt;- as.data.frame(cbind(mu_AB, sd_AB, x_A, x_B)) colnames(risk_return_df) &lt;- c(&quot;Portfolio_Return&quot;, &quot;Portfolio_Risk&quot;, &quot;Weight_A&quot;, &quot;Weight_B&quot;) # Now, let&#39;s visualise the relationship risk_return_df %&gt;% ggplot(aes(x= Portfolio_Risk, y = Portfolio_Return)) + geom_point() + geom_point(data = subset(risk_return_df, Weight_A &gt;= 0 &amp; Weight_B &gt;= 0), color = &quot;goldenrod&quot;, aes(x= Portfolio_Risk, y = Portfolio_Return)) + geom_point(data = subset(risk_return_df, Portfolio_Risk == min(Portfolio_Risk)), color = &quot;dodgerblue3&quot;, aes(x= Portfolio_Risk, y = Portfolio_Return)) + geom_point(data = subset(risk_return_df, Weight_A == 1), color = &quot;springgreen4&quot;, aes(x= Portfolio_Risk, y = Portfolio_Return)) + geom_point(data = subset(risk_return_df, Weight_B == 1), color = &quot;darkorchid1&quot;, aes(x= Portfolio_Risk, y = Portfolio_Return)) + annotate(&#39;text&#39;,x = 0.22 ,y = 0.16, label = paste(&#39;PF of Asset A only&#39;), size = 3, color = &quot;springgreen4&quot;) + annotate(&#39;text&#39;,x = 0.22 ,y = 0.07, label = paste(&#39;PF of Asset B only&#39;), size = 3, color = &quot;darkorchid1&quot;) + ylab(expression(mu[p])) + xlab(expression(sigma[p])) + ggtitle(&quot;Portfolio Frontier for 30 PFs formed with differing weights of two assets&quot;) + labs(color=&#39;Factor Portfolios&#39;) + theme(plot.title= element_text(size=14, color=&quot;grey26&quot;, hjust=0.3,lineheight=2.4, margin=margin(15,0,15,0)), panel.background = element_rect(fill=&quot;#f7f7f7&quot;), panel.grid.major.y = element_line(size = 0.5, linetype = &quot;solid&quot;, color = &quot;grey&quot;), panel.grid.minor = element_blank(), panel.grid.major.x = element_blank(), plot.background = element_rect(fill=&quot;#f7f7f7&quot;, color = &quot;#f7f7f7&quot;), axis.title.y = element_text(color=&quot;grey26&quot;, size=12, margin=margin(0,10,0,10)), axis.title.x = element_text(color=&quot;grey26&quot;, size=12, margin=margin(10,0,10,0)), axis.line = element_line(color = &quot;grey&quot;)) This plot is referred to as Markowitz Bullet. The black dots represent 30 distinct risk and return combinations, created by varying the individual constituent weights. The golden, purple, green and blue dots represent long-only portfolios. The black dots represent long-short strategies of two assets. Further, the purple and the green dot represent portfolios consisting of only the Asset B and A, respectively. Moreover, the blue dot represents the global minimum variance portfolio, a notion which we will cover in more detail later. This shape is vital for the portfolio management theory as it displays the concept of diversification quite clearly. The idea that we just visualised behind the diversification potential can be understood as follows. Suppose we first have an investment in Asset B only. We can diversify and improve our portfolio metrics by rebalancing the portfolio such that we now include some of Asset A. This is indicated by th golden dots. As we can see, diversification increases the expected return while simultaneously decreasing the risk associated. Consequently, these portfolios are superior to the B only case. This is achieved until we reach the blue dot, which displays the minimum variance of a portfolio. This is also known as minimum-variance portfolio. Following this portfolio, we see that both risk and return increase monotonically. Consequently, the “optimal” portfolio in these cases depend on the investor preferences. We then follow a long-only strategy until we reach the green dot, which only consists of portfolio A. Afterwards, we follow with a long-A and short-B strategy. 5.1.3 The Minimum-Variance Portfolio The Minimum-Variance (MV) Portfolio is a key concept in Markowitz’s Portfolio optimisation. It is used to give you an intuition on how to form weights on the portfolio frontier. It can be derived using a constrained optimisation problem. We make a small exercise in calculus, using the substitution method, to derive the weight of the MV portfolio. As a side note, you can also use the Lagrangian method to determine the weights. To get the MV portfolio, we use the minimisation of the portfolio variance, subject to the fact that both weights must add up to 1. \\[ \\begin{align} \\min_{x_A, x_B} \\sigma_P^2 &amp;= x_A\\sigma_A^2 + x_B\\sigma_B^2 + 2x_Ax_B\\sigma_{AB} \\\\ \\text{s.t. } x_a + x_B &amp;= 1 \\end{align} \\] Let’s start with the optimisation. If we substitute \\(x_B = 1 - x_A\\), we obtain \\[ \\begin{align} \\min_{x_A} \\sigma_P^2 &amp;= x_A\\sigma_A^2 + x_B\\sigma_B^2 + 2x_Ax_B\\sigma_{AB} \\\\ &amp;= x_A^2\\sigma_A^2 +(1-x_A)^2\\sigma_B^2 + 2x_A(1-x_A)\\sigma_{AB} &amp;&amp; x_B = 1-x_A \\\\ \\frac{d\\sigma_P^2}{dx_A} = 0 &amp;= 2x_A\\sigma_A^2 - 2(1-x_A)\\sigma_B^2 + 2\\sigma_{AB} -4x_A\\sigma_{AB} \\\\ &amp;= 2x_A\\sigma_A^2 - 2(1-x_A)\\sigma_B^2 + 2\\sigma_{AB}(1-x_A) \\end{align} \\] If we now solve for \\(x_A\\), we obtain the optimal weight of Asset A and Asset B for the MV portfolio as: \\[ \\begin{align} x_A &amp;= \\frac{\\sigma_B^2 - \\sigma_{AB}}{\\sigma_A^2 + \\sigma_B^2 - 2\\sigma_{AB}}\\\\ x_B &amp;= 1 - x_A \\end{align} \\] In the case of our assets, let’s quickly calculate the respective weights: x_A_MV = (sigma_B^2 - sigma_AB)/(sigma_A^2 + sigma_B^2 - 2*sigma_AB) x_B_MV = 1 - x_A_MV mu_MV = mu_A*x_A_MV + mu_B*x_B_MV sd_MV = sqrt(sigma_A^2*x_A_MV^2 + sigma_B^2*x_B_MV^2 + 2*x_A_MV*x_B_MV*sigma_AB) MV_df &lt;- as.data.frame(cbind(x_A_MV, x_B_MV, mu_MV, sd_MV)) colnames(MV_df) &lt;- c(&quot;Weight Asset A&quot;, &quot;Weight Asset B&quot;, &quot;Expected Return&quot;, &quot;Volatility (StD)&quot;) MV_df ## Weight Asset A Weight Asset B Expected Return Volatility (StD) ## 1 0.3076735 0.6923265 0.09222923 0.1321746 5.1.4 The role of correlation on the frontier of portfolios As we said, the portfolio frontier shows the correlation properties between assets and can therein display the diversification potential. As such, it is interesting to see how the portfolio changes with varying correlations. In order to show this, we need to slightly adjust the formula for the variance such that we are able to write the formula in terms of the correlation coefficient, \\(\\rho_{AB}\\). # Let&#39;s define the variance as follows: # First define some return and risk characteristics mu_A = 0.149 sigma_A = 0.265 mu_B = 0.067 sigma_B = 0.167 # Then, we also define a sequence of 30 portfolio weights for A and B x_A = seq(from=-0.8, to=2.2, by=0.1) x_B = 1 - x_A # Calculate the mean return (the same for all as it does not depend on the rho&#39;s) mu_AB = x_A*mu_A + x_B*mu_B # Define a list with rho&#39;s rho &lt;- c(-1, -0.5, 0, 0.5, 1) # Create the different standard deviations for (i in rho){ cov_AB = sigma_B*sigma_A*i var_AB = x_A^2*sigma_A^2 + x_B^2*sigma_B^2 + 2*x_A*x_B*cov_AB sd_AB = sqrt(var_AB) if (i == -1){ sd_AB_final &lt;- sd_AB } else { sd_AB_final &lt;- cbind(sd_AB_final, sd_AB) } } sd_AB_final_df &lt;- as.data.frame(cbind(mu_AB, sd_AB_final)) colnames(sd_AB_final_df) &lt;- c(&quot;Return&quot;, &quot;rho_1_neg&quot;, &quot;rho_0.5_neg&quot;, &quot;rho0&quot;, &quot;rho0.5&quot;, &quot;rho1&quot;) # Visualise the relationship sd_AB_final_df %&gt;% ggplot(aes(x= rho_1_neg, y = Return)) + geom_point() + geom_path() + geom_point(color = &quot;darkorange2&quot;, aes(x= rho_0.5_neg, y = Return)) + geom_path(color = &quot;darkorange2&quot;, aes(x= rho_0.5_neg, y = Return)) + geom_point(color = &quot;darkorchid1&quot;, aes(x= rho0, y = Return)) + geom_path(color = &quot;darkorchid1&quot;, aes(x= rho0, y = Return)) + geom_point(color = &quot;dodgerblue1&quot;, aes(x= rho0.5, y = Return)) + geom_path(color = &quot;dodgerblue1&quot;, aes(x= rho0.5, y = Return)) + geom_point(color = &quot;springgreen3&quot;, aes(x= rho1, y = Return)) + geom_path(color = &quot;springgreen3&quot;, aes(x= rho1, y = Return)) + annotate(&#39;text&#39;,x = 0.70 ,y = 0.19, label = paste(&#39;rho = -1&#39;), size = 3.5) + annotate(&#39;text&#39;,x = 0.70 ,y = 0.175, label = paste(&#39;rho = -0.5&#39;), size = 3.5, color = &quot;darkorange2&quot;) + annotate(&#39;text&#39;,x = 0.70 ,y = 0.16, label = paste(&#39;rho = 0&#39;), size = 3.5, color = &quot;darkorchid1&quot;) + annotate(&#39;text&#39;,x = 0.70 ,y = 0.14, label = paste(&#39;rho = 0.5&#39;), size = 3.5, color = &quot;dodgerblue1&quot;) + annotate(&#39;text&#39;,x = 0.70 ,y = 0.125, label = paste(&#39;rho = 1&#39;), size = 3.5, color = &quot;springgreen3&quot;) + ylab(expression(mu[p])) + xlab(expression(sigma[p])) + ggtitle(&quot;Portfolio Frontier for 30 PFs for different correlation coefficients&quot;) + labs(color=&#39;Factor Portfolios&#39;) + theme(plot.title= element_text(size=14, color=&quot;grey26&quot;, hjust=0.3,lineheight=2.4, margin=margin(15,0,15,0)), panel.background = element_rect(fill=&quot;#f7f7f7&quot;), panel.grid.major.y = element_line(size = 0.5, linetype = &quot;solid&quot;, color = &quot;grey&quot;), panel.grid.minor = element_blank(), panel.grid.major.x = element_blank(), plot.background = element_rect(fill=&quot;#f7f7f7&quot;, color = &quot;#f7f7f7&quot;), axis.title.y = element_text(color=&quot;grey26&quot;, size=12, margin=margin(0,10,0,10)), axis.title.x = element_text(color=&quot;grey26&quot;, size=12, margin=margin(10,0,10,0)), axis.line = element_line(color = &quot;grey&quot;)) 5.1.5 Efficient Portfolios with two risky assets As we have seen, the bullet shape of each portfolio combination splits the portfolio set horizontally into two regions, based on their respective risk and return relation. To be precise, we can observe that, for a quasi identical level of risk we can attain two vastly different levels of expected portfolio return. Based on this observation, we can sort the portfolio into two regions: Efficient and Inefficient portfolios. Efficient Portfolios are the set of all attainable portfolios that have the highest return for any given level of risk (measured by its standard deviation). Graphically, these are all the portfolios that are at or above the MV portfolio. Inefficient Portfolios are the set of all attainable portfolios that do not have the highest return for any given level of risk (measured by its standard deviation). Graphically, these are all the portfolios that are at or below the MV portfolio. We can visually depict this as follows: # First define some return and risk characteristics mu_A = 0.149 sigma_A = 0.265 mu_B = 0.067 sigma_B = 0.167 rho_AB = -0.135 sigma_AB = sigma_B*sigma_A*rho_AB # Then, we also define a sequence of 30 portfolio weights for A and B x_A = seq(from=-0.8, to=2.2, by=0.1) x_B = 1 - x_A # Create the expected return as well as the variance and standard deviation of each portfolios mu_AB = x_A*mu_A + x_B*mu_B var_AB = x_A^2*sigma_A^2 + x_B^2*sigma_B^2 + 2*x_A*x_B*sigma_AB sd_AB = sqrt(var_AB) # Create a data frame for the relationship risk_return_df &lt;- as.data.frame(cbind(mu_AB, sd_AB, x_A, x_B)) colnames(risk_return_df) &lt;- c(&quot;Portfolio_Return&quot;, &quot;Portfolio_Risk&quot;, &quot;Weight_A&quot;, &quot;Weight_B&quot;) # Now, let&#39;s visualise the relationship risk_return_df %&gt;% ggplot(aes(x= Portfolio_Risk, y = Portfolio_Return)) + geom_point() + geom_point(data = subset(risk_return_df, Portfolio_Return &gt;= 0.0916), color = &quot;goldenrod&quot;, aes(x= Portfolio_Risk, y = Portfolio_Return)) + geom_point(data = subset(risk_return_df, Portfolio_Return &lt; 0.0916), color = &quot;dodgerblue2&quot;, aes(x= Portfolio_Risk, y = Portfolio_Return)) + ylab(expression(mu[p])) + xlab(expression(sigma[p])) + ggtitle(&quot;Portfolio Frontier for 30 PFs - Efficient and Inefficient PFs&quot;) + geom_vline(xintercept = 0.385, linetype = &quot;dashed&quot;) + annotate(&#39;text&#39;,x = 0.42 ,y = 0.17, label = paste(&#39;Efficient PF&#39;), size = 3.5, color = &quot;goldenrod&quot;) + annotate(&#39;text&#39;,x = 0.424 ,y = 0.02, label = paste(&#39;Inefficient PF&#39;), size = 3.5, color = &quot;dodgerblue2&quot;) + labs(color=&#39;Factor Portfolios&#39;) + theme(plot.title= element_text(size=14, color=&quot;grey26&quot;, hjust=0.3,lineheight=2.4, margin=margin(15,0,15,0)), panel.background = element_rect(fill=&quot;#f7f7f7&quot;), panel.grid.major.y = element_line(size = 0.5, linetype = &quot;solid&quot;, color = &quot;grey&quot;), panel.grid.minor = element_blank(), panel.grid.major.x = element_blank(), plot.background = element_rect(fill=&quot;#f7f7f7&quot;, color = &quot;#f7f7f7&quot;), axis.title.y = element_text(color=&quot;grey26&quot;, size=12, margin=margin(0,10,0,10)), axis.title.x = element_text(color=&quot;grey26&quot;, size=12, margin=margin(10,0,10,0)), axis.line = element_line(color = &quot;grey&quot;)) 5.2 Markowitz Portfolio Theory with a Risky and a Risk-Free Asset In the previous chapters we also introduced the concept of a risk-free asset. In essence, risk-free assets are such assets that always have the same pay-off, irrespective of the state of the world. Thus, their volatility structure is constant. As a consequence, they have a quasi-zero variance and standard deviation. Prominent examples of risk-free assets are the Swiss 1 Year Government Bond with which we have worked throughout the class or the US T-Bills. These notions imply the following properties of risk free assets, \\(r_f\\): \\[ \\begin{align} E[r_f] &amp;= r_f \\\\ \\sigma_f^2 &amp;= 0 \\\\ cov(r_A, r_f) &amp;= 0 \\end{align} \\] If we introduce the risk-free asset, into the portfolio theory, our portfolio return becomes: \\[ R_P = x_fr_f + x_Ar_A = (1-x_A)r_f + x_Ar_A = r_f + x_A(r_A - r_f) \\] whereas \\(x_A(r_A - r_f)\\) is the weighted excess return of the risky asset over the risk-free asset. Consistent with the risk parity hypothesis, we expect this excess return to be positive, as rational investors expect a higher return when incorporating riskier assets to compensate for said increase in risk. In said case, we can calculate both the portfolio expected return as well as the variance as: \\[ \\begin{align} \\mu_P &amp;= r_f + x_A(\\mu_A - r_f) \\\\ \\sigma_P &amp;= x_A\\sigma_A \\end{align} \\] In case of a risky and a risk-free asset, we can draw the portfolio frontier in a similar fashion to the frontier assuming that we have two perfectly negatively correlated assets: # First define some return and risk characteristics mu_A = 0.149 sigma_A = 0.265 mu_f = 0.035 sigma_f = 0 rho_Af = 0 sigma_Af = 0 # Then, we also define a sequence of 30 portfolio weights for A and B x_A = seq(from=-0.8, to=2.2, by=0.1) x_f = 1 - x_A # Create the expected return as well as the variance and standard deviation of each portfolios mu_Af = x_A*mu_A + x_f*mu_f var_Af = x_A^2*sigma_A^2 + x_f^2*sigma_f^2 + 2*x_A*x_f*sigma_Af sd_Af = sqrt(var_Af) # Create a data frame for the relationship risk_return_df_rf &lt;- as.data.frame(cbind(mu_Af, sd_Af, x_A, x_f)) colnames(risk_return_df_rf) &lt;- c(&quot;Portfolio_Return&quot;, &quot;Portfolio_Risk&quot;, &quot;Weight_A&quot;, &quot;Weight_B&quot;) # Now, let&#39;s visualise the relationship risk_return_df_rf %&gt;% ggplot(aes(x= Portfolio_Risk, y = Portfolio_Return)) + geom_point() + geom_point(data = subset(risk_return_df_rf, Portfolio_Return &gt;= 0.0350), color = &quot;goldenrod&quot;, aes(x= Portfolio_Risk, y = Portfolio_Return)) + geom_point(data = subset(risk_return_df_rf, Portfolio_Return &lt; 0.0350), color = &quot;dodgerblue2&quot;, aes(x= Portfolio_Risk, y = Portfolio_Return)) + geom_point(data = subset(risk_return_df_rf, Weight_A &gt;= 1), color = &quot;darkorchid1&quot;, aes(x= Portfolio_Risk, y = Portfolio_Return)) + annotate(&#39;text&#39;,x = -0.02 ,y = 0.05, label = &quot;risk-free&quot;, size = 3.5, color = &quot;black&quot;) + annotate(&#39;text&#39;, x = 0.32 ,y = 0.14, label = paste(&#39;Risky Asset PF&#39;), size = 3.5, color = &quot;darkorchid1&quot;) + ylab(expression(mu[p])) + xlab(expression(sigma[p])) + ggtitle(&quot;Portfolio Frontier for 30 PFs - Efficient and Inefficient PFss&quot;) + geom_hline(yintercept = 0.0350, linetype = &quot;dashed&quot;) + labs(color=&#39;Factor Portfolios&#39;) + theme(plot.title= element_text(size=14, color=&quot;grey26&quot;, hjust=0.3,lineheight=2.4, margin=margin(15,0,15,0)), panel.background = element_rect(fill=&quot;#f7f7f7&quot;), panel.grid.major.y = element_line(size = 0.5, linetype = &quot;solid&quot;, color = &quot;grey&quot;), panel.grid.minor = element_blank(), panel.grid.major.x = element_blank(), plot.background = element_rect(fill=&quot;#f7f7f7&quot;, color = &quot;#f7f7f7&quot;), axis.title.y = element_text(color=&quot;grey26&quot;, size=12, margin=margin(0,10,0,10)), axis.title.x = element_text(color=&quot;grey26&quot;, size=12, margin=margin(10,0,10,0)), axis.line = element_line(color = &quot;grey&quot;)) 5.2.1 The Capital Allocation Line (CAL) As we can see, we again obtain two straight lines, consisting of inefficient and efficient portfolios. Under the assumption that \\(x &gt; 0\\) (no short sales) we can use the variance equation and solve for x: \\[ x_A = \\frac{\\sigma_P} {\\sigma_A} \\] This implies that the optimal weight of the risky asset is given by the variance of the asset relative to the variance of the portfolio. As such, the return of the risky asset is weighted by the inverse of the asset risk relative to the overall portfolio risk. If we now substitute this result into the expected return, we obtain: \\[ \\mu_P = r_f + \\frac{\\mu_A - r_f}{\\sigma_A}\\cdot \\sigma_P \\] This variable is called the Capital Allocation Line (CAL). This is exactly the line that consists of the dots we just displayed. It describes how the investment is allocated between the risk-free asset and the risky asset. For dots in close proximity to the risk-free asset, we understand that most of the capital is allocated in riskless assets (consequently, the return is close to the risk-free). The further we deviate from the risk-free asset, the more we allocate into the risky assets, with the (theoretical) maximum of a portfolio consisting only of the risky asset. The purple dots represent long-short portfolios where the risk-free asset is shorted to invest in additional quantities of the risky asset. The slope of the CAL is familiar to us. If we take the derivative w.r.t \\(\\sigma_P\\), we obtain: \\[ \\frac{d\\mu_P}{d\\sigma_P} = \\frac{\\mu_A - r_f}{\\sigma_A} \\] Note from the previous chapters that it puts an excess return in relation to the risk. This is also known as Sharpe Ratio, which measures the risk premium per additional unit of risk. The Sharpe ratio ultimately quantifies the efficiency of different risky and risk-free portfolio combinations. In essence, we understand that the portfolio with the higher Sharpe Ratio is more efficient than the portfolio with the lower ratio. Consequently, these portfolio variations according to weight assignments will induce a steeper slope. This is the reason for the appeal of the Sharpe Ratio in investment assessments. # First define some return and risk characteristics mu_A = 0.149 sigma_A = 0.265 mu_B = 0.074 mu_B = 0.067 sigma_B = 0.167 mu_f = 0.035 sigma_f = 0 rho_Af = 0 sigma_Af = 0 rho_Bf = 0 sigma_Bf = 0 # Then, we also define a sequence of 30 portfolio weights for A and B x_A = seq(from=-0.8, to=2.2, by=0.1) x_f = 1 - x_A # Create the expected return as well as the variance and standard deviation of each portfolios mu_Af = x_A*mu_A + x_f*mu_f var_Af = x_A^2*sigma_A^2 + x_f^2*sigma_f^2 + 2*x_A*x_f*sigma_Af sd_Af = sqrt(var_Af) # Do the same for PF B x_B = seq(from=-0.8, to=2.2, by=0.1) x_f = 1 - x_B mu_Bf = x_B*mu_B + x_f*mu_f var_Bf = x_B^2*sigma_B^2 + x_f^2*sigma_f^2 + 2*x_B*x_f*sigma_Bf sd_Bf = sqrt(var_Bf) # Create a data frame for the relationship risk_return_df_rf &lt;- as.data.frame(cbind(mu_Af, sd_Af, mu_Bf, sd_Bf, x_A, x_f)) colnames(risk_return_df_rf) &lt;- c(&quot;Portfolio_Return_A&quot;, &quot;Portfolio_Risk_A&quot;, &quot;Portfolio_Return_B&quot;, &quot;Portfolio_Risk_B&quot;, &quot;Weight_A&quot;, &quot;Weight_B&quot;) # Subset with only efficient PFs risk_return_df_rf_sub &lt;- subset(risk_return_df_rf, Weight_A &gt;= 0) # Now, let&#39;s visualise the relationship risk_return_df_rf_sub %&gt;% ggplot(aes(x= Portfolio_Risk_A, y = Portfolio_Return_A)) + geom_point(color = &quot;goldenrod&quot;,) + geom_point(color = &quot;dodgerblue2&quot;, aes(x= Portfolio_Risk_B, y = Portfolio_Return_B)) + annotate(&#39;text&#39;,x = 0.35 ,y = 0.16, label = &quot;Portfolio A CAL&quot;, size = 3.5, color = &quot;goldenrod&quot;) + annotate(&#39;text&#39;,x = 0.25 ,y = 0.065, label = &quot;Portfolio B CAL&quot;, size = 3.5, color = &quot;dodgerblue2&quot;) + annotate(&#39;text&#39;,x = -0.02 ,y = 0.05, label = &quot;risk-free&quot;, size = 3.5, color = &quot;black&quot;) + ylab(expression(mu[p])) + xlab(expression(sigma[p])) + ggtitle(&quot;Portfolio Frontier for 30 PFs - Comparison of risk-free and risky portfolios&quot;) + labs(color=&#39;Factor Portfolios&#39;) + theme(plot.title= element_text(size=14, color=&quot;grey26&quot;, hjust=0.3,lineheight=2.4, margin=margin(15,0,15,0)), panel.background = element_rect(fill=&quot;#f7f7f7&quot;), panel.grid.major.y = element_line(size = 0.5, linetype = &quot;solid&quot;, color = &quot;grey&quot;), panel.grid.minor = element_blank(), panel.grid.major.x = element_blank(), plot.background = element_rect(fill=&quot;#f7f7f7&quot;, color = &quot;#f7f7f7&quot;), axis.title.y = element_text(color=&quot;grey26&quot;, size=12, margin=margin(0,10,0,10)), axis.title.x = element_text(color=&quot;grey26&quot;, size=12, margin=margin(10,0,10,0)), axis.line = element_line(color = &quot;grey&quot;)) As we can see, the Slope of PF A is higher than the slope of PF B. Accordingly, for each unit of risk, we have a superior excess return profile. We can also quantify this by calculating the Sharpe Ratio directly. In our case, this is the following. ## [1] &quot;The Sharpe Ratio of Asset A is 0.43 and the Sharpe Ratio of Asset B is 0.27&quot; 5.3 Markowitz Portfolio Theory with two Risky and a Risk-Free Asset In a next step, we can try to quantify a portfolio consisting of three assets, whereas two are risky and one is risk-free. From earlier results we understand that any linear combination of two portfolios consisting of a risky and the same risk-free asset will also follow a linear slope. Given the two different CALs of portfolio A and B in the previous chapter, it is logical that a weighted average of both returns and risk will quantify the slope of the resulting portfolio. Consequently, the efficient set of portfolios will be a straight line with intercept in \\(r_f\\). Further, we need to understand whether a linear combination of the assets induces a more efficient portfolio, compared to if we would only invest in one risky asset and the risk-free. As such, we can derive that the three-piece portfolio must have the maximum slope while it still incorporates any linear combination of risky assets. Consequently, we need to solve for a constrained maximisation of the Sharpe Ratio to obtain the optimal, or most efficient, slope and derive the optimal weighting of the individual assets. In order to find this, we need to define the Tangency Portfolio. 5.3.1 Tangency Portfolio The Tangency Portfolio represents the portfolio in which the CAL of the three-piece portfolio is exactly tangent to the set of differently weighted risky asset only portfolios. In essence, it is the portfolio which is a combination of the two risky and the risk-free assets such that it is in the set of feasible risky assets and maximises the slope of the CAL. It is the portfolio which has the best possible excess return per additional unit of risk. Geometrically speaking, this is the “steepest sloped”, or most efficient, portfolio combination we can attain conditional on the fact that the portfolio must consist of a linear combination of the assets under consideration. As a consequence, portfolios consisting of the risk-free asset as well as the tangency portfolio are the efficient portfolio consisting of risk-free and risky assets. Let’s visually depict this in more detail below. # First define some return and risk characteristics mu_A = 0.149 sigma_A = 0.265 mu_B = 0.074 mu_B = 0.067 sigma_B = 0.167 mu_f = 0.035 sigma_f = 0 rho_Af = 0 sigma_Af = 0 rho_Bf = 0 sigma_Bf = 0 # Then, we also define a sequence of 30 portfolio weights for A and B x_A = seq(from=-0.8, to=2.2, by=0.1) x_f = 1 - x_A # Create the expected return as well as the variance and standard deviation of each portfolios mu_Af = x_A*mu_A + x_f*mu_f var_Af = x_A^2*sigma_A^2 + x_f^2*sigma_f^2 + 2*x_A*x_f*sigma_Af sd_Af = sqrt(var_Af) # Do the same for PF B x_B = seq(from=-0.8, to=2.2, by=0.1) x_f = 1 - x_B mu_Bf = x_B*mu_B + x_f*mu_f var_Bf = x_B^2*sigma_B^2 + x_f^2*sigma_f^2 + 2*x_B*x_f*sigma_Bf sd_Bf = sqrt(var_Bf) # Create a data frame for the relationship risk_return_df_rf &lt;- as.data.frame(cbind(mu_Af, sd_Af, mu_Bf, sd_Bf, x_A, x_f)) colnames(risk_return_df_rf) &lt;- c(&quot;Portfolio_Return_A&quot;, &quot;Portfolio_Risk_A&quot;, &quot;Portfolio_Return_B&quot;, &quot;Portfolio_Risk_B&quot;, &quot;Weight_A&quot;, &quot;Weight_B&quot;) # Subset with only efficient PFs risk_return_df_rf_sub &lt;- subset(risk_return_df_rf, Weight_A &gt;= 0) # Now, let&#39;s visualise the relationship risk_return_df_rf_sub %&gt;% ggplot(aes(x= Portfolio_Risk_A, y = Portfolio_Return_A)) + geom_point(color = &quot;goldenrod&quot;) + geom_point(data = risk_return_df, aes(x = Portfolio_Risk, y = Portfolio_Return)) + geom_point(color = &quot;dodgerblue2&quot;, aes(x= Portfolio_Risk_B, y = Portfolio_Return_B)) + annotate(&#39;text&#39;,x = 0.35 ,y = 0.22, label = &quot;Portfolio A CAL&quot;, size = 3.5, color = &quot;goldenrod&quot;) + annotate(&#39;text&#39;,x = 0.35 ,y = 0.08, label = &quot;Portfolio B CAL&quot;, size = 3.5, color = &quot;dodgerblue2&quot;) + annotate(&#39;text&#39;,x = -0.02 ,y = 0.05, label = &quot;risk-free&quot;, size = 3.5, color = &quot;black&quot;) + annotate(&#39;text&#39;,x = 0.55 ,y = 0.19, label = paste(&#39;Set of Risky Asset portfolios&#39;), size = 3, color = &quot;black&quot;) + geom_point(data = subset(risk_return_df, Weight_A == 1), color = &quot;springgreen4&quot;, aes(x= Portfolio_Risk, y = Portfolio_Return)) + geom_point(data = subset(risk_return_df, Weight_B == 1), color = &quot;darkorchid1&quot;, aes(x= Portfolio_Risk, y = Portfolio_Return)) + geom_point(data = subset(risk_return_df, Weight_B == 0.7), color = &quot;firebrick3&quot;, aes(x= Portfolio_Risk, y = Portfolio_Return)) + annotate(&#39;text&#39;,x = 0.22 ,y = 0.16, label = paste(&#39;PF of Asset A only&#39;), size = 3, color = &quot;springgreen4&quot;) + annotate(&#39;text&#39;,x = 0.25 ,y = 0.065, label = paste(&#39;PF of Asset B only&#39;), size = 3, color = &quot;darkorchid1&quot;) + annotate(&#39;text&#39;,x = 0.065 ,y = 0.09, label = paste(&#39;PF of Asset A and B&#39;), size = 3, color = &quot;firebrick3&quot;) + ylab(expression(mu[p])) + xlab(expression(sigma[p])) + ggtitle(&quot;Efficient portfolios, risky asset portfolios and the CAL&quot;) + labs(color=&#39;Factor Portfolios&#39;) + theme(plot.title= element_text(size=14, color=&quot;grey26&quot;, hjust=0.3,lineheight=2.4, margin=margin(15,0,15,0)), panel.background = element_rect(fill=&quot;#f7f7f7&quot;), panel.grid.major.y = element_line(size = 0.5, linetype = &quot;solid&quot;, color = &quot;grey&quot;), panel.grid.minor = element_blank(), panel.grid.major.x = element_blank(), plot.background = element_rect(fill=&quot;#f7f7f7&quot;, color = &quot;#f7f7f7&quot;), axis.title.y = element_text(color=&quot;grey26&quot;, size=12, margin=margin(0,10,0,10)), axis.title.x = element_text(color=&quot;grey26&quot;, size=12, margin=margin(10,0,10,0)), axis.line = element_line(color = &quot;grey&quot;)) This shows well the constrained optimisation problem. In essence, we need to find a portfolio such that we can have the maximal slope of the CAL of the linear combination of the assets while still be in the set of feasible portfolios, which is defined by the set of risky asset portfolios. Consequently, the resulting portfolio must either cross or be tangent to the weighted risky asset portfolio path. As we can see, this is currently only the case for three weighted portfolios. The first is indicated with a purple dot, displaying the portfolio of only investing in Asset B. The second is the green dot, displaying the portfolio of only investing in Asset A. The third one is the red dot, displaying the portfolio for investing in both Asset A and B (In our case, this is a 30-70 split in Asset A and B, respectively). As we can already see graphically, the red and green portfolio are more efficient than the purple portfolio, and red and green have the same efficiency. We already calculated the individual Sharpe Ratios in the previous chapter. ## [1] &quot;The Sharpe Ratio of Asset A of Asset AB is 0.43 and the Sharpe Ratio of Asset B is 0.27&quot; However, we can further improve the slope of the CAL and thus induce a more efficient portfolio combination by only slightly “touching” the set of feasible risky assets. That is, we can find a slope such that we are tangent to this set. This can be achieved by solving for the tangency portfolio. Mathematically, we do so by solving a constrained maximisation problem. This problem looks as follows: \\[ \\begin{align} \\max_{x_A, x_B} \\frac{\\mu_P - r_f}{\\sigma_P} s.t. \\\\ \\mu_P &amp;= x_A\\mu_A + x_B\\mu_B \\\\ \\sigma_P &amp;= \\sqrt{x_A^2\\sigma_A^2 + x_B^2\\sigma_B^2 + 2x_Ax_B\\sigma_{AB}}\\\\ x_A + x_B &amp;= 1 \\end{align} \\] As we already did in the risky assets only case, we can now perform substitution of \\(x_B = 1 - x_A\\) and insert the conditions into the maximisation problem and then solve w.r.t \\(x_A\\). Without going into the derivation based on straight-forward calculus methods, when performing all these steps we obtain: \\[ \\begin{align} x_A^{t} &amp;= \\frac{(\\mu_A - r_f)\\sigma_B^2 - (\\mu_B - r_f)\\sigma_{AB}}{(\\mu_A - r_f)\\sigma_B^2 + (\\mu_B - r_f)\\sigma_A^2 - (\\mu_A + \\mu_B - 2r_f)\\sigma_{AB}} \\\\ x_B^{t} &amp;= 1 - x_A^{t} \\end{align} \\] With this formula, we obtain the optimal weights for both the risky assets such that we have a three-piece portfolio with the steepest efficient and attainable slope that maximises the excess return per additional unit of risk. Let’s quickly calculate this. # Calculate the tangency portfolio weights x_t_A = ((mu_A - mu_f)*sigma_B^2 - (mu_B - mu_f)*sigma_AB) / ((mu_A - mu_f)*sigma_B^2 + (mu_B - mu_f)*sigma_A^2 - (mu_A + mu_B - 2*mu_f)*sigma_AB) x_t_B = 1 - x_t_A # Calculate the mean return and standard deviation mu_t_AB &lt;- x_t_A*mu_A + x_t_B*mu_B sigma_t_AB &lt;- sqrt(x_t_A^2*sigma_A^2 + x_t_B^2*sigma_B^2 + 2*x_t_A*x_t_B*sigma_AB) # Calculate the Sharpe Ratio SR_t_AB &lt;- (mu_t_AB - mu_f) / sigma_t_AB # Create a data frame for all three combinations df_t_AB &lt;- as.data.frame(cbind(x_t_A, x_t_B, mu_t_AB, sigma_t_AB, SR_t_AB)) colnames(df_t_AB) &lt;- c(&quot;Weight A&quot;, &quot;Weight B&quot;, &quot;Expected Return&quot;, &quot;Standard Deviation&quot;, &quot;Sharpe Ratio&quot;) df_A &lt;- as.data.frame(cbind(1, 0, mu_A, sigma_A, SR_A)) colnames(df_A) &lt;- c(&quot;Weight A&quot;, &quot;Weight B&quot;, &quot;Expected Return&quot;, &quot;Standard Deviation&quot;, &quot;Sharpe Ratio&quot;) df_B &lt;- as.data.frame(cbind(0, 1, mu_B, sigma_B, SR_B)) colnames(df_B) &lt;- c(&quot;Weight A&quot;, &quot;Weight B&quot;, &quot;Expected Return&quot;, &quot;Standard Deviation&quot;, &quot;Sharpe Ratio&quot;) # Combine the frame df_total &lt;- as.data.frame(rbind(round(df_t_AB,3), round(df_A,3), round(df_B, 3))) rownames(df_total) &lt;- c(&quot;Tangency PF&quot;, &quot;PF A&quot;, &quot;PF B&quot;) # Show the results df_total ## Weight A Weight B Expected Return Standard Deviation Sharpe Ratio ## Tangency PF 0.535 0.465 0.111 0.152 0.499 ## PF A 1.000 0.000 0.149 0.265 0.430 ## PF B 0.000 1.000 0.067 0.167 0.192 As we can see, a portfolio consisting of 53.5 % of Asset A and 46.5% of Asset B induces the highest Sharpe Ratio and thus has the steepest CAL slope. Consequently, this portfolio is feasible and efficient, as we can visualise below: # First define some return and risk characteristics mu_A = 0.149 sigma_A = 0.265 mu_B = 0.074 mu_B = 0.067 sigma_B = 0.167 mu_f = 0.035 sigma_f = 0 rho_Af = 0 sigma_Af = 0 rho_Bf = 0 sigma_Bf = 0 # Then, we also define a sequence of 30 portfolio weights for A and B x_A = seq(from=-0.8, to=2.2, by=0.1) x_f = 1 - x_A # Create the expected return as well as the variance and standard deviation of each portfolios mu_Af = x_A*mu_A + x_f*mu_f var_Af = x_A^2*sigma_A^2 + x_f^2*sigma_f^2 + 2*x_A*x_f*sigma_Af sd_Af = sqrt(var_Af) # Do the same for PF B x_B = seq(from=-0.8, to=2.2, by=0.1) x_f = 1 - x_B mu_Bf = x_B*mu_B + x_f*mu_f var_Bf = x_B^2*sigma_B^2 + x_f^2*sigma_f^2 + 2*x_B*x_f*sigma_Bf sd_Bf = sqrt(var_Bf) # Create a data frame for the relationship risk_return_df_rf &lt;- as.data.frame(cbind(mu_Af, sd_Af, mu_Bf, sd_Bf, x_A, x_f)) colnames(risk_return_df_rf) &lt;- c(&quot;Portfolio_Return_A&quot;, &quot;Portfolio_Risk_A&quot;, &quot;Portfolio_Return_B&quot;, &quot;Portfolio_Risk_B&quot;, &quot;Weight_A&quot;, &quot;Weight_B&quot;) # Subset with only efficient PFs risk_return_df_rf_sub &lt;- subset(risk_return_df_rf, Weight_A &gt;= 0) # Now, let&#39;s visualise the relationship risk_return_df_rf_sub %&gt;% ggplot(aes(x= Portfolio_Risk_A, y = Portfolio_Return_A)) + geom_point(color = &quot;goldenrod&quot;) + geom_point(data = risk_return_df, aes(x = Portfolio_Risk, y = Portfolio_Return)) + geom_point(color = &quot;dodgerblue2&quot;, aes(x= Portfolio_Risk_B, y = Portfolio_Return_B)) + annotate(&#39;text&#39;,x = 0.55 ,y = 0.25, label = &quot;Portfolio A CAL&quot;, size = 3, color = &quot;goldenrod&quot;) + annotate(&#39;text&#39;,x = 0.35 ,y = 0.08, label = &quot;Portfolio B CAL&quot;, size = 3, color = &quot;dodgerblue2&quot;) + annotate(&#39;text&#39;,x = -0.02 ,y = 0.05, label = &quot;risk-free&quot;, size = 3, color = &quot;black&quot;) + annotate(&#39;text&#39;,x = 0.55 ,y = 0.19, label = paste(&#39;Set of Risky Asset portfolios&#39;), size = 3, color = &quot;black&quot;) + geom_point(data = subset(risk_return_df, Weight_A == 1), color = &quot;springgreen4&quot;, aes(x= Portfolio_Risk, y = Portfolio_Return)) + geom_point(data = subset(risk_return_df, Weight_B == 1), color = &quot;darkorchid1&quot;, aes(x= Portfolio_Risk, y = Portfolio_Return)) + geom_point(aes(x=sigma_t_AB, y = mu_t_AB), color = &quot;darkorange3&quot;) + annotate(&#39;text&#39;,x = 0.085 ,y = 0.12, label = &quot;Tangency portfolio&quot;, size = 3.5, color = &quot;darkorange3&quot;) + ylab(expression(mu[p])) + xlab(expression(sigma[p])) + ggtitle(&quot;Tangency portfolio, efficient and feasible portfolios&quot;) + labs(color=&#39;Factor Portfolios&#39;) + theme(plot.title= element_text(size=14, color=&quot;grey26&quot;, hjust=0.3,lineheight=2.4, margin=margin(15,0,15,0)), panel.background = element_rect(fill=&quot;#f7f7f7&quot;), panel.grid.major.y = element_line(size = 0.5, linetype = &quot;solid&quot;, color = &quot;grey&quot;), panel.grid.minor = element_blank(), panel.grid.major.x = element_blank(), plot.background = element_rect(fill=&quot;#f7f7f7&quot;, color = &quot;#f7f7f7&quot;), axis.title.y = element_text(color=&quot;grey26&quot;, size=12, margin=margin(0,10,0,10)), axis.title.x = element_text(color=&quot;grey26&quot;, size=12, margin=margin(10,0,10,0)), axis.line = element_line(color = &quot;grey&quot;)) 5.3.2 Mutual Fund Theorem and Derivation of the Capital Market Line (CML) We now obtained a calculative method to get the weights of both risky assets such that we can maximise the sharpe ratio and obtain efficient and feasible portfolios. Now, we still need to incorporate the risk-free asset into our portfolio. We stated that the efficient portfolios are a combination of risk-free assets and the tangency portfolio. Consequently, they can lie on the line with intercept of the risk-free rate and slope of the Sharpe Ratio of the tangency portfolio. Thus, we can derive the formula for the efficient and feasible set of portfolios in the same way as we did for the usual CAL. To do so, we first calculate the expected return as well as standard deviation of a portfolio consisting of the tangency portfolio and the risk-free asset. \\[ \\begin{align} \\mu_P^e &amp;= x_{tan}\\cdot \\mu_{tan} + (1-x_{tan})\\cdot r_f = r_f + x_{tan}\\cdot (\\mu_{tan} - r_f) \\\\ \\sigma_P^e &amp;= x_{tan}\\cdot \\sigma_{tan} \\end{align} \\] whereas \\(x_{tan}\\) is the weight we want to invest in the tangency portfolio (similar to the CAL in which we defined the weight into the risky asset). In this case, the tangency portfolio can be considered as a mutual fund two risky assets. The optimal allocation of risky and risk-free assets in this case depends on so-called investor preferences. If the investor is risk-averse, then she will choose portfolios with a low volatility, implying a larger investment share in the risk-free asset. If she is risk-seeking, then she will choose portfolios with a higher volatility, implying a larger investment share in the risky asset. However, all the weighted portfolios of the risk-free asset and the tangency portfolio are efficient and feasible. The CAL which combines all these portfolios is also called the Capital Market Line (CML). The slope of the CML is the maximum Sharpe Ratio. Consequently, we can derive the expected return by combining both formulas. \\[ x_{tan} = \\frac{\\sigma_P^e} {\\sigma_{tan}} \\] This implies again that the optimal weight is proportional to the relative standard deviations of the tangency portfolio (= optimal risky portfolio) and the overall portfolio. Substituting again gives us then the CML as: \\[ \\mu_P^e = r_f + \\frac{\\sigma_P^e}{\\sigma_{tan}} \\cdot(\\mu_{tan} - r_f) \\] In this case, we can again take the first derivative of the portfolio return, \\(\\mu_P^e\\), w.r.t the risk of the portfolio, \\(\\sigma_P^e\\), and obtain the slope of the efficient and feasible CAL, which is the Sharpe Ratio of the Tangency Portfolio: \\[ \\frac{\\delta \\mu_P^e}{\\delta \\sigma_P^e} = SR_{tan} = \\frac{\\mu_{tan} - r_f}{\\sigma_{tan}} \\] Similar to the CAL before, this is the excess return per additional unit of risk. In the case of the tangency portfolio, this is both efficient and feasible, implying that it has the best ratio of all weighted portfolios. This makes sense, given the theoretical foundations we discussed earlier. # First define some return and risk characteristics mu_A = 0.149 sigma_A = 0.265 mu_B = 0.074 mu_B = 0.067 sigma_B = 0.167 mu_f = 0.035 sigma_f = 0 rho_Af = 0 sigma_Af = 0 rho_Bf = 0 sigma_Bf = 0 # Then, we also define a sequence of 30 portfolio weights for A and B x_A = seq(from=-0.8, to=2.2, by=0.1) x_f = 1 - x_A # Create the expected return as well as the variance and standard deviation of each portfolios mu_Af = x_A*mu_A + x_f*mu_f var_Af = x_A^2*sigma_A^2 + x_f^2*sigma_f^2 + 2*x_A*x_f*sigma_Af sd_Af = sqrt(var_Af) # Do the same for PF B x_B = seq(from=-0.8, to=2.2, by=0.1) x_f = 1 - x_B mu_Bf = x_B*mu_B + x_f*mu_f var_Bf = x_B^2*sigma_B^2 + x_f^2*sigma_f^2 + 2*x_B*x_f*sigma_Bf sd_Bf = sqrt(var_Bf) # Create the tangency portfolio ## Define the sequence x_tan = seq(from=-0.8, to=2.2, by=0.1) x_f = 1 - x_tan ## Define the expected return and std of the tangency pf mu_tan &lt;- df_t_AB$`Expected Return` sigma_tan &lt;- df_t_AB$`Standard Deviation` ## Calculate the metrics mu_tanf = x_tan*mu_tan + x_f*mu_f var_tanf = x_tan^2*sigma_tan^2 + x_f^2*sigma_f^2 + 2*x_tan*x_f*sigma_Af sd_tanf = sqrt(var_tanf) # Create a data frame for the relationship risk_return_df_rf &lt;- as.data.frame(cbind(mu_Af, sd_Af, mu_Bf, sd_Bf, mu_tanf, sd_tanf, x_A, x_f)) colnames(risk_return_df_rf) &lt;- c(&quot;Portfolio_Return_A&quot;, &quot;Portfolio_Risk_A&quot;, &quot;Portfolio_Return_B&quot;, &quot;Portfolio_Risk_B&quot;, &quot;Portfolio_Return_Tangency_PF&quot;, &quot;Portfolio_Risk_Tangency_PF&quot;, &quot;Weight_A&quot;, &quot;Weight_B&quot;) # Subset with only efficient PFs risk_return_df_rf_sub &lt;- subset(risk_return_df_rf, Weight_A &gt;= 0) # Now, let&#39;s visualise the relationship risk_return_df_rf_sub %&gt;% ggplot(aes(x= Portfolio_Risk_A, y = Portfolio_Return_A)) + geom_point(color = &quot;goldenrod&quot;) + geom_path(color = &quot;goldenrod&quot;) + geom_point(data = risk_return_df, aes(x = Portfolio_Risk, y = Portfolio_Return)) + geom_path(data = risk_return_df, aes(x = Portfolio_Risk, y = Portfolio_Return)) + geom_point(color = &quot;dodgerblue2&quot;, aes(x= Portfolio_Risk_B, y = Portfolio_Return_B)) + geom_path(color = &quot;dodgerblue2&quot;, aes(x = Portfolio_Risk_B, y = Portfolio_Return_B)) + annotate(&#39;text&#39;,x = 0.55 ,y = 0.25, label = &quot;Portfolio A CAL&quot;, size = 3, color = &quot;goldenrod&quot;) + annotate(&#39;text&#39;,x = 0.35 ,y = 0.08, label = &quot;Portfolio B CAL&quot;, size = 3, color = &quot;dodgerblue2&quot;) + annotate(&#39;text&#39;,x = -0.02 ,y = 0.05, label = &quot;risk-free&quot;, size = 3, color = &quot;black&quot;) + annotate(&#39;text&#39;,x = 0.55 ,y = 0.19, label = paste(&#39;Set of Risky Asset portfolios&#39;), size = 3, color = &quot;black&quot;) + geom_point(data = subset(risk_return_df, Weight_A == 1), color = &quot;springgreen4&quot;, aes(x= Portfolio_Risk, y = Portfolio_Return)) + geom_point(data = subset(risk_return_df, Weight_B == 1), color = &quot;darkorchid1&quot;, aes(x= Portfolio_Risk, y = Portfolio_Return)) + geom_point(aes(x=sigma_t_AB, y = mu_t_AB), color = &quot;darkorange3&quot;) + annotate(&#39;text&#39;,x = 0.085 ,y = 0.12, label = &quot;Tangency portfolio&quot;, size = 3.5, color = &quot;darkorange3&quot;) + geom_point(color = &quot;aquamarine4&quot;, aes(x= Portfolio_Risk_Tangency_PF, y = Portfolio_Return_Tangency_PF)) + geom_path(color = &quot;aquamarine4&quot;, aes(x = Portfolio_Risk_Tangency_PF, y = Portfolio_Return_Tangency_PF)) + annotate(&#39;text&#39;,x = 0.22 ,y = 0.23, label = &quot;Capital Market Line (CML) \\n = Efficient &amp; Feasible CAL&quot;, size = 3.5, color = &quot;aquamarine4&quot;) + ylab(expression(mu[p])) + xlab(expression(sigma[p])) + ggtitle(&quot;Tangency portfolio, efficient and feasible portfolios, CML&quot;) + labs(color=&#39;Factor Portfolios&#39;) + theme(plot.title= element_text(size=14, color=&quot;grey26&quot;, hjust=0.3,lineheight=2.4, margin=margin(15,0,15,0)), panel.background = element_rect(fill=&quot;#f7f7f7&quot;), panel.grid.major.y = element_line(size = 0.5, linetype = &quot;solid&quot;, color = &quot;grey&quot;), panel.grid.minor = element_blank(), panel.grid.major.x = element_blank(), plot.background = element_rect(fill=&quot;#f7f7f7&quot;, color = &quot;#f7f7f7&quot;), axis.title.y = element_text(color=&quot;grey26&quot;, size=12, margin=margin(0,10,0,10)), axis.title.x = element_text(color=&quot;grey26&quot;, size=12, margin=margin(10,0,10,0)), axis.line = element_line(color = &quot;grey&quot;)) The concept of the CML as efficient and feasible CAL is an important concept when considering asset returns in the case of the CAPM. The major difference is that the CAPM uses the systematic risk component (the covariance of the security with the market relative to the variance of the market) as factor which quantifies the excess return per additional unit of risk the CAPM uses a Market portfolio instead of the tangency portfolio to account for the risk associated. However, we understand that a Market Portfolio is nothing else than a portfolio consisting of multiple assets. Consequently, before we can dive into the CAPM derivation, it is useful to consider MV optimisation cases in a general setting. That is, in settings in which we have N assets. 5.4 Markowitz Portfolio Theory with N Risky Assets In this chapter, we generalise the mean-variance optimisation of Markowitz. That is, we allow to incorporate a non-sparse model configuration of N assets, whereas N can be any potentially large number. Such a generalisation allows us to assess the MV portoflio allocation in real-world settings, and to comprehend whether a general allocation technique follows the same principles as the two-asset case. Further, by introducing the general case we can extend the idea of Markowitz one step further and draw a market portfolio consisting of these N assets. This will become especially handy when trying to bridge the idea of MV optimising investors related to systematic and idiosyncratic risk components, one of the foundations of the Capital Asset Pricing Model (CAPM). However, generalising the two asset case into N assets poses mathematical difficulties. For instance when working with large portfolios, the algebraic representation of portfolios becomes quite burdensome and heavy to comprehend. However, as we have seen in the matrix algebra repetition, the use of linear algebra manipulations can greatly simplify the calculations in larger dimensional spaces. Further, they allow for an efficient computation of the portfolios, as they use faster paths to perform the respective operations (such as matrix multiplications). As such, they both facilitate improved views and speed to the portfolio construction. As a consequence, when considering general cases of portfolio construction, we will work with linear algebra. 5.4.1 The theoretical foundations of N Assets We start again with the basic configuration of any portfolio. That is, we assume that the returns are IID and follow an approximately normal distribution, as we already did in the last chapter on the SIM. Further, we consider a portfolio consisting of N assets, whereas N can be any arbitrary number (e.g. 100’000). Moreover, \\(R_i\\) is the return of asset i. In this case, we assume that the following holds for each asset i: \\[ \\begin{align} R_i &amp;\\sim N(\\mu_i, \\sigma_i^2)\\\\ cov(R_i,R_j) &amp;= \\sigma_{ij} \\end{align} \\] For any portfolio consisting of the N assets, we assume that the following holds: \\[ \\begin{align} \\mu_p &amp;= \\sum_{i=1}^Nx_i\\mu_i\\\\ \\sigma_p^2 &amp;= \\sum_{i=1}^Nx_i^2\\sigma_i^2 + 2\\sum_{i=1}^n\\sum_{i \\neq j}x_ix_j\\sigma_{ij} \\\\ \\sum_{i=1}^N x_i &amp;= 1 \\end{align} \\] whereas \\(x_i\\) represents the weights allocated to asset i, which is not constrained to short-selling. That is, we allow for \\(x_i\\) to also include negative weights. We will extend this idea later on. As we have seen, the variance of the portfolio depends on: N individual variance terms N(N-1) individual covariance terms To illustrate this. A portfolio consisting of 100’000 assets has a variance consisting of N = 100’000 variance terms and N(N-1) = 100’000*99’999 = 9’999’900’000 covariance terms. You already notice that trying to write this down in non matrix notation is quite impossible. 5.4.1.1 Repetition of the Matrix Notation Let’s quickly recap the main portfolio moments in matrix notation. We first define the return structure. To do so, we create a \\(N \\times 1\\) vector containing the asset returns as well as a \\(N \\times 1\\) vector of weights for any period. That is: \\[ \\textbf{R} = \\begin{pmatrix} R_1 \\\\ \\vdots \\\\ R_N \\end{pmatrix}, \\textbf{x} = \\begin{pmatrix} x_1 \\\\ \\vdots \\\\ x_N \\end{pmatrix} \\] As we understand it, the vector is a random vector. We know that the probability distribution of any random vector is just the joint probability distribution of its individual constituents. Further, we know that a linear transformation of this vector still follows the same, linearly adjusted, joint probability distribution. As the Markowitz model assumes joint normality and since this distribution is characterised entirely by its mean, variance and covariance properties, we can easily express the entire framework with two matrices. First, let’s define the matrix of expected returns. \\[ \\textbf{R} = E\\left[\\begin{pmatrix} R_1 \\\\ \\vdots \\\\ R_N \\end{pmatrix}\\right] = \\begin{pmatrix} E[R_1] \\\\ \\vdots \\\\E[R_N] \\end{pmatrix} = \\begin{pmatrix} \\mu_1 \\\\ \\vdots \\\\ \\mu_N \\end{pmatrix} = \\mu \\] Then, we can define the \\(N \\times N\\) variance-covariance matrix. \\[ \\begin{align} var(\\textbf{R}) &amp;= \\begin{pmatrix} var(R_1) &amp; cov(R_1,R_2) &amp; \\dots &amp; cov(R_1, R_N) \\\\ cov(R_2, R_1) &amp; var(R_2) &amp; \\dots &amp; cov(R_2, R_N) \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ cov(R_N, R_1) &amp; cov(R_N, R_2) &amp; \\dots &amp; var(R_N) \\end{pmatrix} \\\\ &amp;= \\begin{pmatrix} \\sigma_1^2 &amp; \\sigma_{12} &amp; \\dots &amp; \\sigma_{1N} \\\\ \\sigma_{21} &amp; \\sigma_2^2 &amp; \\dots &amp; \\sigma_{2N} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\sigma_{N1} &amp; \\sigma_{N2} &amp; \\dots &amp; \\sigma_N^2 \\end{pmatrix}\\\\ &amp;= \\Sigma \\end{align} \\] which is a symmetric and positive definite matrix given that no perfect multicollinearity exists an no random variable is a constant. Based on these two formulations, we can now calculate the expected portfolio return and variance. The expected portfolio return is calculated as the matrix product of the weights and the expected returns : \\[ \\mu_P = E[\\textbf{x}&#39;\\textbf{R}] = \\textbf{x}&#39;\\mu = \\begin{pmatrix} x_1 &amp; \\dots &amp; x_N \\end{pmatrix} \\cdot \\begin{pmatrix} \\mu_1 \\\\ \\vdots \\\\ \\mu_N \\end{pmatrix} = x_1\\mu_1 + \\dots + x_N\\mu_N \\] Also, we have shown that the variance of a linear combination of random vectors \\(var(\\textbf{x}&#39;\\textbf{R})\\) can be written as \\(\\textbf{x}&#39;\\Sigma\\textbf{x}\\). \\[ \\begin{align} var(\\textbf{x}&#39;\\textbf{R}) = \\textbf{x}&#39;\\Sigma\\textbf{x} &amp;= \\begin{pmatrix} x_1 &amp; \\dots &amp; x_N \\end{pmatrix} \\cdot \\begin{pmatrix} \\sigma_1^2 &amp; \\sigma_{12} &amp; \\dots &amp; \\sigma_{1N} \\\\ \\sigma_{21} &amp; \\sigma_2^2 &amp; \\dots &amp; \\sigma_{2N} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\sigma_{N1} &amp; \\sigma_{N2} &amp; \\dots &amp; \\sigma_N^2 \\end{pmatrix} \\cdot \\begin{pmatrix} x_1 \\\\ \\vdots \\\\ x_N \\end{pmatrix} \\\\ &amp;= \\sum_{i=1}^Nx_i^2\\sigma_i^2 + 2\\sum_{i=1}^n\\sum_{i \\neq j}x_ix_j\\sigma_{ij} \\end{align} \\] Lastly, the covariance of the returns on portfolio x and portfolio y can be calculated according to the repetition on the covariance between linear combination of two random vectors. For instance, if we have portfolio x and y with different weights such that \\(x \\neq y\\), the transformation property allows us to calculate their portfolio covariance matrix as: \\[ \\begin{align} var(\\textbf{x}&#39;\\textbf{R}) = \\textbf{x}&#39;\\Sigma\\textbf{y} &amp;= \\begin{pmatrix} x_1 &amp; \\dots &amp; x_N \\end{pmatrix} \\cdot \\begin{pmatrix} \\sigma_1^2 &amp; \\sigma_{12} &amp; \\dots &amp; \\sigma_{1N} \\\\ \\sigma_{21} &amp; \\sigma_2^2 &amp; \\dots &amp; \\sigma_{2N} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\sigma_{N1} &amp; \\sigma_{N2} &amp; \\dots &amp; \\sigma_N^2 \\end{pmatrix} \\cdot \\begin{pmatrix} y_1 \\\\ \\vdots \\\\ y_N \\end{pmatrix} \\end{align} \\] 5.4.1.2 Application to the Swiss Market Index We can now start working with real-world portfolio applications. A1 &lt;- read.csv(&quot;~/Desktop/Master UZH/Data/A1_dataset_01_Ex_Session.txt&quot;, header = T, sep = &quot;\\t&quot;, dec = &#39;.&#39;) date = as.Date(A1[,1]) # Here, we first assign a date format to the date variable, otherwise the xts package cannot read it. # Other forms of transformation (as.POSIXct etc.) would certainly also work. A1ts &lt;- xts(x = A1[,-1], order.by = date) A1_return &lt;- Return.calculate(A1ts, method = &#39;discrete&#39;) # Calculation of Returns A1_returnts &lt;- xts(x = A1_return, order.by = date) # Only take some of the companies A6_ts_ret &lt;- A1_returnts[, c(&quot;ABB&quot;, &quot;Actelion&quot;, &quot;Adecco&quot;, &quot;Credit_Suisse_Group&quot;, &quot;Compagnie_Financiere_Richemont&quot;, &quot;Geberit&quot;, &quot;Givaudan&quot;, &quot;Julius_Baer_Group&quot;, &quot;LafargeHolcim&quot;, &quot;Nestle_PS&quot;, &quot;Novartis_N&quot;, &quot;Roche_Holding&quot;, &quot;SGS&quot;, &quot;The_Swatch_Group_I&quot;, &quot;Swiss_Re&quot;, &quot;Swisscom&quot;, &quot;Syngenta&quot;, &quot;Transocean&quot;, &quot;Zurich_Insurance_Group_N&quot;)][-1,][&#39;2013-01-31/2016-12-31&#39;] # Calculate the mean vector and covariance matrix mean_ret &lt;- colMeans(A6_ts_ret, na.rm = T) cvar_ret &lt;- cov(na.omit(A6_ts_ret)) # Calculate individual weights for (i in colnames(A6_ts_ret)[-19]){ weight = runif(10000, min=-1.5, max=1.5) names = paste0(i,&quot;_weight&quot;) if (i == &quot;ABB&quot;){ weight_final = weight names_final = names } else { weight_final = cbind(weight_final, weight) names_final = cbind(names_final, names) } } # Get the dataframe and matrix on the weights weight_df &lt;- as.data.frame(weight_final) colnames(weight_df) &lt;- names_final weight_df$sum &lt;- rowSums(weight_df) weight_df$Zurich_Insurance_Group_N &lt;- 1 - weight_df$sum weight_df$sum &lt;- NULL matrix_weights &lt;- as.matrix(weight_df) # Calculate the feasible expected returns and standard deviations feasible_pf_mu = matrix_weights%*%mean_ret feasible_pf_sd = apply(matrix_weights, 1, function(x) sqrt(t(x) %*% cvar_ret %*% x)) # Construct the feasible dataframe, consisting of 100 differently weighted risk and return combinations feasible_pf &lt;- as.data.frame(cbind(feasible_pf_mu, feasible_pf_sd)) colnames(feasible_pf) &lt;- c(&quot;Portfolio_Return&quot;, &quot;Portfolio_Risk&quot;) # Now, let&#39;s visualise the relationship feasible_pf %&gt;% ggplot(aes(x= Portfolio_Risk, y = Portfolio_Return)) + geom_point(color = &quot;grey&quot;) + geom_point(data = subset(feasible_pf, Portfolio_Risk &lt;= 0.12 &amp; Portfolio_Return &gt;= 0), color = &quot;darkorchid3&quot;, shape = 1, aes(x= Portfolio_Risk, y = Portfolio_Return)) + geom_point(data = subset(feasible_pf, Portfolio_Risk &gt; 0.12 &amp; Portfolio_Risk &lt;= 0.14 &amp; Portfolio_Return &gt;= 0.07), color = &quot;darkorchid3&quot;, shape = 1,aes(x= Portfolio_Risk, y = Portfolio_Return)) + geom_point(data = subset(feasible_pf, Portfolio_Risk &gt; 0.14 &amp; Portfolio_Risk &lt;= 0.16 &amp; Portfolio_Return &gt;= 0.09), color = &quot;darkorchid3&quot;, shape = 1,aes(x= Portfolio_Risk, y = Portfolio_Return)) + geom_point(data = subset(feasible_pf, Portfolio_Risk &gt; 0.16 &amp; Portfolio_Risk &lt;= 0.18 &amp; Portfolio_Return &gt;= 0.1), color = &quot;darkorchid3&quot;, shape = 1,aes(x= Portfolio_Risk, y = Portfolio_Return)) + geom_point(data = subset(feasible_pf, Portfolio_Risk &gt; 0.18 &amp; Portfolio_Risk &lt;= 0.20 &amp; Portfolio_Return &gt;= 0.11), color = &quot;darkorchid3&quot;,shape = 1, aes(x= Portfolio_Risk, y = Portfolio_Return)) + geom_point(data = subset(feasible_pf, Portfolio_Risk &gt; 0.2 &amp; Portfolio_Risk &lt;= 0.22 &amp; Portfolio_Return &gt;= 0.11), color = &quot;darkorchid3&quot;, shape = 1, aes(x= Portfolio_Risk, y = Portfolio_Return)) + ylab(expression(mu[p])) + xlab(expression(sigma[p])) + ggtitle(&quot;Porfolio Frontier of SMI PF for 10&#39;000 different weights&quot;) + labs(color=&#39;Factor Portfolios&#39;) + theme(plot.title= element_text(size=14, color=&quot;grey26&quot;, hjust=0.3,lineheight=2.4, margin=margin(15,0,15,0)), panel.background = element_rect(fill=&quot;#f7f7f7&quot;), panel.grid.major.y = element_line(size = 0.5, linetype = &quot;solid&quot;, color = &quot;grey&quot;), panel.grid.minor = element_blank(), panel.grid.major.x = element_blank(), plot.background = element_rect(fill=&quot;#f7f7f7&quot;, color = &quot;#f7f7f7&quot;), axis.title.y = element_text(color=&quot;grey26&quot;, size=12, margin=margin(0,10,0,10)), axis.title.x = element_text(color=&quot;grey26&quot;, size=12, margin=margin(10,0,10,0)), axis.line = element_line(color = &quot;grey&quot;)) When we introduce a large number (e.g. 10’000) different weightings for a portfolio consisting of a large fraction of Swiss bluechip companies, then we can see that the outlines are similar to the Markowitz Bullet we encountered in the two-asset case. That is, the outer boundaries of this combination appear to be similar to the set of feasible portfolios when only considering two assets, with the tip of the distribution representing the Minimum Variance portfolio. Further, recall that, in the case of a bullet shape, the set of efficient and feasible portfolios when considering only risky assets is represented by the outer boundary of portfolios at or above the minimum-variance portfolio, as they deliver the smallest possible risk for each unit of return. These portfolios are represented by the purple bounded dots. The results show that a portfolio consisting of N assets follows approximately the same intuition and distribution as we have observed in the two-asset case, which is handy for generalisation purposes. 5.4.2 The Minimum Variance Portfolio for N assets Next, we start to define efficient portfolios by first determining the constitution of weights for the minimum-variance portfolio. As we said, we can do this by following some matrix algebra properties that are also given in the repetition part. Note again that the definition of the minimum-variance portfolio is a constrained optimisation problem. That is, we want to find a weight vector such that the variance of the portfolio is minimised, conditional that all weights add up to 1. That is, we define again the variance-covariance matrix and matrix-multiply it with its respective, optimal weights, to obtain a scalar which quantifies the smallest possible standard deviation. We do this by defining the following problem: \\[ \\begin{align} \\min_{x} \\sigma_p^2 = \\textbf{x&#39;}\\Sigma\\textbf{x} &amp;&amp; \\text{ s.t. } \\textbf{x&#39;}\\textbf{1} = 1 \\end{align} \\] In order to solve this, we need to make use of the Lagrangian formula. The Lagrangian is a commonly used tool to define constrained optimisation problems. In essence, we define two functions in which the constraint is set equal to zero. That is, we define the following: \\[ L(\\textbf{x}, \\lambda) = \\textbf{x&#39;}\\Sigma\\textbf{x} + \\lambda(\\textbf{x&#39;}\\textbf{1} - 1) \\] We now take the FOC’s for both x and \\(\\lambda\\) and obtain: \\[ \\begin{align} \\frac{\\delta L} {\\delta \\textbf{x}} &amp;= 2\\Sigma\\textbf{x} + \\lambda1 \\\\ \\frac{\\delta L} {\\delta \\lambda} &amp;= \\textbf{x&#39;}\\textbf{1} - 1 \\end{align} \\] We can represent this is vector notation as: \\[ \\begin{pmatrix} 2\\Sigma &amp; \\textbf{1} \\\\ \\textbf{1&#39;} &amp; 0 \\end{pmatrix} \\begin{pmatrix} \\textbf{x} \\\\ \\lambda \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\] This is nothing else than backinduction of the usual matrix multiplication: \\[ \\begin{align} 2\\Sigma \\cdot \\textbf{x} + \\lambda\\cdot \\textbf{1} &amp;= 0\\\\ \\textbf{1&#39;}\\cdot \\textbf{x} + 0\\cdot \\lambda &amp;= 1 \\end{align} \\] If you don’t know anymore what we did here, please refer to Chapter 3.2.4 (System of linear equations). In general, this implies \\(\\textbf{A} \\cdot \\textbf{z} = \\textbf{b}\\), where: \\[ \\textbf{A} = \\begin{pmatrix} 2\\Sigma &amp; \\textbf{1} \\\\ \\textbf{1&#39;} &amp; 0 \\end{pmatrix}, \\textbf{z} = \\begin{pmatrix} \\textbf{x} \\\\ \\lambda \\end{pmatrix}, \\textbf{b} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\] Note that we are interested in the weights for this system of linear equations. That is, we want to find out the vector of \\(\\textbf{x}\\), as these minimise the global variance of our portfolio. In order to retrieve these weights, it is quite straight-forward to see that we need to take the inverse the matrix \\(\\textbf{A}\\) and multiply it with the matrix \\(\\textbf{b}\\). This is only feasible if the matrix \\(\\textbf{A}\\) is invertible. As long as we can determine the elements in \\(\\textbf{A}^{-1}\\), then we can solve for the values of x in the linear equations system of the vector \\(\\textbf{z}\\). In this case, the first N elements of the vector \\(\\textbf{z}\\) will be the variance minimising weights. Let’s do this for our portfolio: # Define the matrix A. It consists of: ## - the covariance matrix multiplied by two ## - a column right to the covariance matrix, consisting of 1&#39;s ## - a row right below the covariance matrix and the additional column, consisting of 1&#39;s and one zero (the zero is in the right-bottom of the resulting matrix) mat_A &lt;- rbind(cbind(2*cvar_ret, rep(1, dim(cvar_ret)[1])), c(rep(1, dim(cvar_ret)[1]), 0)) # Define the vector b as vector of zeros with dimension of the covariance matrix (19 in this case) and one 1 at the bottom vec_b &lt;- c(rep(0, dim(cvar_ret)[1]), 1) # Calculate the inverse and perform matrix multiplication to get the vector z z &lt;- solve(mat_A)%*%vec_b # Derive the first N elements of the vector to retrieve the actual values x_MV &lt;- z[1:dim(cvar_ret)[1]] # Check that the sum adds up to 1 sum(x_MV) ## [1] 1 Now, we got the appropriate weights per company of our portfolio. Let’s calculate the expected return as well as the standard deviation. # Calculate the expected return: mu_MV &lt;- x_MV %*% mean_ret sd_MV &lt;- sqrt(t(x_MV) %*% cvar_ret %*% x_MV) # Create the appropriate dataframe MV_PF &lt;- as.data.frame(cbind(mu_MV, sd_MV, t(x_MV))) colnames(MV_PF) &lt;- c(&quot;Mu_MV&quot;, &quot;Sd_MV&quot;,names_final, &quot;Zurich_Insurance_Group_Weight&quot;) as.data.frame(t(MV_PF)) ## V1 ## Mu_MV 0.02126306 ## Sd_MV 0.02321974 ## ABB_weight 0.09286278 ## Actelion_weight 0.15521845 ## Adecco_weight 0.46741548 ## Credit_Suisse_Group_weight -0.21374129 ## Compagnie_Financiere_Richemont_weight 0.07482529 ## Geberit_weight 0.18564346 ## Givaudan_weight -0.10473719 ## Julius_Baer_Group_weight 0.16461766 ## LafargeHolcim_weight -0.11800013 ## Nestle_PS_weight 0.38496885 ## Novartis_N_weight 0.32945099 ## Roche_Holding_weight 0.21882863 ## SGS_weight 0.14659400 ## The_Swatch_Group_I_weight -0.44283485 ## Swiss_Re_weight 0.01692448 ## Swisscom_weight 0.09185468 ## Syngenta_weight -0.01166684 ## Transocean_weight -0.02403974 ## Zurich_Insurance_Group_Weight -0.41418471 # Now, let&#39;s visualise the relationship feasible_pf %&gt;% ggplot(aes(x= Portfolio_Risk, y = Portfolio_Return)) + geom_point(color = &quot;grey&quot;) + # This is just to colour in the &quot;optimal PFs&quot; geom_point(data = subset(feasible_pf, Portfolio_Risk &lt;= 0.12 &amp; Portfolio_Return &gt;= 0.02), color = &quot;darkorchid3&quot;, shape = 1, aes(x= Portfolio_Risk, y = Portfolio_Return)) + geom_point(data = subset(feasible_pf, Portfolio_Risk &gt; 0.12 &amp; Portfolio_Risk &lt;= 0.14 &amp; Portfolio_Return &gt;= 0.07), color = &quot;darkorchid3&quot;, shape = 1,aes(x= Portfolio_Risk, y = Portfolio_Return)) + geom_point(data = subset(feasible_pf, Portfolio_Risk &gt; 0.14 &amp; Portfolio_Risk &lt;= 0.16 &amp; Portfolio_Return &gt;= 0.09), color = &quot;darkorchid3&quot;, shape = 1,aes(x= Portfolio_Risk, y = Portfolio_Return)) + geom_point(data = subset(feasible_pf, Portfolio_Risk &gt; 0.16 &amp; Portfolio_Risk &lt;= 0.18 &amp; Portfolio_Return &gt;= 0.1), color = &quot;darkorchid3&quot;, shape = 1,aes(x= Portfolio_Risk, y = Portfolio_Return)) + geom_point(data = subset(feasible_pf, Portfolio_Risk &gt; 0.18 &amp; Portfolio_Risk &lt;= 0.20 &amp; Portfolio_Return &gt;= 0.11), color = &quot;darkorchid3&quot;,shape = 1, aes(x= Portfolio_Risk, y = Portfolio_Return)) + geom_point(data = subset(feasible_pf, Portfolio_Risk &gt; 0.2 &amp; Portfolio_Risk &lt;= 0.22 &amp; Portfolio_Return &gt;= 0.11), color = &quot;darkorchid3&quot;, shape = 1, aes(x= Portfolio_Risk, y = Portfolio_Return)) + # Calculate and plot the Minimum Variance PF geom_point(color = &quot;goldenrod&quot;, aes(x= MV_PF$Sd_MV, y = MV_PF$Mu_MV), size = 3) + annotate(&#39;text&#39;,x = -0.01 ,y = -0.014, label = &quot;Minimum Variance PF&quot;, size = 3.5, color = &quot;goldenrod&quot;) + ylab(expression(mu[p])) + xlab(expression(sigma[p])) + ggtitle(&quot;Minimum Variance portfolio&quot;) + labs(color=&#39;Factor Portfolios&#39;) + xlim(-0.10, 0.75) + theme(plot.title= element_text(size=14, color=&quot;grey26&quot;, hjust=0.43,lineheight=2.4, margin=margin(15,0,15,0)), panel.background = element_rect(fill=&quot;#f7f7f7&quot;), panel.grid.major.y = element_line(size = 0.5, linetype = &quot;solid&quot;, color = &quot;grey&quot;), panel.grid.minor = element_blank(), panel.grid.major.x = element_blank(), plot.background = element_rect(fill=&quot;#f7f7f7&quot;, color = &quot;#f7f7f7&quot;), axis.title.y = element_text(color=&quot;grey26&quot;, size=12, margin=margin(0,10,0,10)), axis.title.x = element_text(color=&quot;grey26&quot;, size=12, margin=margin(10,0,10,0)), axis.line = element_line(color = &quot;grey&quot;)) 5.4.3 The Efficient Frontier for N assets 5.4.3.1 Determining Efficient Portfolios Now that we were able to determine the global minimum variance portfolio, we can go a step further and define the set of efficient portfolios under the assumption of N risky assets. In order to determine the set of risky assets that are efficient, we can follow the same strategy as we already did with 2 assets. In the case of 1 or 2 assets, the investment opportunity set is graph or a hyperbola (bullet), respectively. However, in the N assets case, the opportunity set is given by values that do not follow a clear trend. This is due to the fact that their risk properties largely depend solely on the covariance between the assets (remember the proof we made in the risk and return chapter?). In essence, when we try to plot a graph as we did in the 2 asset case, this is the result due to the covariances \\(\\sigma_{ij}\\). In the N asset case, we understand that deriving a clear structure that incorporates the position and behaviour of each constituent portfolio, as the covariance properties do not allow for any clearly visible trends. However, as we stated earlier, we do not need to care about every single relation within our portfolio set. Under the assumption that investors want to maximise return given a level of risk or, equivalently, minimise risk given a level of return, we can simplify the asset allocation to only incorporate the set of efficient and feasible portfolios, which are characterised by the purple dots. As it is depicted graphically, the efficient portfolios are the most outer portfolios at or above the minimum variance portfolio. S imilar to the two-asset case, these are Analogously, these are the portfolios that, for a given level of return, induce the lowest risk. In essence, we need to find the portfolios with the best possible risk-return trade-off. We can define these portfolios throughout a constrained optimisation problem in two distinct ways. First, we need to find the portfolios that, for a given level of risk, induce the highest return. This is the case when: \\[ \\begin{align} \\max_x \\mu_P = \\textbf{x&#39;}\\mu &amp;&amp; \\text{s.t. } \\sigma_P^2 = \\textbf{x&#39;}\\Sigma\\textbf{x} = \\sigma_{P, Spec}^2\\text{ and } \\textbf{x&#39;}1 = 1 \\end{align} \\] whereas \\(\\sigma_{P, Spec}\\) defines a given, specific level of risk. Second, we need to find the portfolios that, for a given level of return, induce the lowest risk. This is the case when: \\[ \\begin{align} \\min_x \\sigma_P^2 = \\textbf{x&#39;}\\Sigma\\textbf{x} &amp;&amp; \\text{s.t. } \\mu_{P}^2 = \\textbf{x&#39;}\\mu = \\mu_{P,Spec} \\text{ and } \\textbf{x&#39;}1 = 1 \\end{align} \\] whereas \\(\\mu_{P, Spec}\\) defines a given, specific level of return. In reality, we often solve for the second constrained optimisation problem, because investors can better define given levels of return than given levels of risk. Just as in the two asset case, the resulting efficient frontier will resemble one side of an hyperbola and is often called the “Markowitz bullet”. As such, we can solve the optimisation problem again with the Lagrangian function. That is, we get: \\[ L(x, \\lambda_1, \\lambda_2) = \\textbf{x&#39;}\\Sigma\\textbf{x} + \\lambda_1(\\textbf{x&#39;}\\mu - \\mu_{P,Spec}) + \\lambda_2(\\textbf{x&#39;}1 - 1) \\] We can solve this problem by taking three partial derivatives w.r.t. their individual components: \\[ \\begin{align} \\frac{\\delta L}{\\delta x} &amp;= 2\\Sigma\\textbf{x} + \\lambda_1\\mu + \\lambda_2\\textbf{1} = 0 \\\\ \\frac{\\delta L}{\\delta \\lambda_1} &amp;= \\textbf{x&#39;}\\mu - \\mu_{P,Spec} = 0 \\\\ \\frac{\\delta L}{\\delta \\lambda_2} &amp;= \\textbf{x&#39;}1 - 1 = 0 \\end{align} \\] Again, we can represent this system of linear equations as follows: $$ {} {} = _{} $$ Now, we can again take the inverse of \\(\\textbf{A}\\) and calculate the vector \\(\\textbf{z}\\) through matrix multiplication. Lastly, we take the first N elements from this vector to obtain the weights that minimise the risk of a portfolio for a given level of expected return. Let’s now try to replicate this idea for the underlying portfolio. Note that in order to be able to solve this portfolio, we need to specify a given level of return first which then can be used to minimise the risk associated. Let’s make this easy in our case. Let’s just say we want to have the return of the company Givaudan the portfolio under consideration. # We first define again the Matrix A mat_A_EF &lt;- rbind(cbind(2*cvar_ret, mean_ret, rep(1,dim(cvar_ret)[1])), cbind(t(mean_ret), 0, 0), cbind(t(rep(1,dim(cvar_ret)[1])), 0, 0)) # Then, we define the vector b ## Define the EW return mu_spec &lt;- mean_ret[7] ## Define the vector b vec_b_EF &lt;- c(rep(0, dim(cvar_ret)[1]), mu_spec, 1) # Now, we can solve for the respective weights z_EF &lt;- solve(mat_A_EF)%*%vec_b_EF # Then, we take the first N elements to get the respective weights x_EF &lt;- z_EF[1:dim(cvar_ret)[1]] # Check that the sum adds up to 1 sum(x_EF) ## [1] 1 Perfect. We now can take these weights again and calculate the respective risk for the given level of return. # Sanity Check: See if the matrix multiplication of the weights and the mean returns indeed gives us the average return again ifelse(round(x_EF%*%mean_ret, 4) == round(mu_spec, 4), &quot;The Sanity Check passes as we get that the expected return is equal to the specified return!&quot;, 0) ## [,1] ## [1,] &quot;The Sanity Check passes as we get that the expected return is equal to the specified return!&quot; # Now, let&#39;s calculate the risk sd_EF &lt;- sqrt(t(x_EF) %*% cvar_ret %*% x_EF) # Further, calculate the risk under the EW strategy EW_weights &lt;- rep(1/dim(cvar_ret)[1], dim(cvar_ret)[1]) sd_EW &lt;- sqrt(t(EW_weights) %*% cvar_ret %*% EW_weights) # Now, we can take these results and compare them with each other: EF_df &lt;- as.data.frame(cbind(round(mu_spec,4), round(sd_EF,4), round(sd_EW,4))) colnames(EF_df) &lt;- c(&quot;Return EW PF&quot;, &quot;Risk Optimised&quot;, &quot;Risk EW PF&quot;) EF_df ## Return EW PF Risk Optimised Risk EW PF ## Givaudan 0.0149 0.0239 0.0414 Great, as we can see, we can greatly improve the risk associated with a given level of return. # Now, let&#39;s visualise the relationship feasible_pf %&gt;% ggplot(aes(x= Portfolio_Risk, y = Portfolio_Return)) + geom_point(color = &quot;grey&quot;) + # This is just to colour in the &quot;optimal PFs&quot; geom_point(data = subset(feasible_pf, Portfolio_Risk &lt;= 0.12 &amp; Portfolio_Return &gt;= 0), color = &quot;darkorchid3&quot;, shape = 1, aes(x= Portfolio_Risk, y = Portfolio_Return)) + geom_point(data = subset(feasible_pf, Portfolio_Risk &gt; 0.12 &amp; Portfolio_Risk &lt;= 0.14 &amp; Portfolio_Return &gt;= 0.07), color = &quot;darkorchid3&quot;, shape = 1,aes(x= Portfolio_Risk, y = Portfolio_Return)) + geom_point(data = subset(feasible_pf, Portfolio_Risk &gt; 0.14 &amp; Portfolio_Risk &lt;= 0.16 &amp; Portfolio_Return &gt;= 0.09), color = &quot;darkorchid3&quot;, shape = 1,aes(x= Portfolio_Risk, y = Portfolio_Return)) + geom_point(data = subset(feasible_pf, Portfolio_Risk &gt; 0.16 &amp; Portfolio_Risk &lt;= 0.18 &amp; Portfolio_Return &gt;= 0.1), color = &quot;darkorchid3&quot;, shape = 1,aes(x= Portfolio_Risk, y = Portfolio_Return)) + geom_point(data = subset(feasible_pf, Portfolio_Risk &gt; 0.18 &amp; Portfolio_Risk &lt;= 0.20 &amp; Portfolio_Return &gt;= 0.11), color = &quot;darkorchid3&quot;,shape = 1, aes(x= Portfolio_Risk, y = Portfolio_Return)) + geom_point(data = subset(feasible_pf, Portfolio_Risk &gt; 0.2 &amp; Portfolio_Risk &lt;= 0.22 &amp; Portfolio_Return &gt;= 0.11), color = &quot;darkorchid3&quot;, shape = 1, aes(x= Portfolio_Risk, y = Portfolio_Return)) + # Calculate and plot the Minimum Variance PF geom_point(color = &quot;goldenrod&quot;, aes(x= MV_PF$Sd_MV, y = MV_PF$Mu_MV), size = 3) + # Calculate and plot the optimal PF for a given level of risk geom_point(data = subset(EF_df, `Return EW PF` == 0.0149 &amp; `Risk Optimised` == 0.0239), color = &quot;dodgerblue1&quot;, size = 3, aes(x=`Risk Optimised` , y= `Return EW PF`)) + geom_point(data = subset(EF_df, `Return EW PF` == 0.0149 &amp; `Risk EW PF` == 0.0414), color = &quot;dodgerblue3&quot;, size = 3, aes(x=`Risk EW PF` , y= `Return EW PF`)) + annotate(&#39;text&#39;,x = 0.0 ,y = 0.072, label = &quot;Minimum Variance PF&quot;, size = 3.5, color = &quot;goldenrod&quot;) + annotate(&#39;text&#39;,x = -0.01 ,y = 0.056, label = &quot;Efficient Frontier PF&quot;, size = 3.5, color = &quot;dodgerblue1&quot;) + annotate(&#39;text&#39;,x = -0.04 ,y = 0.040, label = &quot;Givaudan PF&quot;, size = 3.5, color = &quot;dodgerblue4&quot;) + ylab(expression(mu[p])) + xlab(expression(sigma[p])) + ggtitle(&quot;Minimum Variance PF, Efficient Frontier PF and Equally Weighted PF&quot;) + labs(color=&#39;Factor Portfolios&#39;) + xlim(-0.10, 0.75) + theme(plot.title= element_text(size=14, color=&quot;grey26&quot;, hjust=0.3,lineheight=2.4, margin=margin(15,0,15,0)), panel.background = element_rect(fill=&quot;#f7f7f7&quot;), panel.grid.major.y = element_line(size = 0.5, linetype = &quot;solid&quot;, color = &quot;grey&quot;), panel.grid.minor = element_blank(), panel.grid.major.x = element_blank(), plot.background = element_rect(fill=&quot;#f7f7f7&quot;, color = &quot;#f7f7f7&quot;), axis.title.y = element_text(color=&quot;grey26&quot;, size=12, margin=margin(0,10,0,10)), axis.title.x = element_text(color=&quot;grey26&quot;, size=12, margin=margin(10,0,10,0)), axis.line = element_line(color = &quot;grey&quot;)) 5.4.3.2 Determining the Efficient Frontier Although we now have a way to compare and compute the efficient portfolio for any given level of return (e.g. the risk minimising portfolio at each expected return level), we may find it still cumbersome to actually compute the entire efficient frontier. This is because it may simply be unfeasible to calculate the appropriate minimum level of risk for each potential return level, as it would require us to perform potentially hundreds of thousand of individual calculations that, aggregated, would form the Markowitz bullet of efficient and feasible portfolios we require. Luckily, Markowitz showed that there is a simpler way to compute the efficient frontier. Without going much into the details here, he showed that any minimum variance portfolio can be formed through a linear combination of any two minimum variance portfolios with different, specified returns. Given that the resulting return on the created portfolio exceeds the return of the global minimum variance portfolio, then the resulting portfolio is located on the efficient frontier. Consequently, to retrieve the Efficient frontier in a general case of N assets, we simply need to define two efficient portfolios and then express the remaining efficient portfolios as linear combinations of these two. Let’s quickly dive into the math of this result. To do so, we now assume that both \\(\\textbf{x}\\) and \\(\\textbf{y}\\) are two efficient portfolios with different specified returns. That is, for their respective level of return they display the smallest standard deviation. Mathematically, this implies: \\[ \\begin{align} \\min_x \\sigma_{P,x}^2 = \\textbf{x&#39;}\\Sigma\\textbf{x} &amp;&amp; \\text{s.t. } \\mu_{P,x}^2 = \\textbf{x&#39;}\\mu = \\mu_{P,Spec-x} \\text{ and } \\textbf{x&#39;}1 = 1\\\\ \\min_y \\sigma_{P,y}^2 = \\textbf{y&#39;}\\Sigma\\textbf{y} &amp;&amp; \\text{s.t. } \\mu_{P,y}^2 = \\textbf{y&#39;}\\mu = \\mu_{P,Spec-y} \\text{ and } \\textbf{y&#39;}1 = 1 \\end{align} \\] Further, we define \\(\\alpha\\) as a constant which puts individual weights on both efficient portfolios \\(\\textbf{x}\\) and \\(\\textbf{y}\\). Further, let \\(\\textbf{z}\\) define the linear combination portfolio consisting on \\(\\alpha\\) weighted portfolios of \\(\\textbf{x}\\) and \\(\\textbf{y}\\). That is, if we get: \\[ \\textbf{z} = \\alpha\\textbf{x} + (1- \\alpha)\\textbf{y} \\] then we can show that the following must hold: \\[ \\begin{align} \\mu_{P,z} &amp;= \\alpha\\mu_{P,x} + (1-\\alpha)\\mu_{P,y} \\\\ \\sigma_{P,z}^2 &amp;= \\alpha^2\\sigma_{P,x}^2 + (1-\\alpha)^2\\sigma_{P,y}^2 + 2\\alpha(1-\\alpha)\\sigma_{P,xy} \\end{align} \\] whereas \\(\\sigma_{P,x}^2 = \\textbf{x&#39;}\\Sigma\\textbf{x}\\), \\(\\sigma_{P,y}^2 = \\textbf{y&#39;}\\Sigma\\textbf{y}\\) and \\(\\sigma_{P,xy} = \\textbf{x&#39;}\\Sigma\\textbf{y}\\). To put this in words. What we do here is nothing else than defining the expected return for the minimum variance portfolios on x and y and calculate the minimum variances for both portfolios. This is what we did in the previous chapter. Further, we need to compute the covariance between these two portfolios, which is defined as \\(\\sigma_{P,xy}\\). Once we have all these, we can again vary the \\(\\alpha\\) levels and calculate the respective expected return and risk combinations that are efficient and feasible through this linear transformation step. Let’s show this quickly: # First we calculate the first Efficient Portfolio # Define the EW return mu_spec_x &lt;- mu_MV mu_spec_y &lt;- 0.125 # We first define again the Matrix A mat_A_EF &lt;- rbind(cbind(2*cvar_ret, mean_ret, rep(1,dim(cvar_ret)[1])), cbind(t(mean_ret), 0, 0), cbind(t(rep(1,dim(cvar_ret)[1])), 0, 0)) # Then, we define the vector b vec_b_EF_x &lt;- c(rep(0, dim(cvar_ret)[1]), mu_spec_x, 1) vec_b_EF_y &lt;- c(rep(0, dim(cvar_ret)[1]), mu_spec_y, 1) # Now, we can solve for the respective weights z_EF_x &lt;- solve(mat_A_EF)%*%vec_b_EF_x z_EF_y &lt;- solve(mat_A_EF)%*%vec_b_EF_y # Then, we take the first N elements to get the respective weights x_EF &lt;- z_EF_x[1:dim(cvar_ret)[1]] y_EF &lt;- z_EF_y[1:dim(cvar_ret)[1]] # Now, let&#39;s calculate the risk sd_EF_x &lt;- sqrt(t(x_EF) %*% cvar_ret %*% x_EF) sd_EF_y &lt;- sqrt(t(y_EF) %*% cvar_ret %*% y_EF) sd_EF_xy &lt;- t(x_EF) %*% cvar_ret %*% y_EF Now, we can compute the individual weights # Lastly, compute the weights and results a = seq(from=0, to=1, by=0.05) # Create the expected return as well as the variance and standard deviation of each portfolios mu_z = a*mu_spec_x + (1-a)*mu_spec_y sd_z = sqrt(a^2*sd_EF_x^2 + (1-a)^2*sd_EF_y^2 + 2*a*(1-a)*sd_EF_xy) # Create a dataframe consisting of different weights z = matrix(0, length(a), 19) for (i in 1:length(a)){ z[i, ] = a[i]*x_EF + (1-a[i])*y_EF } # Create a dataframe consisting of only the efficient linear transformation portfolios z_df &lt;- as.data.frame(cbind(mu_z, sd_z)) colnames(z_df) &lt;- c(&quot;Efficient_PF_Return&quot;, &quot;Efficient_PF_Risk&quot;) And finally we can calculate the Efficient Frontier # Now, let&#39;s visualise the relationship z_df %&gt;% ggplot(aes(x= Efficient_PF_Risk, y = Efficient_PF_Return), color = &quot;goldenrod&quot;) + geom_point() + geom_path() + ylab(expression(mu[p])) + xlab(expression(sigma[p])) + ggtitle(&quot;The Efficient Frontier for the Swiss Market Portfolio&quot;) + labs(color=&#39;Factor Portfolios&#39;) + theme(plot.title= element_text(size=14, color=&quot;grey26&quot;, hjust=0.3,lineheight=2.4, margin=margin(15,0,15,0)), panel.background = element_rect(fill=&quot;#f7f7f7&quot;), panel.grid.major.y = element_line(size = 0.5, linetype = &quot;solid&quot;, color = &quot;grey&quot;), panel.grid.minor = element_blank(), panel.grid.major.x = element_blank(), plot.background = element_rect(fill=&quot;#f7f7f7&quot;, color = &quot;#f7f7f7&quot;), axis.title.y = element_text(color=&quot;grey26&quot;, size=12, margin=margin(0,10,0,10)), axis.title.x = element_text(color=&quot;grey26&quot;, size=12, margin=margin(10,0,10,0)), axis.line = element_line(color = &quot;grey&quot;)) 5.4.4 The Efficient and Feasible Portfolio CAL of N risky Assets and a risk-free Asset We showed in the two-asset case that the efficient and feasible portfolios are those that have the highest Sharpe Ratio, which is indicated by the tangency portfolio. Based on this result, we derived the CML, which is the efficient and feasible CAL. Further, we showed that, depending on risk preferences, investors choose to invest more into risky or risk-free assets. Now we can show that the same results hold when generalising the case to incorporate N assets. #### Tangency Portfolio Calculation with N Assets Remember, to obtain the efficient and feasible portfolio CAL, we need first to obtain the portfolio which is tangent to the set of risky assets. In the N-dimensional case, this implies we need to find a line which “slightly touches” the outer boundary of the feasible set of risky portfolios. This will maximise the slope of the resulting, constrained CAL. Subsequently, since the slope of the CAL is defined as the Sharpe Ratio, we need to conditionally maximise this metric. The portfolio which we require to do so is thereby called the tangency portfolio. In essence, we need to: \\[ \\begin{align} \\max{x} = \\frac{\\mu^{tan}_P - r_f}{\\sigma_P^{tan}} = \\frac{\\textbf{x&#39;}\\mu_P - r_f}{(\\textbf{x&#39;}\\Sigma_P\\textbf{x})^{1/2}} &amp;&amp; \\text{ s.t. } \\textbf{x&#39;1}=1 \\end{align} \\] Let’s calculate the Lagrangian again for this constrained optimisation problem. This will be: \\[ L(\\textbf{x}, \\lambda_1) = \\frac{\\textbf{x&#39;}\\mu_P - r_f}{(\\textbf{x&#39;}\\Sigma_P\\textbf{x})^{1/2}} + \\lambda_1(\\textbf{x&#39;1} - 1) \\] The resulting FOCs from this optimisation are then: \\[ \\begin{align} \\frac{\\delta L}{\\delta \\textbf{x}} &amp;= \\mu_P(\\textbf{x&#39;}\\Sigma_P\\textbf{x})^{-1/2} - (\\textbf{x&#39;}\\mu_P - r_f)(\\textbf{x&#39;}\\Sigma_P\\textbf{x})^{-3/2}\\Sigma_P\\textbf{x} + \\textbf{1} = 0 \\\\ \\frac{\\delta L}{\\delta \\lambda_1} &amp;= \\textbf{x&#39;1} = 1 \\end{align} \\] Based on this, we can use the same, although definitively more complicated, approach to solving the system of linear equations. However, exactly calculating would excedd the aim of this course as this requires an in-depth understanding of linear algebra concepts. Consequently, we will simply show you that the formula to calculate the weight vector of a tangency portfolio which maximises the respective Sharpe Ratio is: \\[ \\textbf{x&#39;}_{tan} = \\frac{\\Sigma^{-1}(\\mu_P - r_f\\cdot \\textbf{1})}{\\textbf{1&#39;}\\Sigma^{-1}(\\mu_P - r_f\\cdot \\textbf{1})} \\] In general, we are always able to derive an exact solution for each of the properties we defined so far. However, given the computational complexity of large matrices, it is important to understand how to retrieve these properties in a non-calculus, but more algebraic fashion. However, if we now insert this formula to retrieve the optimal weights, we will get the following: # Calculate the TP mu_f = mean(A2_ts_1_year_monthly[&#39;2013-01-31/2016-12-31&#39;]) ## First the numerator and denominator numerator_T_N &lt;- inv(cvar_ret)%*%(mean_ret - mu_f*rep(1, length(mean_ret))) denominator_T_N &lt;- t(rep(1, length(mean_ret)))%*%inv(cvar_ret)%*%(mean_ret - mu_f*rep(1, length(mean_ret))) ## Then the weights weights_T_N &lt;- numerator_T_N[,1] / denominator_T_N ## Warning in numerator_T_N[, 1]/denominator_T_N: Recycling array of length 1 in vector-array arithmetic is deprecated. ## Use c() or as.vector() instead. ## Check if the weights sum up to 1 sum(weights_T_N) ## [1] 1 Great, this is the formula for calculating the weights for the tangency portfolio. We can now calculate the respective risk and return and visualise this in the plot. # Calculate the risk and return metrics mu_T_N &lt;- weights_T_N %*%mean_ret sd_T_N &lt;- sqrt(t(weights_T_N)%*%cvar_ret%*%weights_T_N) # Create another dataframe mu_sd_T_N_df &lt;- as.data.frame(cbind(mu_T_N, sd_T_N)) colnames(mu_sd_T_N_df) &lt;- c(&quot;mu_T_N&quot;, &quot;sd_T_N&quot;) # Now, let&#39;s visualise the relationship z_df %&gt;% ggplot(aes(x= Efficient_PF_Risk, y = Efficient_PF_Return), color = &quot;goldenrod&quot;) + geom_point() + geom_path() + geom_point(data = mu_sd_T_N_df, aes(x= sd_T_N, y = mu_T_N), color = &quot;goldenrod&quot;) + ylab(expression(mu[p])) + xlab(expression(sigma[p])) + ggtitle(&quot;The Tangency Portfolio on the EF for the Swiss Market Portfolio&quot;) + labs(color=&#39;Factor Portfolios&#39;) + theme(plot.title= element_text(size=14, color=&quot;grey26&quot;, hjust=0.3,lineheight=2.4, margin=margin(15,0,15,0)), panel.background = element_rect(fill=&quot;#f7f7f7&quot;), panel.grid.major.y = element_line(size = 0.5, linetype = &quot;solid&quot;, color = &quot;grey&quot;), panel.grid.minor = element_blank(), panel.grid.major.x = element_blank(), plot.background = element_rect(fill=&quot;#f7f7f7&quot;, color = &quot;#f7f7f7&quot;), axis.title.y = element_text(color=&quot;grey26&quot;, size=12, margin=margin(0,10,0,10)), axis.title.x = element_text(color=&quot;grey26&quot;, size=12, margin=margin(10,0,10,0)), axis.line = element_line(color = &quot;grey&quot;)) 5.4.4.1 Calculating of the CML through the Mutual Fund Theorem Recall the mutual fund theorem. It states that, if a risk-free asset is given, then the optimal allocation of portfolios lies on the efficient and feasible CAL, which is defined by having the slope of the Sharpe Ratio and touching the set of risky securities at the tangency portfolio. Consequently, as we did in the two-asset case, we can compute the expected return and standard deviation of these portfolios as follows: \\[ \\begin{align} \\mu_P^e &amp;= x_{tan}\\mu_P^{tan} + (1-x_{tan})r_f = r_f + x_{tan}(\\mu_P^{tan} - r_f)\\\\ \\sigma_P^e &amp;= x_{tan}\\sigma_P^{tan} \\end{align} \\] The optimal allocation of risky and risk-free assets in this case again depends on so-called investor preferences. If the investor is risk-averse, then she will choose portfolios with a low volatility, implying a larger investment share in the risk-free asset. If she is risk-seeking, then she will choose portfolios with a higher volatility, implying a larger investment share in the risky asset. However, all the weighted portfolios of the risk-free asset and the tangency portfolio are efficient and feasible. The CAL which combines all these portfolios is again called the Capital Market Line (CML), just now in a general case of N assets. The slope of the CML is the maximum Sharpe Ratio. Consequently, we can derive the expected return by combining both formulas as we did in the two-asset case. # Create the tangency portfolio ## Define the sequence x_tan = seq(from=-0.8, to=2.2, by=0.1) x_f = 1 - x_tan ## Calculate the metrics mu_tanf = x_tan*mu_T_N + x_f*mu_f var_tanf = x_tan^2*sd_T_N^2 + x_f^2*sigma_f^2 + 2*x_tan*x_f*sigma_Af sd_tanf = sqrt(var_tanf) # Only get the &quot;positive returns&quot; from the r_f on mu_tanf_real &lt;- mu_tanf[9:31] sd_tanf_real &lt;- sd_tanf[9:31] # Create a df cml_N &lt;- as.data.frame(cbind(mu_tanf_real, sd_tanf_real)) colnames(cml_N) &lt;- c(&quot;MU_CML&quot;, &quot;SD_CML&quot;) mu_sd_T_N_df ## mu_T_N sd_T_N ## 1 0.05273657 0.03641726 # Now, let&#39;s visualise the relationship z_df %&gt;% ggplot(aes(x= Efficient_PF_Risk, y = Efficient_PF_Return), color = &quot;goldenrod&quot;) + geom_point() + geom_path() + geom_point(data = cml_N, aes(x = SD_CML, y = MU_CML), color = &quot;aquamarine4&quot;) + geom_path(data = cml_N, aes(x = SD_CML, y = MU_CML), color = &quot;aquamarine4&quot;) + geom_point(data = mu_sd_T_N_df, aes(x= sd_T_N, y = mu_T_N), color = &quot;darkorange3&quot;) + geom_point(aes(x = 0, y = -0.0002970927), color = &quot;goldenrod&quot;) + annotate(&#39;text&#39;,x = 0.087 ,y = 0.10, label = paste(&#39;Efficient Frontier&#39;), size = 3, color = &quot;black&quot;) + annotate(&#39;text&#39;,x = 0.063 ,y = 0.11, label = paste(&#39;Capital Market Line&#39;), size = 3, color = &quot;aquamarine4&quot;) + annotate(&#39;text&#39;,x = 0.026 ,y = 0.055, label = paste(&#39;Tangency Portfolio&#39;), size = 3, color = &quot;darkorange3&quot;) + annotate(&#39;text&#39;,x = -0.005 ,y = 0.005, label = paste(&#39;Risk-Free&#39;), size = 3, color = &quot;goldenrod&quot;) + ylab(expression(mu[p])) + xlab(expression(sigma[p])) + ggtitle(&quot;The Tangency PF, CML and set of feasible PF for the Swiss Market&quot;) + labs(color=&#39;Factor Portfolios&#39;) + xlim(-0.01,0.1) + theme(plot.title= element_text(size=14, color=&quot;grey26&quot;, hjust=0.3,lineheight=2.4, margin=margin(15,0,15,0)), panel.background = element_rect(fill=&quot;#f7f7f7&quot;), panel.grid.major.y = element_line(size = 0.5, linetype = &quot;solid&quot;, color = &quot;grey&quot;), panel.grid.minor = element_blank(), panel.grid.major.x = element_blank(), plot.background = element_rect(fill=&quot;#f7f7f7&quot;, color = &quot;#f7f7f7&quot;), axis.title.y = element_text(color=&quot;grey26&quot;, size=12, margin=margin(0,10,0,10)), axis.title.x = element_text(color=&quot;grey26&quot;, size=12, margin=margin(10,0,10,0)), axis.line = element_line(color = &quot;grey&quot;)) Great, we now derived the idea behind the modern portfolio theory. Throughout, we showed the behaviour of the set of feasible portfolios, defined how to characterise the portfolio distribution solely by risk and return characteristics, calculated efficient and inefficient portfolios, derived the minimum variance portfolio and introduced risk-free assets. Therein, we then computed the efficient frontier and defined the Capital Allocation Line. Based on the slope of the CAL, we then derived a constrained optimisation problem to maximise the Sharpe Ratio such that it still touches the set of feasible and efficient portfolios, and defined the risk and return characteristics of this portfolio as tangency portfolio. Ultimately, we defined the Mutual Fund Theorem, stating that the set of efficient and feasible portfolios consisting of risky and risk-free assets is characterised by the graph connecting the risk-free rate with the tangency portfolio, and the slope of the tangency portfolio Sharpe Ratio. This is the entire idea behind Markowitz’s asset allocation theory. 5.5 The Capital Asset Pricing Model 5.5.1 Intuition of the CAPM We now have derived the fundamental notions of the modern portfolio theory. Especially, we were able to show an N-asset type result. This was not only important to show a generalisable result of the underlying relationship, but it also serves as basis to introduce the next fundament of modern portfolio theories. By introducing a portfolio consisting of N assets, all stemming from the same market in this case, we also indirectly introduced the concept of a market portfolio. A market portfolio is a portfolio of assets that are able to characterise the return behaviour of an underlying market. So far, we characterised the portfolio return on the CML as a function which relates investor preferences to the weights in risky assets. As we have derived, the weights can be characterised as the standard deviation of the efficient portfolio, relative to the standard deviation of the optimal portfolio. The higher this fraction, the higher is the risk of the efficient, investor specific portfolio compared to the tangent, or optimal, portfolio. This is the idea behind the CML, which combines the risk-free asset with n risky assets, or with one risky market portfolio. Now, we may be interested to understand what happens if we do not hold the market portfolio, but rather any arbitrary, but efficient, asset or portfolio. Especially, we may be eager to understand the connection between individual optimisation behaviour and the market behaviour when investors can draw on risky as well as risk-free assets. To do this, we can contrast the slope of the CML with a portfolio mix consisting of risky assets as well as the market portfolio. By the tangency property these two slopes must be equal. The intuition for this result is the following. We state that all investors are mean-variance optimisers, then they will all hold a combination of the risk-free asset and the tangency portfolio (Two-Fund Separation Theorem). Consequently, all individuals hold a certain fraction of risky assets as that of the tangency portfolio. Now, in equilibrium, when we assume that the market clears and all risky assets must be held, the market portfolio (being a linear combination of the individual portfolios) has the same fraction of assets as the tangency portfolio. This is because if we require to hold all assets then, on average, we will invest 100% of the funds into risky assets. Remember that, when investing 100% into risky and efficient assets, we retrieve the tangency portfolio. As the tangency portfolio maximises expected return for a given level of risk and since the investors are mean-variance optimisers, in equilibrium both slopes must cross the tangency portfolio (as only this maximises the Sharpe Ratio for both cases). 5.5.2 Mathematical Derivation of the Security Market Line (SML) Based on this result, we can thus mathematically introduce the property which transforms the MV optimisation to the CAPM formula by solving for two constrained optimisation problems: \\[ \\begin{align} \\max_{x_{tan}}\\mu_P^e &amp;= x_{tan}\\mu_P^{tan} + (1-x_{tan})r_f = r_f + x_{tan}(\\mu_P^{tan} - r_f) &amp;&amp; \\text{ s.t. } \\sigma_P^e = x_{tan}\\sigma_P^{tan}\\\\ \\max_{x_{tan}} \\mu_P &amp;= x_{tan}\\mu_P^{i} + (1-x_{tan})\\mu_m = r_m + x_{tan}(\\mu_P^{i} - \\mu_m) &amp;&amp; \\text{ s.t. } \\sigma_P^ = \\sqrt{\\sigma_i^2x_{tan}^2 + (1-x_{tan})^2\\sigma_m^2 + 2x_{tan}(1-x_{tan})\\sigma_{im}} \\end{align} \\] By solving this optimisation problem, we substitute the fraction of variances as \\(x_{tan}\\) and assume that none of the wealth is invested in the risk-free asset on average, we obtain the following: \\[ \\begin{align} \\frac{\\delta \\mu_P^e}{\\delta x_{tan}} &amp;= \\frac{\\mu_P^{tan} - r_f}{\\sigma_P^{tan}} \\\\ \\frac{\\delta \\mu_P}{\\delta x_{tan}} &amp;= \\frac{\\mu_P^{i} - \\mu_m}{\\frac{\\sigma_{im} - \\sigma_m^2}{\\sigma_m}} \\end{align} \\] In a next step, we set both slopes equal to each other. \\[ \\begin{align} \\frac{\\mu_P^{tan} - r_f}{\\sigma_P^{tan}} &amp;= \\frac{\\mu_P^{i} - \\mu_m}{\\frac{\\sigma_{im} - \\sigma_m^2}{\\sigma_m}} \\\\ \\frac{\\mu_P^{tan} - r_f}{\\sigma_P^{tan}} &amp;= \\frac{(\\mu_P^{i} - \\mu_m)\\sigma_m}{\\sigma_{im} - \\sigma_m^2} \\\\ (\\mu_P^{tan} - r_f)(\\sigma_{im} - \\sigma_m^2) &amp;= \\sigma_P^{tan}((\\mu_P^{i} - \\mu_m)\\sigma_m) &amp;&amp; \\text{Market Clearing: } \\mu_P^{tan} = \\mu_m, \\sigma_P^{tan} = \\sigma_m \\\\ \\mu_m\\sigma_{im} -r_f\\sigma_{im} - \\mu_m\\sigma_m^2 + r_f\\sigma_m^2 &amp;= \\sigma_m^2\\mu_p^i - \\mu_m\\sigma_m^2 \\\\ \\sigma_{im}(\\mu_m - r_f) &amp;= \\sigma_m^2(\\mu_p^i - r_f) (\\sigma_P^{tan}) \\end{align} \\] Overall, this delivers the baseline result for Security Market Line (SML), which characterises the CAPM, is the following: \\[ \\mu_P^i = r_f + \\beta_i(\\mu_m - r_f) \\] whereas \\(\\beta_i = \\frac{\\sigma_{im}}{\\sigma_m^2}\\) is the relative risk, or volatility of any asset or portfolio compared to the risk of the underlying market. Note that, when we considered \\(\\beta\\) in earlier chapters, we did nothing else than define this coefficient as quantification of the movements of a dependent variable relative to the movements of any explanatory variable. Now, we specified this co-movement dependency by stating that the explanatory variable is the market portfolio return and the dependent variable is the individual security return. In essence, the resulting formula for the SML allows us to draw an equivalent model, in which we define the \\(\\beta\\) coefficient on the x-axis and the subsequent expected return \\(\\mu_i\\) on the y axis. Basing the definition on the tangency, or market, portfolio we understand that a \\(\\beta\\) coefficient of 1implies that the security expected return equals the market return. This is intuitive, when stating that the SML compensates for additional risk encountered relative to the market risk. 5.5.3 Similarity of the CML and SML In essence, the CML is the graph of the efficient frontier and the SML is the graph of the CAPM. The SML is derived from the CML. While the CML shows the rates of return for a specific portfolio, the SML represents the market’s risk and return at a given time, and shows the expected returns of individual assets. As is evident, there is a clear similarity with the CML formula. The main difference is the coefficient for the multiplicative term which defines the slope of the curve. In terms of the CML, the slope is characterised as \\(\\frac{\\sigma_P^e}{\\sigma_P}\\), and, in terms of the SML, as \\(\\frac{\\sigma_{im}}{\\sigma_m^2}\\). Based on linearity properties, we can square the coefficient of the CML to have a better comparability. In this case, we understand that the denominator is the same. Consequently, we understand that both lines set a certain risk relative to the underlying market risk (defined by the tangency portfolio in equilibrium). In the case of the CML, in which we only assess risky assets and a risk free asset, this is the co-variation of the risky assets with themselves, because the covariance term with the risk-free asset is zero, resulting in only the variance of the portfolio. In the case of the SML, in which we assess risky assets and the (risky) market portfolio, this is the co-variation of the risky assets with the market portfolio, because the covariance term is not zero. However, the intuition is the same, namely that an increase in the variation relative to the underlying market risk is associated with an increase in expected returns. Further, note that, by definition of the \\(\\beta\\) coefficient, we only consider systematic risk components in the case of the CAPM. This is intuitive, given that we consider the covariance of the asset with the market. Recall, we showed in earlier chapters that, as N approaches a large number, the portfolio risk is characterised entirely by its covariance structure. This is what we considered as diversification. Based on this diversification, we expect that investors already diversified any idiosyncratic risk away and thus are located on the efficient and feasible CAL, implying that, for any given level of return, they minimise the resulting risk. Consequently, the CAPM only considers the reimbursement for additional, systematic risk, relative to the underlying market risk. This “reimbursement” is quantified by the \\(\\beta\\) coefficient. Besides similarities, it also makes sense to point out the main differences between both graphs. Initially, the CML examines the expected returns on efficient portfolios and their total risk (measured by standard deviation). The SML examines the expected returns on individual assets and their systematic risk (measured by beta). If the relationship between expected return and beta is valid for any individual securities, it must also be valid for portfolios constructed with any of these securities. So, the SML is valid for both efficient portfolios and individual assets. Further, the slope of the CML is the market portfolio’s Sharpe ratio and the slope of the SML is the market risk premiums. We derived this result earlier by taking the first derivative w.r.t. the respective variables. Lastly, it is important to note that all properly priced securities and efficient portfolios lie on the SML. However, only efficient portfolios lie on the CML. 5.5.4 Diagnostic Tests of the CAPM We understand that, according to the CAPM, the expected return of any security is characterised as the risk-free asset and a scaled factor of the Market Risk Premium, where the scale is quantified by the \\(\\beta\\) coefficient. We need to test whether the assumptions of the relationship being characterised by this coefficient hold empirically. 5.5.5 Fama-MacBeth (1973) approach 5.5.5.1 Intuition and Computation of the FM Approach The most common test to assess the validity of any factor model, including the CAPM, is to use the Fama-MacBeth (1973) (=FM) two-step regression approach. It is quite straight-forward and one can show that its asymptotic properties hold when slightly adjusting the covariance matrix. Hence, the results from the approach can be quantified and put into the usual hypothesis framework. Throughout the FM approach, we conduct two distinct types of regressions. The first is a time-series regression and the second is a cross-sectional regression. Let’s assume that we have a portfolio consisting of N assets for a period of T months. In a first step, we run N time-series regressions of the individual security returns on the market risk. This is defined as: \\[ r_{i,t} - r_{ft} = \\alpha_i + \\beta_{i}(r_{mt} - r_{ft}) + \\epsilon_{it} \\] This will result in N \\(\\beta\\) coefficients. We call these Factor Loadings. We can also elaborate this step and take rolling regressions with a rolling window of W to obtain \\(N\\cdot (T - (W-1))\\) \\(\\beta\\) coefficients. This is because we start with a window W, implying that we need at least W observations to construct the first estimate. From this on, if we move the window by 1 unit W each time, and have T observations in total, we retrieve T-W coefficients for each security. If we have N securities, this is multiplied accordingly. As a small example, consider that we have 3 assets and a period of 20 months, with rolling window regression of 15 months. As such, the first \\(\\beta\\) coefficient is given in t = 15 (by taking regressions for t = 1-15). From then on, we take regressions for t=16 (t=2-16), t=17, t=18, t=19 and t=20. Overall, this gives us 6 coefficients for the entire window. Thus, T = 20, W = 15, and overall we get 20-(15-1) = 6 coefficients per stock. For three securities, we thus obtain \\(6\\cdot 3 = 18\\) \\(\\beta\\) coefficients. Now, we take these coefficients and run a second, cross-sectional regression. This involves the following procedure. At each observational date, we run the cross-section of excess returns on the \\(\\beta\\) estimates. This is a cross-sectional regression because we get the returns for each asset at time t and run it on the respective factor loadings, thereby comparing the differences between assets at a given date. This will result in so-called Factor Risk Premia, which we define as \\(\\hat{\\lambda}\\): \\[ r_{i,t} - r_{f,t} = \\lambda_{t,0} + \\lambda_{t,1}\\hat{\\beta_i} + u_{i,t} \\] In the case of rolling regressions, we would to the identical approach. But note that we now have the \\(\\beta\\) coefficients of rolling windows each representing a window W. As such, we would adjust the expected return to represent the average return of the respective window instead of the entire time period. Consequently, we would retrieve \\(N\\cdot (T - (W-1))\\) factor loadings, following the similar logic as above. Notice that there are two ways to define the expected returns. Either, the second stage regression now involves the actual returns. Otherwise, research also used average excess returns. Although there is no clear solution to what should be taken, it is common to use the actual, not excess, returns. To assess the statistical validity of the approach, consider the following approach. For each of the \\(\\lambda\\) factors, we have one estimate per time period, \\(\\hat{\\lambda}_{i,t}\\). For each of the factor loadings, we then take the time average to obtain \\(\\hat{\\lambda}_{i}\\). For instance, if we have only one average observation per time-series (because we take the entire time-series), then \\(\\hat{\\lambda}_{i,t} = \\hat{\\lambda}_{i}\\). For rolling window regressions, we take the average of all windows per factor loadings. Further, we take the standard error of each factor loading. Based on both estimates, we calculate the t-ratio as average of the expected value divided by its standard error. The test statistic is then \\[ t_{\\hat{\\lambda_i}} = \\sqrt{T}\\frac{\\hat\\lambda_i}{\\hat{\\sigma_i}} \\] which is asymptotically standard normal, or follows a t distribution with T − 1 DOF in finite samples. 5.5.5.2 Interpretation of the FM Approach Essentially, the CAPM says that stocks with higher betas are more risky and therefore should command higher average returns to compensate investors for that risk. This implies that, in the cross-section, which compares average returns of individual assets, securities with higher exposure towards \\(\\beta\\) should compensate for this by inducing a higher expected return. If the CAPM is valid, then we would expect the following: \\(\\lambda_0 = r_f\\) \\(\\lambda_1 = r_m - r_f\\) That is, to find support for the CAPM, we would expect to see the intercept estimate being close to the risk-free rate of interest and the slope being close to the market risk premium. To understand the reasoning for this, recall the drawing of the SML. Here, we state that the first derivative of the SML w.r.t. \\(\\beta\\) is the market risk premium, \\(r_m - r_f\\). Since the first derivative of a variable indicates its slope, we understand that the resulting coefficient of the second-pass regression should be equal to the market risk premium in order for the CAPM to hold empirically. As such, the two-stage regression is quite a useful instrument to assess the validity of an underlying asset pricing model, because it represents the factor of interest as explanatory variable (which is the variable on the x-axis) in the second-pass regression, which enables us to interpret an average behaviour of said variable, relative to an underlying, economic, theory (in our case, that the slope is equal to the market risk premium). Furthermore, we can test two additional hypotheses: The individual return and \\(\\beta\\) relationship is linear No other variables can explain the cross-sectional variation in returns We can test these implications by running the following, cross-sectional regression: \\[ r_{i,t} - r_{f,t} = \\lambda_{t,0} + \\lambda_{t,1}\\hat{\\beta_i} + \\lambda_{t,2}\\hat{\\beta_i^2} + \\lambda_{t,3}\\hat{\\sigma}_i^2 + v_{i,t} \\] whereas \\(\\hat{\\beta}_i^2\\) are the estimates of the squared beta for stock i and \\(\\hat{\\sigma_}i^2\\) the variance of the residual term of the first stage regression. Here: \\(\\beta_i^2\\) represents a non-linearity factor of a higher order (we could potentially increase the order to construct so-called “splines”) . \\(\\sigma_i^2\\) represents a measure of idiosyncratic risk for stock i (which we assume should be zero since only systematic risk is represented) Consequently, we would argue that: \\(\\lambda_2 = 0\\) \\(\\lambda_3 = 0\\) 5.5.5.3 Application of the Fama-MacBeth Approach Let’s now use the idea and create the FM approach for the BigFour Portfolio # Load Datasets A1 - A3: A1 &lt;- read.csv(&quot;~/Desktop/Master UZH/Data/A3_dataset_01_Ex_Session.txt&quot;, header = T, sep = &quot;\\t&quot;) A2 &lt;- read.csv(&quot;~/Desktop/Master UZH/Data/A3_dataset_02_Ex_Session.txt&quot;, header = T, sep = &quot;\\t&quot;) A3 &lt;- read.csv(&quot;~/Desktop/Master UZH/Data/A3_dataset_03_Ex_Session.txt&quot;, header = T, sep = &quot;\\t&quot;) # Again, we define a date to convert the data set into an xts object # First, note that we need to set the date into an appropriate format such that the time series object is able to read it A1$Date &lt;- format(as.Date(A1$Date, format = &quot;%d.%m.%Y&quot;), &quot;%Y-%m-%d&quot;) A1_date &lt;- as.Date(A1$Date) A1_ts &lt;- xts(x = A1[,-1], order.by = A1_date) # Then, we use the return calculate function to calculate the returns of each stock. Note that the method &quot;log&quot; defines the log returns calculation of the returns A1_ts_ret &lt;- Return.calculate(A1_ts, method = &quot;log&quot;) A1_ts_ret &lt;- A1_ts_ret[-1,] # We now perform the same steps for the market returns A2$Date &lt;- format(as.Date(A2$Date, format = &quot;%d.%m.%Y&quot;), &quot;%Y-%m-%d&quot;) A2_date &lt;- as.Date(A2$Date) A2_ts &lt;- xts(x = A2[,-1], order.by = A2_date) A2_ts_ret &lt;- Return.calculate(A2_ts, method = &quot;log&quot;) A2_ts_ret &lt;- A2_ts_ret[-1,] A2_ts_ret &lt;- A2_ts_ret[&#39;2012-07-31/2017-04-28&#39;] # Now, we perform the same steps for the risk-free asset, defined as the 1 year Swiss Bond Yield A3$Date &lt;- format(as.Date(A3$Date, format = &quot;%d.%m.%Y&quot;), &quot;%Y-%m-%d&quot;) A3_date &lt;- as.Date(A3$Date) A3_ts &lt;- xts(x = A3[,-1], order.by = A3_date) # Calculate the time series for the individual bond yields ## First, since the data is given in percent, we need to adjust by a factor of 100 A3_ts_yearly &lt;- A3_ts / 100 A3_ts_1_year_monthly &lt;- ((1 + A3_ts_yearly)^(1/12) - 1) A3_ts_1_year_monthly &lt;- log(1+A3_ts_1_year_monthly)[&#39;2012-07-31/2017-04-28&#39;] # Then, load the portfolio of returns bigfour_ret &lt;- A1_ts_ret[, c(&quot;Nestle_PS&quot;, &quot;Novartis_N&quot;, &quot;Roche_Holding&quot;, &quot;UBS_N&quot;)][&#39;2012-07-31/2017-04-28&#39;] Based on this dataframe, we can now conduct the analysis # Run the first stage regression for (i in colnames(bigfour_ret)[1:4]){ # Run the regression first_pass_reg &lt;- lm(bigfour_ret[&#39;2012-07-31/2017-04-28&#39;,i] - A3_ts_1_year_monthly[&#39;2012-07-31/2017-04-28&#39;] ~ A2_ts_ret[&#39;2012-07-31/2017-04-28&#39;] - A3_ts_1_year_monthly[&#39;2012-07-31/2017-04-28&#39;]) # Get the coefficient of interest beta &lt;- summary(first_pass_reg)$coefficients[2,1] comp_name &lt;- paste0(&quot;Beta_&quot;,i) # Create appropriate data frame if (i == &quot;Nestle_PS&quot;){ beta_final &lt;- beta comp_name_final &lt;- comp_name } else { beta_final &lt;- cbind(beta_final, beta) comp_name_final &lt;- cbind(comp_name_final, comp_name) } } # We now retrieve the Factor Loadings of each company. That is the Beta coefficient. # Based on the factor loadings, we now want to run a cross-sectional regression of the # asset returns on these loadings at each time period, or date. # In order to do so, we first need to transpose the list of all Betas such that we # obtain a &quot;long&quot; formatted data frame. betas &lt;- as.data.frame(beta_final) colnames(betas) &lt;- comp_name_final betas &lt;- t(betas) colnames(betas) &lt;- &quot;MKT_RF&quot; # We call the Beta factor here MKT_RF (Market Excess) # Based on this, we need to take the returns of the assets under consideration at each date # and then transpose this list as well to get the same dimensions. Accordingly, we will # obtain a data frame consisting of the factor loading for each company as well as the respective # asset return corresponding to the factor loading at each date. # Note the dimensions here. # We ran a time-series regression in the first-pass over 58 months (t=58) for 4 companies (N = 4). # This provides us with 4 factor loadings (4 betas). At each time period, we now need to run the # excess returns again on the factor loadings. As such, we need to create a data frame with # 58 + k columns (k = number of factors used) (=59) and 4 rows and then iterate again the lm function through # each column to get the cross-sectional returns at each date. ## As such, let&#39;s define a dataframe including both average return and beta average_returns &lt;- t(bigfour_ret[,1:4]) beta_ret &lt;- as.data.frame(cbind(betas, average_returns)) dim(beta_ret) ## [1] 4 59 Great, we now were able to construct the dataframe required. Just to give you an insight to what it looks like in order to strengthen your intuition: beta_ret ## MKT_RF 2012-07-31 2012-08-31 2012-09-28 2012-10-31 2012-11-30 2012-12-31 2013-01-31 2013-02-28 2013-03-29 ## Beta_Nestle_PS 0.7710810 0.06005235 -0.011725427 -0.0008428151 -0.003378382 0.02588871 -0.017464061 0.06966379 0.02473078 0.04697099 ## Beta_Novartis_N 1.0668244 0.07989728 -0.018494583 0.0228480941 -0.026409986 0.02292869 0.001742161 0.07621938 0.02626494 0.05798726 ## Beta_Roche_Holding 0.9609108 0.05759977 0.002881016 0.0108727824 0.019166314 0.01825777 0.008733680 0.09035722 0.06394872 0.02892100 ## Beta_UBS_N 1.2655581 -0.07125788 0.037200284 0.0696168965 0.198206367 0.03795255 -0.015989218 0.10058388 -0.06209116 -0.01906116 ## 2013-04-30 2013-05-31 2013-06-28 2013-07-31 2013-08-30 2013-09-30 2013-10-31 2013-11-29 2013-12-31 2014-01-31 ## Beta_Nestle_PS -0.03332408 -0.0454448623 -0.02392459 0.012033840 -0.02666825 0.03540193 0.03495502 0.01063032 -0.013688427 0.0076278020 ## Beta_Novartis_N 0.02344430 -0.0007243753 -0.02792246 -0.007479466 0.01859481 0.02402737 0.01357649 0.01688992 -0.006300336 0.0083916576 ## Beta_Roche_Holding 0.05029732 0.0284218495 -0.01729636 -0.030239885 0.01782268 0.04958999 0.02949113 0.00595358 -0.013947227 0.0004012036 ## Beta_UBS_N 0.13181170 0.0243987110 -0.05622514 0.125492325 -0.01158634 0.02628848 -0.05214714 -0.01607382 -0.021053409 0.0613196938 ## 2014-02-28 2014-03-31 2014-04-30 2014-05-30 2014-06-30 2014-07-31 2014-08-29 2014-09-30 2014-10-31 2014-11-28 ## Beta_Nestle_PS 0.01208474 -0.0007510327 0.020818596 0.03328817 -0.022311109 -0.0168811348 0.05332675 -0.01413451 0.002132955 0.03008272 ## Beta_Novartis_N 0.02272043 0.0208832109 0.017184825 0.05109668 0.000000000 -0.0093838662 0.03459270 0.09049649 -0.009473462 0.04595995 ## Beta_Roche_Holding 0.08493676 -0.0238639214 -0.026770241 0.02109378 0.003787883 0.0007558579 0.01126984 0.05593324 0.002117150 0.01989247 ## Beta_UBS_N 0.04775731 -0.0328604842 0.007637789 -0.02309064 -0.099937108 -0.0375748649 0.04918514 0.01207744 0.002398083 0.03818379 ## 2014-12-31 2015-01-30 2015-02-27 2015-03-31 2015-04-30 2015-05-29 2015-06-30 2015-07-31 2015-08-31 2015-09-30 ## Beta_Nestle_PS 0.005498296 -0.03487104 0.05455270 -0.01353201 -0.006835297 0.000000000 -0.07696104 0.08106782 -0.02629909 0.02698192 ## Beta_Novartis_N -0.012375730 -0.02411076 0.07786448 -0.01342995 0.005186734 -0.001553197 -0.04612532 0.08574452 -0.05844821 -0.05759332 ## Beta_Roche_Holding -0.069758294 -0.08220634 0.04021038 0.03530448 0.004837219 0.024927977 -0.05278074 0.06322564 -0.05562104 -0.02687302 ## Beta_UBS_N -0.015099009 -0.10477555 0.08468031 0.08959510 0.029049918 0.069628936 -0.01947627 0.11514607 -0.10511086 -0.10630399 ## 2015-10-30 2015-11-30 2015-12-31 2016-01-29 2016-02-29 2016-03-31 2016-04-29 2016-05-31 2016-06-30 2016-07-29 ## Beta_Nestle_PS 0.031578035 0.009871748 -0.023857990 0.00868119 -0.07022844 0.024657818 -0.004883163 0.02622649 0.02356218 0.033369141 ## Beta_Novartis_N 0.005020931 -0.021373142 -0.013161850 -0.10050800 -0.09130547 -0.027592837 0.048995103 0.07498581 0.01571865 0.001869742 ## Beta_Roche_Holding 0.043029673 0.026481871 0.003261463 -0.04627885 -0.02649416 -0.082282568 0.024208194 0.07313548 -0.01856920 -0.034157411 ## Beta_UBS_N 0.093744167 -0.001517835 -0.011713910 -0.14827657 -0.09269921 0.009730858 0.069208041 -0.07763597 -0.20045371 0.060203362 ## 2016-08-31 2016-09-30 2016-10-31 2016-11-30 2016-12-30 2017-01-31 2017-02-28 2017-03-31 2017-04-28 ## Beta_Nestle_PS 0.007692346 -0.022603481 -0.06475627 -0.047815030 0.06577131 -0.010320009 0.02661365 0.03441769 -0.002605865 ## Beta_Novartis_N -0.036782840 -0.013004084 -0.08178943 -0.002132955 0.05335522 -0.022518864 0.07892726 -0.05304025 0.029160449 ## Beta_Roche_Holding -0.031605339 0.004991691 -0.05764670 -0.003522681 0.02569268 0.001718213 0.04899452 0.04436292 0.017438923 ## Beta_UBS_N 0.063133040 -0.072162446 0.05657035 0.144100344 -0.01369884 0.001253133 -0.03180930 0.03555930 0.058751377 This is what we meant by “performing a cross-sectional regression of the asset returns on the MKT_RF factor (our beta factor loading) for each date”. In essence, we now create a regression model which just runs through each date column and performs the regression. Note that this functional form may appear a little tricky to solve at first, but, in essence, it follows the same principle as the for loop we are usually working with. We just include this form here because it is for (i in names(beta_ret)[2:59]){ # This is just the &quot;tidy&quot; version of regression. second_pass_output &lt;- lm(beta_ret[,i] ~ beta_ret[, &quot;MKT_RF&quot;]) %&gt;% summary() %&gt;% &#39;$&#39;(coef) %&gt;% data.frame() %&gt;% dplyr::select(Estimate) if (i == &quot;2012-07-31&quot;){ second_pass_output_final &lt;- second_pass_output } else{ second_pass_output_final &lt;- cbind(second_pass_output_final, second_pass_output) } } # Let&#39;s create a data frame lambdas &lt;- as.data.frame(t(second_pass_output_final)) colnames(lambdas) &lt;- c(&quot;Intercept&quot;, &quot;MKT_RF&quot;) rownames(lambdas) &lt;- names(beta_ret)[2:59] # Now we have the risk premia estimates for each factor at based on the cross-sectional # regression at each date t. Based on this, we now create a dataframe with the average estimates, # the standard deviation as well as the square root of the time-series observation amount. # We can now calculate the respective t statistics T_Period &lt;- length(names(beta_ret[,2:59])) average &lt;- colMeans(lambdas, na.rm = T) sd &lt;- colStdevs(lambdas, na.rm = T) se &lt;- sd / sqrt(T_Period) t_value &lt;- average/se # Create a final data frame second_pass &lt;- as.data.frame(cbind(average, se, t_value)) colnames(second_pass) &lt;- c(&quot;Risk Premia Estimate&quot;, &quot;Standard Error&quot;, &quot;T-Stat&quot;) round(second_pass, 4) ## Risk Premia Estimate Standard Error T-Stat ## Intercept 0.0032 0.0191 0.1675 ## MKT_RF 0.0035 0.0203 0.1734 For completeness, we also have the way of doing this in an lapply setting again. ## Perform the second pass regression ### We define first that we only have one factor in the regression running factors = 1 ### Here, we run the second-pass regression. That is, we &quot;apply&quot; the linear model (lm) ### formula on each date of the cross-sectional asset returns. That is, we have the paste ### function in which we say: &quot;Apply the pasted function f as an lm() based on the data provided. models &lt;- lapply(paste(&quot;`&quot;,names(beta_ret[,2:59]) , &quot;`&quot;, &#39; ~ MKT_RF&#39;, sep = &quot;&quot;), function(f){ lm(as.formula(f), data = beta_ret) %&gt;% # Call lm(.) summary() %&gt;% # Gather the output &quot;$&quot;(coef) %&gt;% # Keep only the coefs data.frame() %&gt;% # Convert to dataframe dplyr::select(Estimate)} # Keep only estimates ) ### This is then the same coefficient retrieval approach as before. lambdas &lt;- as.data.frame(matrix(unlist(models), ncol = factors + 1, byrow = T)) colnames(lambdas) &lt;- c(&quot;Constant&quot;, &quot;MKT_RF&quot;) # We can now calculate the respective t statistics T_Period &lt;- length(names(beta_ret[,2:59])) average &lt;- colMeans(lambdas, na.rm = T) sd &lt;- colStdevs(lambdas, na.rm = T) se &lt;- sd / sqrt(T_Period) t_value &lt;- average/se # Create a final data frame second_pass &lt;- as.data.frame(cbind(average, se, t_value)) colnames(second_pass) &lt;- c(&quot;Risk Premia Estimate&quot;, &quot;Standard Error&quot;, &quot;T-Stat&quot;) round(second_pass, 4) ## Risk Premia Estimate Standard Error T-Stat ## Constant 0.0032 0.0191 0.1675 ## MKT_RF 0.0035 0.0203 0.1734 As we can see, both approaches are identical. In accordance to the theoretical notions made above, we would expect the following here: \\(\\lambda_0 = -0.3296\\) \\(\\lambda_1 = 0.3360\\) Whereas \\(\\lambda_0\\) is the average risk-free rate and \\(\\lambda_1\\) is the average market risk premium. As is applicable from the regression output, neither hypothesis holds. Furthermore, we see that the coefficient on the factor loadings is statistically indistinguishable from zero at conventional levels. This implies that the average effect of the beta coefficients on the individual excess returns is not equal to the slope of the Security Market Line. Especially, we can observe that the slope, compared with the SML, is too flat. This is a result which we have discussed in the lecture already. Let’s visually derive what we mean by this (we don’t need to talk about the statistical invalidity of the approach with only four observations. But this is just to show you the idea behind what we did): ## `geom_smooth()` using formula &#39;y ~ x&#39; ## `geom_smooth()` using formula &#39;y ~ x&#39; This is the visual interpretation of the results. At first, we can see that the intercept as well as the slope of the SML are incorrect. Further, the slope is very imprecise, telling by the 95% Confidence Interval which is marked in gray. The CAPM implied Security Market Line is given in orange. This would be the theoretically correct SML. The purple dots would thus induce the theoretically accurate Mean Excess Returns for any given level of \\(\\beta\\), such that the slope would constitute the market risk premium. However, we see that the dots are off to quite some extent, thereby invalidating the hypothesis that the CAPM is indeed a valid measure to quantify the average variation of security returns in the cross section. "],["factor-models.html", "Chapter 6 Factor Models 6.1 The idea behind factor models 6.2 The quest of detecting anomalies 6.3 The Fama-French Three Factor Model 6.4 Testing for factor validity 6.5 Create your own factors: A step-by-step example", " Chapter 6 Factor Models 6.1 The idea behind factor models In the previous chapter, we looked at the Markowitz Mean-Variance optimisation and introduced its composition. Therein, we derived the model under the consideration of the mean, variance as well as covariance matrix between the individual, observed assets. In the case of n assets, we thus stated that we require n means, n variances and n(n-1)/2 covariances, resulting in a total of 2n + n(n-1)/2 parameters to describe the model. For instance, if we have 1000 assets, then we would require 501’500 parameters to describe the model inherently. Furthermore, we can show that attempting to estimate the model parameters with a sufficient precision is nearly unfeasible using historical price data over time. This is due to the fact that the precision of any variable within an iid setting depends on the square root of the number of observations. Consequently, if the number of observations is insufficiently large, confidence intervals to determine the accuracy of any expected variable value would be deemed redundant. Given the large number of parameters required to describe the model and the number of observations required to estimate the model parameters with sufficient precision, we understand that the Markowitz model is deemed to be a very data intensive model. As a consequence, we need to find simpler models which are less data intensive but can still capture a sufficient amount of the underlying, true asset variation. Based on this notion, the term Factor Models was primed. These models assume that the correlation between any two assets is explained by systematic factors. That is, the underlying return structure of multiple assets depends on common components which can be represented by quantifiable proxies. Using this assumption, one can restrict attention to only K (non-diversifiable) factors. Factor models have the advantage that they drastically reduce the number of input variables. Further, they allow to estimate systematic risk components by analysing expected return structures. However, they are purely statistical models and rely on past data. Further, they all assume stationarity. The aim of factor models is to understand the drivers of asset prices. Broadly speaking, the main concept behind factor investing is that the financial performance of firms depends on distinct factors, whether they are latent or macroeconomic, or related to intrinsic, firm-specific characteristics. Cochrane (2011) states that the first essential question is which characteristics really provide independent information about average returns. Understanding whether the exposure of assets towards common variable(s) can be used to help explain the cross-sectional variation in returns is therein the main emphasis of factor investing. As we have covered in the lecture, factor models are natural extensions of the Arbitrage Pricing Theory model (APT) introduced by Ross (1976), who assumes that security returns return can be modeled as a linear combination of underlying factors \\(f_k\\) and that investors operate in functioning security markets which do not allow for the persistence of arbitrage opportunities: \\[ r_{i,t} = \\alpha_i + \\sum_{i=1}^K \\beta_{i,k}f_{t,k} + \\epsilon_{i,t} \\] Here, the usual IID setting econometric assumptions hold, implying that \\(cov(\\epsilon_{i,t},\\epsilon_{j,t}) = 0\\) and \\(cov(\\epsilon_{i},f_{i}) = 0\\). A quasi factor-based model is The CAPM. The CAPM is a commonly used model to assess asset returns. Previously, we introduced both the theoretical intuition as well as the practical implementation of the model. We derived how the exposure towards market movements influence stock returns and showed how to compute the underlying framework. Further, we derived options to test the model’s validity and showed that the CAPM does not hold empirically. Consequently, we were able to present that the theoretical foundations of the CAPM are not sufficient to effectively explain the variation in asset returns. This implies that other factors are needed to help explain the remaining variation in asset returns unexplained by the market. To put it in other words, the existence of factors that can explain asset returns further contradicts the validity of the CAPM, which assumes that the variation solely depends on the market portfolio. Consequently, factors are also regarded as anomalies. A quasi factor-based model is The CAPM. The CAPM is a commonly used model to assess asset returns. Previously, we introduced both the theoretical intuition as well as the practical implementation of the model. We derived how the exposure towards market movements influence stock returns and showed how to compute the underlying framework. Further, we derived options to test the model’s validity and showed that the CAPM does not hold empirically. Consequently, we were able to present that the theoretical foundations of the CAPM are not sufficient to effectively explain the variation in asset returns. This implies that other factors are needed to help explain the remaining variation in asset returns unexplained by the market. To put it in other words, the existence of factors that can explain asset returns further contradicts the validity of the CAPM, which assumes that the variation solely depends on the market portfolio. Consequently, factors are also regarded as anomalies. Importantly, each factor portfolio is a tracking portfolio. That is, the returns on such a portfolio are tracked by the evolution of one particular source of risk but are uncorrelated with other sources of risk. 6.2 The quest of detecting anomalies How can we thus find such anomalies? As we already mentioned, there is no clear theoretical foundation to the use of certain factors. Mostly, researchers or industry experts consider market movements and attempt to pin down these movements based on correlating characteristics (either micro- or macro-oriented) of the firms under consideration. Usually then, based on these observations, theoretical arguments which suit the given movements are constructed around the anomalies. For instance, Chen Roll and Ross introduced one of the most fundamental multifactor models in 1986 by incorporating macroeconomic variables such as Growth in industrial productiion, inflation expected changes or corporate rated bonds and found a significant effect in the cross-section of returns. Based on their research, Fama and French (1993) created their famous Size and Value portfolio strategies on the observations that small size (measured by market capitalisation) as well as value (measured by high book-to-market ratios) securities outperform their opposite counterparts. Further notable examples include the Momentum strategy (by Jegadeesh and Titman (1993) and Carhart (1997)), Profitability (by Fama and French (2015) and Bouchaud et al. (2019)), Investment (by Fama and French (2015) as well as Hou, Xue, and Zhang (2015)) as well as low-risk (by Frazzini and Pedersen (2014)) or Liquidity factor (by Acharya and Pedersen (2005)).While these factors (i.e., long-short portfolios) exhibit time-varying risk premia and are magnified by corporate news and announcements, it is well-documented (and accepted) that they deliver positive returns over long horizons. In essence, factors that are shown to hold empirically are manifold. For instance, Chen and Zimmermann (2019) attempt to replicate the 300 most commonly cited factors introduced during the last 40 years and publish the final factors on their website called . Although their work greatly facilitates an improved accessibility of factor investment opportunities, they also show the inherent challenges associated with factor investing. These include, among others: The limited possibility to replicate the factor exposure The dependence of the variational properties on data-specific idiosyncracies The likelihood of p-hacking strategies The time-wise dependence on factor exposure The destruction of potentially valid factor premia may be due to herding The fading of anomalies due to the publication and subsequent reduction of arbitrage opportunities (through re-investment of multiple market participants) Consequently, although the pool of potentially valid factors is quite large, several of these factors suffer from one or multiple of the aforementioned issues, thereby rendering their empirical validity at least partially questionable. As such, although a multitude of potential characteristics are able to explain asset returns both time-series wise and cross-sectionally, the large-scale use of high-dimensional factor models must be addressed critically. 6.3 The Fama-French Three Factor Model The perhaps most famous factor model is the Fama-French three-factor model (1993). It was based on observations by the researchers which found that small size (measured by market capitalisation) as well as value (measured by high book-to-market ratios) securities outperform their opposite counterparts. The theoretical reasoning for this were mostly pinned down to risk-based anomalies. For instance, it was argued that small companies outperform their larger counterparts because they are exposed towards a higher liquidation, operational as well as credit based risk, implying that they need to compensate for said risk throughout a higher return structure. Furthermore, the value factor was usually explained through perceptions of future performance based on financing activities, which might also constitute as financial distress assessments. Fama and French do not only highlight the importance of size and value, but they also develop a method to generate factor portfolios. Their main strategy is the following. They construct a portfolio of large firms and one of small firms. As break point they use the median size of the firms listed on NYSE to classify all stocks traded on NYSE, Amex and Nasdaq. These portfolios are value weighted. They then construct a zero net investment size factor portfolio by going long the small and going short the big stock portfolio. Further, they construct a Book-to-Market (B2M) exposure by sorting the stocks into low (bottom 30%), medium (middle 40%) and high (top 30%) groups based on B2M. Based on this, they construct six portfolios based on the intersection of size and B/M sorts: S/L: Small Stocks with Low B2M S/M: Small Stocks with Medium B2M S/H: Small Stocks with High B2M B/L: Big Stocks with Low B2M B/M: Big Stocks with Medium B2M B/H: Big Stocks with High B2M The returns of the zero-net-investment factors SMB (Small minus Big) and HML (High minus Low) are created from these portfolios \\[ \\begin{align} R_{SMB} &amp;= 1/3(R_{S/L} + R_{S/M} + R_{S/H}) - 1/3(R_{B/L} + R_{B/M} + R_{B/H}) \\\\ R_{HML} &amp;= 1/2(R_{S/H} + R_{B/H}) - 1/2(R_{S/L} + R_{B/L}) \\end{align} \\] Based on these factors, the factor betas of the stocks are calculated in a First Pass regression: \\[ E(R_i) - r_f = \\alpha_i + \\beta_i(E(R_M) - r_f) + \\gamma_iE[R_{SMB}] + \\delta_iE[R_{HML}] \\] The method used of Fama-French to construct the factors is the most widely adopted approach to create factors. Most of the subsequent factors were created using said approach of sorting stocks into either \\(2 \\times 3\\) or \\(2 \\times 2\\) portfolios. For instance, to construct the Momentum Factor by Jegadeesh and Titman (1996) as well as Carhart (1997), you can use the same approach as for the HML factor, but just double-sort it based on size and the previous Up/Down movements. 6.4 Testing for factor validity As we discovered, factor investing describes an investment strategy in which quantifiable corporate or macroeconomic data is considered. Therein, the variation in security returns is poised to depend collectively on these characteristics. In order to test for the validity of the underlying characteristic in determining asset returns, we thus need to set the asset returns in a relation to the factor under consideration. The industry surrounding this investment style poses several methods to test said validity, both within time-series as well as cross-sectional settings. Throughout the subsequent chapter, we will go over the most common techniques to test for factor validity. Throughout, we will cover the theoretical underpinnings, the actual testing strategy as well as the intuition behind each approach. Further, we will illustrate each testing strategy by using code-based examples on the Size Factor first introduced by Fama and French (1993). 6.4.1 Portfolio Sorts This is the simplest and most widely applied form of factor testing. The idea is straight-forward: We sort portfolios based on the risk factor under consideration into percentiles (e.g. deciles, quintiles). Then, we look at the return and variance structure of each portfolio and compare their structure throughout the individual percentiles to observe whether we see significant differences between them. In essence, on each date t, we perform the following steps: Rank firms according to a criterion (e.g. Market Capitalisation) Sort the firms into portfolios based on the criterion (e.g 10 portfolios for decile portfolios) Calculate the weights (either equally or value-weighted) Calculate the portfolio returns at t+1 The resulting outcome is a time series of portfolio returns for each portfolio j, \\(r_t^j\\). Then, we identify an anomaly in the following way: Construct a Long-Short portfolio strategy by buying the highest and selling the lowest portfolio percentile group Conduct a t-test of the long-short strategy In essence, what we want to observe is the following: A monotonous (possibly significant) change (either increase or decrease) in average returns throughout the portfolio sorts A significant t-value for the long-short strategy Based on the literature, if we can observe both, then we can strengthen our assumption that the factor materially affects security returns. The reasoning is the following. We hypothesise that asset returns depend on a common factor. As such, the larger the exposure of a security towards said factor is, the stronger the returns should be affected by the factor. Consequently, the return of assets in higher portfolios should behave differently to that in lower portfolios. Further, if we assume that the factor under consideration influences the return structure of all assets, then we would expect that the effect grows continuously when moving from one portfolio to the other, inducing a monotonous effect throughout our assets. First, let’s calculate quintile portfolios based on the characteristic # Load the datasets Prices_Adj &lt;- read.csv(&quot;~/Desktop/Master UZH/Data/A4_dataset_01_Ex_Session.txt&quot;, header = T, sep = &quot;\\t&quot;) Prices_Unadj &lt;- read.csv(&quot;~/Desktop/Master UZH/Data/A4_dataset_04_Ex_Session.txt&quot;, header = T, sep = &quot;\\t&quot;) Shares &lt;- read.csv(&quot;~/Desktop/Master UZH/Data/A4_dataset_03_Ex_Session.txt&quot;, header = T, sep = &quot;\\t&quot;) # Create Time-Series objects Prices_Adj_ts &lt;- xts(x = Prices_Adj[,-1], order.by = as.Date(dmy(Prices_Adj[,1]))) Prices_Unadj_ts &lt;- xts(x = Prices_Unadj[,-1], order.by = as.Date(dmy(Prices_Unadj[,1]))) Shares_ts &lt;- xts(x = Shares[,-1], order.by = as.Date(dmy(Shares[,1]))) # Calculate the Returns Returns_Adj_ts &lt;- Return.calculate(Prices_Adj_ts, method = &quot;discrete&quot;) # Create the market cap Time-Series Market_Cap_ts &lt;- Shares_ts * Prices_Unadj_ts # Perform the decile portfolio sorts ## Take the quintile values quintiles &lt;- c(0.2, 0.4, 0.6, 0.8) ## Create variables indicating the respective 20&#39;th, 40&#39;th, 60&#39;th, 80&#39;th quintile for each month for (i in quintiles){ assign(paste0(&quot;Market_Cap_Cutoff_&quot;, i, &quot;_ts&quot;), matrixStats::rowQuantiles(as.matrix(Market_Cap_ts), probs = i, na.rm = T)) } # Create the portfolio returns for each quintile for (i in names(Market_Cap_ts)){ # First create the market capitalisation of each quintile Market_Cap_Q1 &lt;- ifelse(Market_Cap_ts[,i] &lt;= Market_Cap_Cutoff_0.2_ts, 1, NA) Market_Cap_Q2 &lt;- ifelse((Market_Cap_ts[,i] &gt; Market_Cap_Cutoff_0.2_ts) &amp; (Market_Cap_ts[,i] &lt;= Market_Cap_Cutoff_0.4_ts), 1, NA) Market_Cap_Q3 &lt;- ifelse((Market_Cap_ts[,i] &gt; Market_Cap_Cutoff_0.4_ts) &amp; (Market_Cap_ts[,i] &lt;= Market_Cap_Cutoff_0.6_ts), 1, NA) Market_Cap_Q4 &lt;- ifelse((Market_Cap_ts[,i] &gt; Market_Cap_Cutoff_0.6_ts) &amp; (Market_Cap_ts[,i] &lt;= Market_Cap_Cutoff_0.8_ts), 1, NA) Market_Cap_Q5 &lt;- ifelse(Market_Cap_ts[,i] &gt; Market_Cap_Cutoff_0.8_ts, 1, NA) # Then multiply the Market Cap quintile indicators with the secutity returns Return_Market_Cap_Q1 &lt;- stats::lag(Market_Cap_Q1, 1) * Returns_Adj_ts[&#39;1991-01-01/2019-12-01&#39;, i] Return_Market_Cap_Q2 &lt;- stats::lag(Market_Cap_Q2, 1) * Returns_Adj_ts[&#39;1991-01-01/2019-12-01&#39;, i] Return_Market_Cap_Q3 &lt;- stats::lag(Market_Cap_Q3, 1) * Returns_Adj_ts[&#39;1991-01-01/2019-12-01&#39;, i] Return_Market_Cap_Q4 &lt;- stats::lag(Market_Cap_Q4, 1) * Returns_Adj_ts[&#39;1991-01-01/2019-12-01&#39;, i] Return_Market_Cap_Q5 &lt;- stats::lag(Market_Cap_Q5, 1) * Returns_Adj_ts[&#39;1991-01-01/2019-12-01&#39;, i] if (i == &quot;NESN&quot;){ Return_Market_Cap_Q1_final &lt;- Return_Market_Cap_Q1 Return_Market_Cap_Q2_final &lt;- Return_Market_Cap_Q2 Return_Market_Cap_Q3_final &lt;- Return_Market_Cap_Q3 Return_Market_Cap_Q4_final &lt;- Return_Market_Cap_Q4 Return_Market_Cap_Q5_final &lt;- Return_Market_Cap_Q5 } else { Return_Market_Cap_Q1_final &lt;- cbind(Return_Market_Cap_Q1_final, Return_Market_Cap_Q1) Return_Market_Cap_Q2_final &lt;- cbind(Return_Market_Cap_Q2_final, Return_Market_Cap_Q2) Return_Market_Cap_Q3_final &lt;- cbind(Return_Market_Cap_Q3_final, Return_Market_Cap_Q3) Return_Market_Cap_Q4_final &lt;- cbind(Return_Market_Cap_Q4_final, Return_Market_Cap_Q4) Return_Market_Cap_Q5_final &lt;- cbind(Return_Market_Cap_Q5_final, Return_Market_Cap_Q5) } } # Create mean returns of each portfolio Mean_Return_EW_Q1_final &lt;- rowMeans(Return_Market_Cap_Q1_final, na.rm = T) Mean_Return_EW_Q2_final &lt;- rowMeans(Return_Market_Cap_Q2_final, na.rm = T) Mean_Return_EW_Q3_final &lt;- rowMeans(Return_Market_Cap_Q3_final, na.rm = T) Mean_Return_EW_Q4_final &lt;- rowMeans(Return_Market_Cap_Q4_final, na.rm = T) Mean_Return_EW_Q5_final &lt;- rowMeans(Return_Market_Cap_Q5_final, na.rm = T) # Merge the entire datatframe to one Dates &lt;- as.Date(dmy(Prices_Adj[,1][14:361])) ## For the normal returns Mean_Return_EW_quintiles &lt;- as.data.frame(cbind(Dates, Mean_Return_EW_Q1_final, Mean_Return_EW_Q2_final, Mean_Return_EW_Q3_final, Mean_Return_EW_Q4_final, Mean_Return_EW_Q5_final)) Mean_Return_EW_quintiles_ts &lt;- xts(x = Mean_Return_EW_quintiles[,-1], order.by = Dates) colnames(Mean_Return_EW_quintiles_ts) &lt;- c(&quot;Quintile 1&quot;, &quot;Quintile 2&quot;, &quot;Quintile 3&quot;, &quot;Quintile 4&quot;, &quot;Quintile 5&quot;) ## For the cumulative returns Mean_Return_EW_quintiles_cp &lt;- as.data.frame(cbind(Dates, cumprod(1+Mean_Return_EW_Q1_final), cumprod(1+Mean_Return_EW_Q2_final), cumprod(1+Mean_Return_EW_Q3_final), cumprod(1+Mean_Return_EW_Q4_final), cumprod(1+Mean_Return_EW_Q5_final))) Mean_Return_EW_quintiles_cp_ts &lt;- xts(x = Mean_Return_EW_quintiles_cp[,-1], order.by = Dates) colnames(Mean_Return_EW_quintiles_cp_ts) &lt;- c(&quot;Quintile 1&quot;, &quot;Quintile 2&quot;, &quot;Quintile 3&quot;, &quot;Quintile 4&quot;, &quot;Quintile 5&quot;) # Finally, we can plot the relationship plot_ret &lt;- tidy(Mean_Return_EW_quintiles_ts) %&gt;% ggplot(aes(x=index,y=value, color=series)) + geom_line() + scale_color_manual(values=c(&quot;tomato3&quot;, &quot;khaki3&quot;, &quot;lightsteelblue3&quot;, &quot;dodgerblue4&quot;, &quot;violetred4&quot;)) + ylab(&quot;Cumulative Returns&quot;) + xlab(&quot;Time&quot;) + ggtitle(&quot;Relationship of Normal Returns on Quintile PF based on Market Cap&quot;) + theme(plot.title= element_text(size=14, color=&quot;grey26&quot;, hjust=0.5, lineheight=1.2), panel.background = element_rect(fill=&quot;#f7f7f7&quot;), panel.grid.major.y = element_line(size = 0.5, linetype = &quot;solid&quot;, color = &quot;grey&quot;), panel.grid.minor = element_blank(), panel.grid.major.x = element_blank(), plot.background = element_rect(fill=&quot;#f7f7f7&quot;, color = &quot;#f7f7f7&quot;), axis.title.x = element_text(color=&quot;grey26&quot;, size=12), axis.title.y = element_text(color=&quot;grey26&quot;, size=12), axis.line = element_line(color = &quot;black&quot;)) # Finally, we can plot the relationship plot_cumret &lt;- tidy(Mean_Return_EW_quintiles_cp_ts) %&gt;% ggplot(aes(x=index,y=value, color=series)) + geom_line() + scale_color_manual(values=c(&quot;tomato3&quot;, &quot;khaki3&quot;, &quot;lightsteelblue3&quot;, &quot;dodgerblue4&quot;, &quot;violetred4&quot;)) + ylab(&quot;Cumulative Returns&quot;) + xlab(&quot;Time&quot;) + ggtitle(&quot;Relationship of Cumulative Returns on Quintile PF based on Market Cap&quot;) + theme(plot.title= element_text(size=14, color=&quot;grey26&quot;, hjust=0.5, lineheight=1.2), panel.background = element_rect(fill=&quot;#f7f7f7&quot;), panel.grid.major.y = element_line(size = 0.5, linetype = &quot;solid&quot;, color = &quot;grey&quot;), panel.grid.minor = element_blank(), panel.grid.major.x = element_blank(), plot.background = element_rect(fill=&quot;#f7f7f7&quot;, color = &quot;#f7f7f7&quot;), axis.title.x = element_text(color=&quot;grey26&quot;, size=12), axis.title.y = element_text(color=&quot;grey26&quot;, size=12), axis.line = element_line(color = &quot;black&quot;)) plot_ret plot_cumret As we can see, there appears to be a strictly increasing trend based on the quintile portfolios. We can confirm this trend by looking at the average returns and their standard deviations. # Calculate the mean returns Mean_Return_EW_quintiles_ts &lt;- xts(x = Mean_Return_EW_quintiles[,-1], order.by = Dates) for (i in names(Mean_Return_EW_quintiles_ts)){ Mean_Return_EW &lt;- mean(Mean_Return_EW_quintiles_ts[&#39;1991-01-01/2019-12-01&#39;, i]) SD_Return_EW &lt;- sd(Mean_Return_EW_quintiles_ts[&#39;1991-01-01/2019-12-01&#39;, i]) n_Return_EW &lt;- length(Mean_Return_EW_quintiles_ts[&#39;1991-01-01/2019-12-01&#39;, i]) if (i == &quot;Mean_Return_EW_Q1_final&quot;){ Mean_Return_EW_final &lt;- Mean_Return_EW SD_Return_EW_final &lt;- SD_Return_EW n_Return_EW_final &lt;- n_Return_EW } else { Mean_Return_EW_final &lt;- cbind(Mean_Return_EW_final, Mean_Return_EW) SD_Return_EW_final &lt;- cbind(SD_Return_EW_final, SD_Return_EW) n_Return_EW_final &lt;- cbind(n_Return_EW_final, n_Return_EW) } } # Create the final dataframe Mean_SD_Size_EW_Quintile_PF &lt;- as.data.frame(rbind(Mean_Return_EW_final, SD_Return_EW_final, n_Return_EW_final)) colnames(Mean_SD_Size_EW_Quintile_PF) &lt;- c(&quot;Quintile_1&quot;, &quot;Quintile_2&quot;, &quot;Quintile_3&quot;, &quot;Quintile_4&quot;, &quot;Quintile_5&quot;) rownames(Mean_SD_Size_EW_Quintile_PF) &lt;- c(&quot;Average Return&quot;, &quot;SD Return&quot;, &quot;N Observations&quot;) round(Mean_SD_Size_EW_Quintile_PF,4) ## Quintile_1 Quintile_2 Quintile_3 Quintile_4 Quintile_5 ## Average Return 0.0060 0.0054 0.0066 0.0067 0.0075 ## SD Return 0.0436 0.0406 0.0450 0.0446 0.0482 ## N Observations 348.0000 348.0000 348.0000 348.0000 348.0000 Finally, we can compute the t-test to check if the average return is statistically different # Let&#39;s create a simple function to calculate the t-test for the average difference t_test_mean_diff &lt;- function(mean_a, mean_b, sd_a, sd_b, n_a, n_b) { t_test &lt;- (mean_a - mean_b) / sqrt(sd_a^2/n_a + sd_b^2/n_b) return(t_test) } # Try out the formula t_test_mean_diff(Mean_SD_Size_EW_Quintile_PF$Quintile_5[1], Mean_SD_Size_EW_Quintile_PF$Quintile_1[1], Mean_SD_Size_EW_Quintile_PF$Quintile_5[2], Mean_SD_Size_EW_Quintile_PF$Quintile_1[2], Mean_SD_Size_EW_Quintile_PF$Quintile_5[3], Mean_SD_Size_EW_Quintile_PF$Quintile_1[3]) ## [1] 0.444822 As we can see, there is no significant difference between the average returns of the high and the low portfolio. This implies, given the IID considerations and the baseline statistical properties assumed, the difference in average returns between the smallest and largest stock portfolio is statistically indistinguishable from zero at conventional levels. Thus, the hypothesis that the average returns are not different from each other cannot be rejected. 6.4.2 Factor Construction The second approach constitutes the creation of so-called risk-factor mimicking portfolios. These are portfolios which are sorted based on the risk factor of interest but are put relative to each other, depending on the exposure towards the characteristic under consideration. Consequently, we follow a quite similar approach to the portfolio sorts. That is, we: First single-sort the assts based on specific characteristics in usually 2 or 3 portfolios Double-sort the assets based on the portfolio allocations of the first sort (e.g. Small and Low B2M assets) to obtain the risk-factor mimicking portfolios Take the difference between the double-sorted portfolios which are more and which are less exposed towards the given risk factor The most usual approaches include forming bivariate sorts and aggregating several portfolios together, as in the original contribution of Fama and French (1993). However, some papers also only single-sort the portfolios based on only the respective risk factor. Seldomly, but sometimes appearing, practitioners use n-fold sorting approaches, implying they sort the assets according to n-folds. However, an important caveat with this strategy is the scalability. For instance, let’s look at the n = 3 case, in which we would sort portfolios based on Size, B2M and previous return (used for the Momentum factor). Let’s assume we would sort portfolios based on each characteristics according to above and below median values. In that case, we would create \\(2 \\times 2 \\times 2 = 8\\) sorts. For instance, the SMB factor would be: \\[ \\begin{equation} SMB = 1/4(R_{SLU} + R_{SHU} + R_{SLD} + R_{SHD}) - 1/4(R_{BLU} + R_{BHU} + R_{BLD} + R_{BHD}) \\end{equation} \\] With this strategy, we now interacted the assets based on each characteristic under consideration. For instance, \\(R_{SHU}\\) is the return of the Small-High-Up portfolio, implying lower than median market capitalisation, higher than median B2M ratio and higher than average past return. It is easy to see that operating on high-dimensional regression settings would quickly render this approach infeasible as the resulting number of created portfolios grows by a factor of \\(2^n\\) if a complete interaction is to be maintained. And this is if we only keep to sort the portfolios based on median values. Given the large amount of factors that have been proposed throughout the last two decades (currently there are over 300 factors that are argued to explain the return structure of US equities), such an approach would require substantial computational power to enable a high-dimensional factor analysis. Consequently, let’s follow two approaches in the subsequent factor creation. First, we single-sort portfolios based on their market capitalisation and then take their difference to create a Long-Short (LS) Size Portfolio Then, we recreate the Fama-French SMB factor for the Swiss market by using the formula that was discussed in the lecture # Load the datasets Prices_Adj &lt;- read.csv(&quot;~/Desktop/Master UZH/Data/A4_dataset_01_Ex_Session.txt&quot;, header = T, sep = &quot;\\t&quot;) Prices_Unadj &lt;- read.csv(&quot;~/Desktop/Master UZH/Data/A4_dataset_04_Ex_Session.txt&quot;, header = T, sep = &quot;\\t&quot;) Shares &lt;- read.csv(&quot;~/Desktop/Master UZH/Data/A4_dataset_03_Ex_Session.txt&quot;, header = T, sep = &quot;\\t&quot;) Book &lt;- read.csv(&quot;~/Desktop/Master UZH/Data/A4_dataset_02_Ex_Session.txt&quot;, header = T, sep = &quot;\\t&quot;) # Create Time-Series objects Prices_Adj_ts &lt;- xts(x = Prices_Adj[,-1], order.by = as.Date(dmy(Prices_Adj[,1]))) Prices_Unadj_ts &lt;- xts(x = Prices_Unadj[,-1], order.by = as.Date(dmy(Prices_Unadj[,1]))) Shares_ts &lt;- xts(x = Shares[,-1], order.by = as.Date(dmy(Shares[,1]))) Book_ts &lt;- xts(x = Book[,-1], order.by = as.Date(dmy(Book[,1]))) # Calculate the Returns Returns_Adj_ts &lt;- Return.calculate(Prices_Adj_ts, method = &quot;discrete&quot;) # Create the market cap Time-Series Market_Cap_ts &lt;- Shares_ts * Prices_Unadj_ts # Create the Book-to-Market ratio B2M_ts &lt;- stats::lag(Book_ts, 6) / Market_Cap_ts # Create the cut-off points of each factor composite Market_Cap_ts_Median_Cutoff &lt;- matrixStats::rowMedians(Market_Cap_ts, na.rm = T) B2M_Low_Cutoff &lt;- matrixStats::rowQuantiles(as.matrix(B2M_ts), probs = 0.3, na.rm = T) B2M_High_Cutoff &lt;- matrixStats::rowQuantiles(as.matrix(B2M_ts), probs = 0.7, na.rm = T) # Based on the Cutoff values, generate dummies that indicate whether a certain data point is within a given cutoff level for (i in names(Market_Cap_ts)){ # Create the indicator variables ## For the Market Cap Market_Cap_Small &lt;- ifelse(Market_Cap_ts[, i] &lt;= Market_Cap_ts_Median_Cutoff, 1, NA) Market_Cap_Big &lt;- ifelse(Market_Cap_ts[, i] &gt; Market_Cap_ts_Median_Cutoff, 1, NA) ## For the B2M Ratio B2M_Low &lt;- ifelse(B2M_ts[, i] &lt;= B2M_Low_Cutoff, 1, NA) B2M_Mid &lt;- ifelse((B2M_ts[, i] &gt; B2M_Low_Cutoff) &amp; (B2M_ts[, i] &lt;= B2M_High_Cutoff), 1, NA) B2M_High &lt;- ifelse(B2M_ts[, i] &gt; B2M_High_Cutoff, 1, NA) ## For the interaction indicator variables to get S/L, S/M, S/H &amp; B/L, B/M, B/H Assets_SL &lt;- ifelse((Market_Cap_ts[, i] &lt;= Market_Cap_ts_Median_Cutoff) &amp; (B2M_ts[, i] &lt;= B2M_Low_Cutoff), 1, NA) Assets_SM &lt;- ifelse((Market_Cap_ts[, i] &lt;= Market_Cap_ts_Median_Cutoff) &amp; (B2M_ts[, i] &gt; B2M_Low_Cutoff) &amp; (B2M_ts[, i] &lt;= B2M_High_Cutoff), 1, NA) Assets_SH &lt;- ifelse((Market_Cap_ts[, i] &lt;= Market_Cap_ts_Median_Cutoff) &amp; (B2M_ts[, i] &gt; B2M_High_Cutoff), 1, NA) Assets_BL &lt;- ifelse((Market_Cap_ts[, i] &gt; Market_Cap_ts_Median_Cutoff) &amp; (B2M_ts[, i] &lt;= B2M_Low_Cutoff), 1, NA) Assets_BM &lt;- ifelse((Market_Cap_ts[, i] &gt; Market_Cap_ts_Median_Cutoff) &amp; (B2M_ts[, i] &gt; B2M_Low_Cutoff) &amp; (B2M_ts[, i] &lt;= B2M_High_Cutoff), 1, NA) Assets_BH &lt;- ifelse((Market_Cap_ts[, i] &gt; Market_Cap_ts_Median_Cutoff) &amp; (B2M_ts[, i] &gt; B2M_High_Cutoff), 1, NA) # Calculate the returns ## For the Market Cap Return_Small &lt;- stats::lag(Market_Cap_Small, 1) * Returns_Adj_ts[&#39;1991-01-01/2019-12-01&#39;, i] Return_Big &lt;- stats::lag(Market_Cap_Big, 1)* Returns_Adj_ts[&#39;1991-01-01/2019-12-01&#39;, i] ## For the B2M Ratio Return_Low &lt;- stats::lag(B2M_Low, 1) * Returns_Adj_ts[&#39;1991-01-01/2019-12-01&#39;, i] Return_Mid &lt;- stats::lag(B2M_Mid, 1) * Returns_Adj_ts[&#39;1991-01-01/2019-12-01&#39;, i] Return_High &lt;- stats::lag(B2M_High, 1)* Returns_Adj_ts[&#39;1991-01-01/2019-12-01&#39;, i] ## For the interaction indicator variables to get S/L, S/M, S/H &amp; B/L, B/M, B/H returns Returns_SL &lt;- stats::lag(Assets_SL, 1) * Returns_Adj_ts[&#39;1991-01-01/2019-12-01&#39;, i] Returns_SM &lt;- stats::lag(Assets_SM, 1) * Returns_Adj_ts[&#39;1991-01-01/2019-12-01&#39;, i] Returns_SH &lt;- stats::lag(Assets_SH, 1) * Returns_Adj_ts[&#39;1991-01-01/2019-12-01&#39;, i] Returns_BL &lt;- stats::lag(Assets_BL, 1) * Returns_Adj_ts[&#39;1991-01-01/2019-12-01&#39;, i] Returns_BM &lt;- stats::lag(Assets_BM, 1) * Returns_Adj_ts[&#39;1991-01-01/2019-12-01&#39;, i] Returns_BH &lt;- stats::lag(Assets_BH, 1) * Returns_Adj_ts[&#39;1991-01-01/2019-12-01&#39;, i] if (i == &quot;NESN&quot;){ Returns_SL_final &lt;- Returns_SL Returns_SM_final &lt;- Returns_SM Returns_SH_final &lt;- Returns_SH Returns_BL_final &lt;- Returns_BL Returns_BM_final &lt;- Returns_BM Returns_BH_final &lt;- Returns_BH Return_Small_final &lt;- Return_Small Return_Big_final &lt;- Return_Big } else { Returns_SL_final &lt;- cbind(Returns_SL_final, Returns_SL) Returns_SM_final &lt;- cbind(Returns_SM_final, Returns_SM) Returns_SH_final &lt;- cbind(Returns_SH_final, Returns_SH) Returns_BL_final &lt;- cbind(Returns_BL_final, Returns_BL) Returns_BM_final &lt;- cbind(Returns_BM_final, Returns_BM) Returns_BH_final &lt;- cbind(Returns_BH_final, Returns_BH) Return_Small_final &lt;- cbind(Return_Small_final, Return_Small) Return_Big_final &lt;- cbind(Return_Big_final, Return_Big) } } # Now, we create average, equally weighted returns EW_Returns_SL &lt;- rowMeans(Returns_SL_final, na.rm = T) EW_Returns_SM &lt;- rowMeans(Returns_SM_final, na.rm = T) EW_Returns_SH &lt;- rowMeans(Returns_SH_final, na.rm = T) EW_Returns_BL &lt;- rowMeans(Returns_BL_final, na.rm = T) EW_Returns_BM &lt;- rowMeans(Returns_BM_final, na.rm = T) EW_Returns_BH &lt;- rowMeans(Returns_BH_final, na.rm = T) EW_Returns_Big &lt;- rowMeans(Return_Big_final, na.rm = T) EW_Returns_Small &lt;- rowMeans(Return_Small_final, na.rm = T) # Based on this, we can use the formula to compute the SMB and HML factors SMB &lt;- 1/3*(EW_Returns_SL + EW_Returns_SM + EW_Returns_SH) - 1/3*(EW_Returns_BL + EW_Returns_BM + EW_Returns_BH) LS_Size &lt;- EW_Returns_Small - EW_Returns_Big # Calculate the Cumulative Product Dates &lt;- as.Date(dmy(Prices_Adj[,1][14:361])) SMB_cp &lt;- as.data.frame(cbind(Dates,cumprod(1+SMB), cumprod(1+LS_Size))) SMB_cp_ts &lt;- xts(x = SMB_cp[,-1], order.by = Dates) names(SMB_cp_ts) &lt;- c(&quot;Cumulative Return LS SMB&quot;, &quot;Cumulative Return LS Size&quot;) 6.4.2.1 Cumulative Returns of the Factor-mimicking Portfolios # Create the plot tidy(SMB_cp_ts) %&gt;% ggplot(aes(x=index,y=value, color=series)) + geom_line() + scale_color_manual(values=c(&quot;tomato3&quot;, &quot;khaki3&quot;, &quot;lightsteelblue3&quot;, &quot;dodgerblue4&quot;, &quot;violetred4&quot;)) + ylab(&quot;Cumulative Returns&quot;) + xlab(&quot;Time&quot;) + ggtitle(&quot;Relationship of Cumulative Returns on SMB LS PF&quot;) + theme(plot.title= element_text(size=14, color=&quot;grey26&quot;, hjust=0.5, lineheight=1.2), panel.background = element_rect(fill=&quot;#f7f7f7&quot;), panel.grid.major.y = element_line(size = 0.5, linetype = &quot;solid&quot;, color = &quot;grey&quot;), panel.grid.minor = element_blank(), panel.grid.major.x = element_blank(), plot.background = element_rect(fill=&quot;#f7f7f7&quot;, color = &quot;#f7f7f7&quot;), axis.title.x = element_text(color=&quot;grey26&quot;, size=12), axis.title.y = element_text(color=&quot;grey26&quot;, size=12), axis.line = element_line(color = &quot;black&quot;)) Great! We were able to construct the size factor based on both single- and double-sorting strategy. Interestingly, we observe that the Fama-French factor shows lower cumulative returns compared to the single-sorted factor. We can assess this observation analytically. For instance, we could deconstruct the factor again into its individual constituents. Note that the main difference between both was the inclusion of a value characteristic through a B2M ratio. Apparently, this inclusion decreases the cumulative returns quite considerably from the 2000’s on. Let’s visualise this as well: # Calculate the Cumulative Product Dates &lt;- as.Date(dmy(Prices_Adj[,1][14:361])) SMB_cp_components &lt;- as.data.frame(cbind(Dates, cumprod(1+EW_Returns_SL), cumprod(1+EW_Returns_SM), cumprod(1+EW_Returns_SH), cumprod(1+EW_Returns_BL), cumprod(1+EW_Returns_BM), cumprod(1+EW_Returns_BH))) # Create the xts object SMB_cp_components_ts &lt;- xts(x = SMB_cp_components[,-1], order.by = Dates) names(SMB_cp_components_ts) &lt;- c(&quot;Cumulative Return S/L&quot;, &quot;Cumulative Return S/M&quot;, &quot;Cumulative Return S/H&quot;, &quot;Cumulative Return B/L&quot;, &quot;Cumulative Return B/M&quot;, &quot;Cumulative Return B/H&quot;) # Create the plot for the double-sorted SMB Factor SMB_plot_comp &lt;- tidy(SMB_cp_components_ts) %&gt;% ggplot(aes(x=index,y=value, color=series)) + geom_line() + scale_color_manual(values=c( &quot;khaki3&quot;, &quot;lightsteelblue3&quot;, &quot;dodgerblue4&quot;, &quot;tomato3&quot;, &quot;violetred4&quot;, &quot;darkorange2&quot;)) + ylab(&quot;Cumulative Returns&quot;) + xlab(&quot;Time&quot;) + ggtitle(&quot;Relationship of Cumulative Returns on SMB LS PF - Components&quot;) + labs(color=&#39;Factor Portfolios&#39;) + scale_colour_manual(values = c(&quot;lightsteelblue3&quot;,&quot;dodgerblue4&quot;, &quot;darkorange2&quot;, &quot;springgreen4&quot;, &quot;violetred4&quot;, &quot;goldenrod&quot;, &quot;khaki3&quot;)) + theme(plot.title= element_text(size=14, color=&quot;grey26&quot;, hjust=0.3,lineheight=2.4, margin=margin(15,0,15,0)), panel.background = element_rect(fill=&quot;#f7f7f7&quot;), panel.grid.major.y = element_line(size = 0.5, linetype = &quot;solid&quot;, color = &quot;grey&quot;), panel.grid.minor = element_blank(), panel.grid.major.x = element_blank(), plot.background = element_rect(fill=&quot;#f7f7f7&quot;, color = &quot;#f7f7f7&quot;), axis.title.y = element_text(color=&quot;grey26&quot;, size=12, margin=margin(0,10,0,10)), axis.title.x = element_text(color=&quot;grey26&quot;, size=12, margin=margin(10,0,10,0)), axis.line = element_line(color = &quot;grey&quot;)) ## Scale for &#39;colour&#39; is already present. Adding another scale for &#39;colour&#39;, which will replace the existing scale. SMB_plot_comp As we can observe, by distributing the average return into its constituents, we see that average returns are mainly driven by High B2M ratios. Furthermore, we can observe that, throughout all B2M characteristics, assets with larger than median market caps outperform their smaller counterparts. Given this mononous behaviour, it is apparent that the Size factor appears to no longer hold, at least when considering the last 30 years. Furthermore, it can be shown that, although the value factor is still valid, its magnitude depends on the market capitalisation of the securities under consideration, as higher B2M ratios outperform their lower counterparts only when we control for size (for instance, the second best performing portfolio is small and high, and it greatly outperforms the big and medium B2M ratio portfolio). 6.4.2.2 Fama-MacBeth Regression We have seen the strength of the Fama-MacBeth (1973) procedure in assessing the ability of a covariate to explain movements in the cross-section of dependent returns during the application of the CAPM, where we were able to empirically prove that the movements of the market returns are unable to sufficiently explain the cross-sectional variation in asset returns. Given that the Market Return in the CAPM is nothing more than a factor itself, it is straight-forward to assume that we can replicate the procedure to include the factors under consideration. However, since we are working with multiple factors, we need to generalise the formula we encountered in the previous chapter. As such, we first run a time-series regression of each asset’s excess return on the factors under consideration. That is, we run: \\[ r_{i,t} - r_{ft} = \\alpha_i + \\sum_{k=1}^K \\beta_{i,k}f_k + \\epsilon_{it} \\] whereas \\(f_k\\) is the k’th factor in the model. Based on that, retrieve the factor loadings of each factor and perform a cross-sectional regression at each date, in which we regress the (excess) returns of each asset on the factor loadings. That is, we run: \\[ r_{i,t} - r_{ft} = \\lambda_{i,0} + \\sum_{k=1}^K \\lambda_{i,k}\\hat{\\beta}_{i,k} + u_{it} \\] This is identical to what we have seen in the previous chapter. The only difference is now that we have multiple factors. This is represented by the sum sign. This means that we take factor loading 1 to k and run, at each date, the cross-section of asset returns on the asset returns. Let’s recreate the FM (1973) approach and apply it to the SMB factor. # Load the risk free rate as well as the Market Index rf &lt;- read.csv(&quot;~/Desktop/Master UZH/Data/A2_dataset_02_Ex_Session.txt&quot;, header = T, sep = &quot;\\t&quot;) rf_sub &lt;- subset(rf, (Date &gt;= &#39;1989-12-01&#39;) &amp; (Date &lt;= &#39;2019-12-31&#39;)) rf_ts &lt;- xts(rf_sub[,-1], order.by = as.Date(dmy(Prices_Adj[,1]))) rf_ts_yearly &lt;- rf_ts$SWISS.CONFEDERATION.BOND.1.YEAR...RED..YIELD / 100 rf_ts_monthly &lt;- ((1 + rf_ts_yearly)^(1/12) - 1)[&#39;1991-01-01/2019-12-01&#39;] colnames(rf_ts_monthly) &lt;- &quot;rf&quot; SMI &lt;- read.csv(&quot;~/Desktop/Master UZH/Data/A2_dataset_03_Ex_Session.txt&quot;, header = T, sep = &quot;\\t&quot;) SMI_sub &lt;- subset(SMI, (Date &gt;= &#39;1989-12-01&#39;) &amp; (Date &lt;= &#39;2019-12-31&#39;)) SMI_ts &lt;- xts(SMI_sub[,2], order.by = as.Date(dmy(Prices_Adj[,1]))) SMI_ts_ret &lt;- Return.calculate(SMI_ts, method = &quot;discrete&quot;) Mkt_rf_SMI &lt;- SMI_ts_ret[&#39;1991-01-01/2019-12-01&#39;] - rf_ts_monthly[&#39;1991-01-01/2019-12-01&#39;] colnames(Mkt_rf_SMI) &lt;- &quot;Index&quot; # Calculate the X and y variables for the regression SPI_Index_ts &lt;- merge.xts(Returns_Adj_ts[&#39;1991-01-01/2019-12-01&#39;], rf_ts_monthly[&#39;1991-01-01/2019-12-01&#39;], Mkt_rf_SMI[&#39;1991-01-01/2019-12-01&#39;], SMB) # First Pass Regression for (i in names(SPI_Index_ts)[1:384]){ col_name &lt;- paste0(i) # Get the rolling window coefficients fit_roll_coefs &lt;- summary(lm(SPI_Index_ts[,i] - SPI_Index_ts[,385] ~ SPI_Index_ts[,386] + SPI_Index_ts[,387]))$coefficients[2:3] if (i == &#39;NESN&#39;){ col_name_final &lt;- col_name fit_roll_coefs_final &lt;- fit_roll_coefs } else{ col_name_final &lt;- cbind(col_name_final, col_name) fit_roll_coefs_final &lt;- cbind(fit_roll_coefs_final, fit_roll_coefs) } } # Set the column names colnames(fit_roll_coefs_final) &lt;- col_name_final # Get the coefficients at each time period beta_smb &lt;- t(fit_roll_coefs_final) colnames(beta_smb) &lt;- c(&quot;MKT_RF&quot;, &quot;SMB&quot;) avg_ret &lt;- t(SPI_Index_ts[,1:384]) # Bind them together second_pass_data &lt;- as.data.frame(cbind(beta_smb, avg_ret)) # Perform the second pass regression factors = 2 models &lt;- lapply(paste(&quot;`&quot;,names(second_pass_data[,2:350]) , &quot;`&quot;, &#39; ~ MKT_RF + SMB&#39;, sep = &quot;&quot;), function(f){ lm(as.formula(f), data = second_pass_data) %&gt;% # Call lm(.) summary() %&gt;% # Gather the output &quot;$&quot;(coef) %&gt;% # Keep only the coefs data.frame() %&gt;% # Convert to dataframe dplyr::select(Estimate)} # Keep only estimates ) lambdas &lt;- matrix(unlist(models), ncol = factors + 1, byrow = T) %&gt;% data.frame() colnames(lambdas) &lt;- c(&quot;Constant&quot;, &quot;MKT_RF&quot;, &quot;SMB&quot;) # We can now calculate the respective t statistics T_Period &lt;- length(names(second_pass_data[,2:350])) average &lt;- colMeans(lambdas, na.rm = T) sd &lt;- colStdevs(lambdas, na.rm = T) se &lt;- sd / sqrt(T_Period) t_value &lt;- average/se # Create a final data frame second_pass &lt;- as.data.frame(cbind(average, se, t_value)) colnames(second_pass) &lt;- c(&quot;Risk Premia Estimate&quot;, &quot;Standard Error&quot;, &quot;T-Stat&quot;) round(second_pass, 4) ## Risk Premia Estimate Standard Error T-Stat ## Constant -0.0016 0.0045 -0.3631 ## MKT_RF -0.0015 0.0021 -0.7409 ## SMB 0.0063 0.0019 3.2394 As we can see, the SMB factor is already quite successful in explaining the cross-sectional movements under consideration. However, note the following. If we leave the standard errors as is, we are highly likely to induce bias into the regression model. This is two-fold. Initially, we can refer to the usual issues in estimation accuracy based on the covariance structure of the assets under consideration. Note therein the heteroskedasticity and serial correlation assumptions we made in earlier chapters. Further, and even more worrisome, the factor loadings \\(\\hat{\\beta}_{i,k}\\) are estimates. This induces a bias arising from potential selection mistakes, or measurement error, as we have discussed. Usually, we need to further correct for this by applying different sorting or standard error correction techniques (to comprehend how, please refer to Shanken (1992)). Lastly, we can visualise the behavior of the factor risk premia over time. Doing so, we will make use of the following visualisation. # Here, we only take from the second row onwards lambdas[2:nrow(lambdas),] %&gt;% # We only select the factors of interet dplyr::select(MKT_RF, SMB) %&gt;% # We then bind the columns such that we can gather them on the same x axis bind_cols(date = as.Date(names(second_pass_data[,3:350]))) %&gt;% # We gather the factor and the lambda value based on each date gather(key = factor, value = lambda, -date) %&gt;% # Here, we just create the ggplot ggplot(aes(x = date, y = lambda, color = factor)) + geom_line() + facet_grid(factor~.) + scale_color_manual(values=c(&quot;tomato3&quot;, &quot;khaki3&quot;, &quot;lightsteelblue3&quot;, &quot;dodgerblue4&quot;, &quot;violetred4&quot;)) + theme_light() We can observe that both factors behave somewhat dependently on each other. That is, they could potentially represent collinearity issues. Both factors compensate in an unclear aggregate effect. This highlights the usefulness of penalized estimates, a machine learning technique that is used to reduce dimensionality of a regression framework. However, it is not the aim of this course to dive into this chapter (yet). 6.4.2.3 Factor Competition As we stated, the main aim of of factors is to explain the cross-section of stock returns. However, as we have shown above, there is a potential that factors are moving simultaneously. In other words, it is likely that some factors are colinear to each other. As we understand it, collinearity is an issue because it absorbs variation of a given covariate, thereby making the estimates of interest redundant. If we have a variable which can be substantially represented by another variable within our model, then the resulting framework will induce variation which does not add explanatory power to the variation of the underlying variable of interest, but only noise. Without stating the theoretical concepts of this situation, this shares great similarities with the bias-and-variance issue usually encountered in machine learning applications. Therein, by introducing highly correlated variables, we add noise to the model which, subsequently, increases the standard errors and thus reduces the precision of the estimates. In addition, Guine (2021) states that, when asset managers decompose the performance of their returns into factors, overlaps (high absolute correlations) between factors yield exposures that are less interpretable; positive and negative exposures compensate each other in an uncomprehensible, or spurious, way. They state a simple protocol to account for redundant factors, by running regressions of each factor against all others. That is, you run: \\[ f_{t,k} = \\alpha_{k} + \\sum_{j \\neq k} \\delta_{k,j}f_{t,j} + \\epsilon_{t,k} \\] The authors state that the interesting metric is then the test statistic associated to the estimation of \\(\\alpha_k\\). If \\(\\alpha_k\\) is significantly different from zero, then the cross-section of (other) factors fails to explain exhaustively the average return of factor k. Otherwise, the return of the factor can be captured by exposures to the other factors and is thus redundant. We will come back to this application once we encountered and retrieved more factors for the Swiss market. 6.5 Create your own factors: A step-by-step example We have now leveraged the linear factor model and understand how to use these models in order to evaluate variation and detect anomalies. So far, the factors that we used were already pre-defined, meaning that we had the data already and “just” had to use the factor construction strategy discussed. However, without data there is not much to compute. Consequently, in the next part, we intend to bring you closer to the actual task of retrieving data needed to construct factor mimicking portfolios and then use the steps discussed above to coherently construct the factor(s) of interest. This will be quite helpful because it constitutes of all steps necessary to construct, apply and interpret factors (and thus resembles the day to day work within factor setting asset management companies). However, a major difference to all external asset managers is that we intend not to take too long in order to retrieve the data at hand :-). As such, we will again introduce the field of the relational database computing. We have elaborated the SQL coding approach to retrieve data from a relational database. However, we now enhanced the function which automates the entire process for us such that we can now get data from CRSP, Compustat as well as Thomson Reuters / Datastream. The latter is important because it allows us to access the worldscope data base, which offers the largeest consortium of company financials as well as security characteristics for Non-US stocks. 6.5.1 Setting up the WRDS proxy The function for WRDS Database Query is the following. Again, you don’t need to know the function by heart. However, there are still multiple databases which this function cannot (yet) retrieve. Thus, if you feel like something is missing, try modifying the function and see if you are able to get what you need. Any help is certainly greatly appreciated! # Create a function to automate the data retrieval jobs on SQL dataset_a = NULL dataset_b = NULL dataset_sql = NULL datafmt = NULL consol = NULL indfmt = NULL sic = NULL gvkey = NULL tic= NULL cusip= NULL isin = NULL filters_list= NULL filters_list_final= NULL filters_list_tweaked= NULL filters_list_tweaked_final= NULL query_sql &lt;- function(dataset_a, dataset_b, column_a, column_b, column_sql, start, end, datafmt, consol, indfmt, sic, gvkey, tic, cusip, isin, multi_function = TRUE, reuters_ds = FALSE){ if (reuters_ds == FALSE){ if (!is.null(column_a)) { for (i in 1:length(column_a)) { column_a[i] = paste(&quot;a.&quot;, column_a[i], sep = &quot;&quot;) column_filter_a = paste(column_a, collapse = &#39;,&#39;) } } else { columns_filter_a = &quot;&quot; } if (!is.null(column_b)) { for (i in 1:length(column_b)) { column_b[i] = paste(&quot;b.&quot;, column_b[i], sep = &quot;&quot;) column_filter_b = paste(column_b, collapse = &#39;,&#39;) } } else { columns_filter_b = &quot;&quot; } if (!is.null(column_sql)) { for (i in 1:length(column_sql)) { column_sql[i] = paste(&quot;b.&quot;, column_sql[i], sep = &quot;&quot;) column_filter_sql = paste(column_sql, collapse = &#39;,&#39;) } } else { columns_filter_sql = &quot;&quot; } if (!is.null(start) &amp; !is.null(end)){ date_filter = paste(&quot;a.datadate BETWEEN &#39;&quot;, start, &quot;&#39; AND &#39;&quot;, end, &quot;&#39;&quot;) } sic_filter = NULL if (!is.null(sic)) { for (i in 1:length(sic)) { sic[i] = paste(&quot;&#39;&quot;, sic[i], &quot;&#39;&quot;, sep = &quot;&quot;) sic_filter = paste(&quot;a.sic IN (&quot;, paste(sic, collapse = &#39;,&#39;), &quot;)&quot;) } } gvkey_filter = NULL if (!is.null(gvkey)) { for (i in 1:length(gvkey)) { gvkey[i] = paste(&quot;&#39;&quot;, gvkey[i], &quot;&#39;&quot;, sep = &quot;&quot;) gvkey_filter = paste(&quot;a.gvkey IN (&quot;, paste(gvkey, collapse = &#39;,&#39;), &quot;)&quot;) } } tic_filter = NULL if (!is.null(tic)) { for (i in 1:length(tic)) { tic[i] = paste(&quot;&#39;&quot;, tic[i], &quot;&#39;&quot;, sep = &quot;&quot;) tic_filter = paste(&quot;a.tic IN (&quot;, paste(tic, collapse = &#39;,&#39;), &quot;)&quot;) } } cusip_filter = NULL if (!is.null(cusip)) { for (i in 1:length(cusip)) { cusip[i] = paste(&quot;&#39;&quot;, cusip[i], &quot;&#39;&quot;, sep = &quot;&quot;) cusip_filter = paste(&quot;a.cusip IN (&quot;, paste(cusip, collapse = &#39;,&#39;), &quot;)&quot;) } } if (!is.null(datafmt)) { for (i in 1:length(datafmt)) { datafmt[i] = paste(&quot;a.datafmt = &#39;&quot;, datafmt[i], &quot;&#39;&quot;, sep = &quot;&quot;) datafmt_filter = paste(datafmt, collapse = &#39;,&#39;) } } if (!is.null(consol)) { for (i in 1:length(consol)) { consol[i] = paste(&quot;a.consol = &#39;&quot;, consol[i], &quot;&#39;&quot;, sep = &quot;&quot;) consol_filter = paste(consol, collapse = &#39;,&#39;) } } if (!is.null(indfmt)) { for (i in 1:length(indfmt)) { indfmt[i] = paste(&quot;a.indfmt = &#39;&quot;, indfmt[i], &quot;&#39;&quot;, sep = &quot;&quot;) indfmt_filter = paste(indfmt, collapse = &#39;,&#39;) } } filters = c(date_filter, cusip_filter, tic_filter, gvkey_filter, sic_filter, datafmt_filter, consol_filter, indfmt_filter) for (i in 1:length(filters)){ if (!is.null(filters[i])){ filters_list[i] = paste(filters[i], sep = &quot;&quot;) filters_list_final = paste(&quot; WHERE &quot;, paste(filters_list, collapse = &quot; AND &quot;)) } } filters_tweaked = c(date_filter, cusip_filter, tic_filter, gvkey_filter, sic_filter) if (!is.null(filters_tweaked[i])){ for (i in 1:length(filters_tweaked)){ filters_list_tweaked[i] = paste(filters_tweaked[i], sep = &quot;&quot;) filters_list_tweaked_final = paste(&quot; WHERE &quot;, paste(filters_list_tweaked, collapse = &quot; AND &quot;)) } } if (multi_function == TRUE){ sql = (paste(&quot;SELECT &quot;, column_filter_a, &quot;, &quot;, column_filter_b, &quot; FROM &quot;, dataset_a, &quot; a&quot;, &quot; inner join &quot;, dataset_b, &quot; b&quot;, &quot; on &quot;, column_a[1], &quot; = &quot;, column_sql[1], &quot; and &quot;, column_a[2], &quot; = &quot;, column_sql[2], &quot; and &quot;, column_a[3], &quot; = &quot;, column_sql[3], filters_list_final)) } else { sql = (paste(&quot;SELECT &quot;, column_filter_a, &quot; FROM &quot;, dataset_a, &quot; a&quot;, filters_list_tweaked_final)) } } else { if (!is.null(column_a)) { for (i in 1:length(column_a)) { column_a[i] = paste(&quot;a.&quot;, column_a[i], sep = &quot;&quot;) column_filter_a = paste(column_a, collapse = &#39;,&#39;) } } else { columns_filter_a = &quot;&quot; } if (!is.null(column_b)) { for (i in 1:length(column_b)) { column_b[i] = paste(&quot;b.&quot;, column_b[i], sep = &quot;&quot;) column_filter_b = paste(column_b, collapse = &#39;,&#39;) } } else { columns_filter_b = &quot;&quot; } if (!is.null(column_sql)) { for (i in 1:length(column_sql)) { column_sql[i] = paste(&quot;b.&quot;, column_sql[i], sep = &quot;&quot;) column_filter_sql = paste(column_sql, collapse = &#39;,&#39;) } } else { columns_filter_sql = &quot;&quot; } if (!is.null(start) &amp; !is.null(end)){ date_filter = paste(&quot;a.year_ BETWEEN &#39;&quot;, start, &quot;&#39; AND &#39;&quot;, end, &quot;&#39;&quot;) } sic_filter = NULL if (!is.null(sic)) { for (i in 1:length(sic)) { sic[i] = paste(&quot;&#39;&quot;, sic[i], &quot;&#39;&quot;, sep = &quot;&quot;) sic_filter = paste(&quot;a.sic IN (&quot;, paste(sic, collapse = &#39;,&#39;), &quot;)&quot;) } } gvkey_filter = NULL if (!is.null(gvkey)) { for (i in 1:length(gvkey)) { gvkey[i] = paste(&quot;&#39;&quot;, gvkey[i], &quot;&#39;&quot;, sep = &quot;&quot;) gvkey_filter = paste(&quot;a.gvkey IN (&quot;, paste(gvkey, collapse = &#39;,&#39;), &quot;)&quot;) } } tic_filter = NULL if (!is.null(tic)) { for (i in 1:length(tic)) { tic[i] = paste(&quot;&#39;&quot;, tic[i], &quot;&#39;&quot;, sep = &quot;&quot;) tic_filter = paste(&quot;a.item5601 IN (&quot;, paste(tic, collapse = &#39;,&#39;), &quot;)&quot;) } } cusip_filter = NULL if (!is.null(cusip)) { for (i in 1:length(cusip)) { cusip[i] = paste(&quot;&#39;&quot;, cusip[i], &quot;&#39;&quot;, sep = &quot;&quot;) cusip_filter = paste(&quot;a.item6004 IN (&quot;, paste(cusip, collapse = &#39;,&#39;), &quot;)&quot;) } } isin_filter = NULL if (!is.null(isin)) { for (i in 1:length(isin)) { isin[i] = paste(&quot;&#39;&quot;, isin[i], &quot;&#39;&quot;, sep = &quot;&quot;) isin_filter = paste(&quot;a.item6008 IN (&quot;, paste(isin, collapse = &#39;,&#39;), &quot;)&quot;) } } filters = c(date_filter, cusip_filter, tic_filter, gvkey_filter, sic_filter, isin_filter) for (i in 1:length(filters)){ if (!is.null(filters[i])){ filters_list[i] = paste(filters[i], sep = &quot;&quot;) filters_list_final = paste(&quot; WHERE &quot;, paste(filters_list, collapse = &quot; AND &quot;)) } } filters_tweaked = c(date_filter, cusip_filter, tic_filter, gvkey_filter, sic_filter, isin_filter) if (!is.null(filters_tweaked[i])){ for (i in 1:length(filters_tweaked)){ filters_list_tweaked[i] = paste(filters_tweaked[i], sep = &quot;&quot;) filters_list_tweaked_final = paste(&quot; WHERE &quot;, paste(filters_list_tweaked, collapse = &quot; AND &quot;)) } } if (multi_function == TRUE){ sql = (paste(&quot;SELECT &quot;, column_filter_a, &quot;, &quot;, column_filter_b, &quot; FROM &quot;, dataset_a, &quot; a&quot;, &quot; inner join &quot;, dataset_b, &quot; b&quot;, &quot; on &quot;, column_a[1], &quot; = &quot;, column_sql[1], &quot; and &quot;, column_a[2], &quot; = &quot;, column_sql[2], &quot; and &quot;, column_a[3], &quot; = &quot;, column_sql[3], filters_list_final)) } else { sql = (paste(&quot;SELECT &quot;, column_filter_a, &quot; FROM &quot;, dataset_a, &quot; a&quot;, filters_list_tweaked_final)) } } } Note that we will retrieve the data needed from the WRDS services. In order to use their services, we need to log onto their system. This can be done again by using the command below. # Open the connection wrds &lt;- dbConnect(Postgres(), host=&#39;wrds-pgdata.wharton.upenn.edu&#39;, port=9737, dbname=&#39;wrds&#39;, sslmode=&#39;require&#39;, user=&#39;gostlow&#39;, password = &quot;climaterisk8K&quot;) Once the credentials are valid, we can access the database and start retrieving the data we require. 6.5.2 Factors of interest Having established a connection to the WRDS services, the next question is which factors we want to create ourselves. In order to do so, we will focus at the most commonly known and publicly cited anomaly factors currently present. These include: Size (Small Minus Big (SMB)) Value (High Minus Low (HML)) Momentum (Winners Minus Losers (WML)) Profitability (Robust Minus Weak (RMW)) Investment (Conservative Minus Aggressive (CMA)) Low Risk (Betting Against Beta (BAB)) We already know how to calculate the Size factor. However, for the remaining factors, we still need to define the common framework of construction. The Value factor is constructed by retrieving data on the Book to Market Ratio (B2M). The formula is: \\[ \\text{B2M}_t = \\frac{\\text{Book-Equity}_{t-6}}{\\text{Market Cap}_t} \\] Using this formula, at each date t, we go long in assets with a high Book-to-Market Ratio and Short in assets with a low Book-to-Market ratio. Therein, we define High and Low as the 30th and 70th percentile of the distribution, respectively. The respective indicator on High and Low B2M Ratios is then interacted with the indicator on small or large cap firms, whereas we use the median value as cut-off for the company size. The factor is then constructed using the following formula: \\[ \\text{HML}_t = 1/2 * (SH_t + BH_t) - 1/2*(SL_t + BL_t) \\] whereas \\(SH_t\\) is the equal- or value-weighted return of small (below or at median) market cap and high B2M (above 70th percentile) stocks, \\(BH_t\\) is the return of Big and High B2M securities and \\(SL_t\\) as well as \\(BL_t\\) are their low B2M counterparts. The Momentum factor is constructed by retrieving the data on Adjusted Stock Prices. The formula is: \\[ \\text{Momentum}_t = \\frac{\\text{Prices Adjusted}_{t-1} - \\text{Prices Adjusted}_{t-12}}{\\text{Prices Adjusted}_{t-12}} \\] Using this formula, at each date t, we go long in assets with a high Past Adjusted return and Short in assets with a low Past Adjusted return. Therein, we define High and Low again as the 30th and 70th percentile of the distribution, respectively. The respective indicator on High and Low Past Return Adjusted Ratios is then interacted with the indicator on small or large cap firms, whereas we use the median value as cut-off for the company size. The factor is then constructed using the following formula: \\[ \\text{WML}_t = 1/2 * (SW_t + BW_t) - 1/2*(SL_t + BL_t) \\] whereas \\(SW_t\\) is the equal- or value-weighted return of small (below or at median) market cap and return winner (above 70th percentile) stocks, \\(BW_t\\) is the return of Big and return winner securities and \\(SL_t\\) as well as \\(BL_t\\) are their return loser counterparts. The Profitability factor is constructed by retrieving the data on revenues, cost, expenses as well as book equity. The formula is: \\[ \\text{Profitability}_t = \\frac{\\text{Operating Income}_{t} - \\text{COGS}_{t} - \\text{SGAE}_{t} - \\text{Interest Expenses}_{t} }{\\text{Total Book Equity}_{t}} \\] whereas Operating Income is just the total income from operations, COGS are the Costs of Goods Sold, SGAE are Selling, General and Administrative Expenses and Total Book Equity is the Equity of the Balance Sheet. Note that COGS and SGAE are also sometimes referred to as Operating Expenses (although they are not identical they match quite well so for calculation purposes it should not depend too greatly on which accounting numbers you take). Using this formula, at each date t, we go long in assets with a high Profitability and Short in assets with a low Profitability. Therein, we define High and Low again as the 30th and 70th percentile of the distribution, respectively. The respective indicator on High and Low Past Return Adjusted Ratios is then interacted with the indicator on small or large cap firms, whereas we use the median value as cut-off for the company size. The factor is then constructed using the following formula: \\[ \\text{RMW}_t = 1/2 * (SR_t + BR_t) - 1/2*(SW_t + BW_t) \\] whereas \\(SR_t\\) is the equal- or value-weighted return of small (below or at median) market cap and robust (above 70th percentile) stocks, \\(BR_t\\) is the return of Big and robust securities and \\(SW_t\\) as well as \\(BW_t\\) are their rprofitability weak counterparts. The Investment factor is constructed by retrieving the data on total assets. The formula is: \\[ \\text{Investment}_t = \\frac{\\text{Total Assets}_{t} - \\text{Total Assets}_{t-1}}{\\text{Total Assets}_{t-1}} \\] Aggressive firms are those that experience the largest growth in assets. Using this formula, at each date t, we go long in assets with a high Asset growth and Short in assets with a low Asset growth. Therein, we define High and Low again as the 30th and 70th percentile of the distribution, respectively. The respective indicator on High and Low Past Return Adjusted Ratios is then interacted with the indicator on small or large cap firms, whereas we use the median value as cut-off for the company size. The factor is then constructed using the following formula: \\[ \\text{CMA}_t = 1/2 * (SC_t + BC_t) - 1/2*(SA_t + BA_t) \\] whereas \\(SC_t\\) is the equal- or value-weighted return of small (below or at median) market cap and conservative (below 30th percentile) stocks, \\(BC_t\\) is the return of Big and conservative securities and \\(SW_t\\) as well as \\(BW_t\\) are their asset growth aggressive counterparts. Lastly, the Betting Against Beta factor is constructed in a somewhat advanced manner. We follow the approach by Frazzini and Pedersen (2014) here and construct the factor by running rolling regressions of the excess returns on market excess returns. However, we will not directly run the regressions. Rather, we will calculate the following formula for each beta at date t: \\[ \\hat{\\beta}_{TS}= \\hat{\\rho}\\cdot \\frac{\\hat{\\sigma}_i}{\\hat{\\sigma}_M} \\] whereas \\(\\hat{\\sigma}_i\\) and \\(\\hat{\\sigma}_i\\) are the estimated volatilities for the stock and the market and \\(\\hat{\\rho}\\) is their estimated their correlation. They use a one-year rolling standard deviation to estimate volatility as well as a five-year horizon for the correlation. Lastly, the authors reduce the influence of outliers by following Vasicek (1973) and construct a shrinkage of the time series estimate of beta towards 1 by running: \\[ \\hat{\\beta}_{i} = 0.4\\cdot \\hat{\\beta}_{TS} + 0.6 \\cdot 1 \\] Having constructed the beta estimates, they rank the betas and go long in assets with a below median beta value and short in assets with an above median beta value. Then, in each portfolio, securities are weighted by the ranked betas (i.e., lower-beta securities have larger weights in the low-beta portfolio and higher-beta securities have larger weights in the high-beta portfolio). To put it more formally, we have that \\(z_i = rank(\\hat{\\beta}_{it})\\) and \\(\\bar{z} = \\frac{1}{n}\\sum_{i=1}^N z_i\\), whereas \\(z_i\\) indicates the rank of the i’th beta and \\(\\bar{z}\\) is the average rank, where n is the number of securities. Based on this, the portfolio weights are constructed as: \\[ w_{it} = k(z_{it} - \\bar{z}_t) \\] whereas \\(k = \\frac{2}{\\sum_{i=1}^N |z_i - z|}\\) is a normalisation constant. Based on these weights, we can finally construct the BAB factor as: \\[ \\text{BAB}_t = \\frac{1}{\\sum_{i=1}^N\\beta_{it}^Lw_{it}^L}(\\sum_{i=1}^N r_{i,t+1}^Lw_{it}^L - r_f) - \\frac{1}{\\sum_{i=1}^N\\beta_{it}^Hw_{it}^H}(\\sum_{i=1}^N r_{i,t+1}^Hw_{it}^H - r_f) \\] For all the factors, it is important to understand that they need to be lagged by one period to avoid any look-ahead bias. 6.5.3 Get the data to construct the factors We now proceed by retrieving the data from the datastream services. Therein, we will need to fetch two distinct types of data. The company stock characteristics which are taken from the datastream equitites database, as well as the company fundamanetals characteristics through the worldscope database. Note that, to show you how to retrieve the data in multiple ways, we decided to retrieve the data on stock characteristics manually while using the function to retrieve the data on company fundamentals. The exact composition of the services of Datastream worldscope as well as the respective names can be found on the WRDS homepage, whereas you can find the compositon of the services of Datastream equities here. For our use, we will need the Daily Stock File (wrds_ds2dsf) from the equities database as well as the Fundamentals Annually (wrds_ws_funda) as well as the Stock Data (wrds_ws_stock) tables of the worldscope database. The variables that we retrieve from the equities database are given below: adjclose: Adjusted Close price (Used for: MOM) close: Unadjusted Close price (Used for: SMB, HML) volume: Volume of traded shares numshrs: Number of shares outstanding (Used for: SMB, HML) Further, we need some identifiers to assure that we only take the Swiss companies under consideration: dscode: Code identifier for the stocks of TR marketdate: Date of the observation currency: Currency the company is traded in region: Region of company’s main location isin: ISIN number ticker: Ticker dslocalcode: Local code (combination of ISIN and Ticker based on TR) While the names for the equities database are quite straight-forward to interpret, the variable names of the worldscope database are presented in “items”, whereas each item is associated with a different accounting number. These are indicated below: item3501: Common Equity (= Book Value. Used for: HML, RMW. From wrds_ws_funda) item2999: Total Assets (Used for: HML, CMA. From: wrds_ws_funda) item3351: Total Liabilities (Used for: HML. From: wrds_ws_funda) item1001: Net Sales or Revenues (Used for: RMW. From: wrds_ws_funda) item1051: COGS excl depreciation (Used for: RMW. From: wrds_ws_funda) item1101: SGA (Used for: RMW, From wrds_ws_funda) item1100: Gross Income item1249: Operating Expenses item1251: Interest Expense on Debt Moreover, we need data on the date and sequence codes year_: Year of observation seq: Sequential code code: Company code from TR item5350: Fiscal Period End date (Used to identify at which date the company had its annual closing. Based on this date, the 12 previous periods will obtain the same accouting numbers when upsampling from annual to monthly observations - e.g. if a company has its reported closing on the 31st of March, then the respective accounting number must be replicated for the subsequent 12 periods in order to base portfolios on the characteristic). Further, we need to define for which companies we require the data at hand. As it was with the data in CRSP and Compustat, we can enter a search query by providing security information based on (I) Stock Ticker (II) ISIN Number (III) CUSIP Number (IV) IBES identifier. However, we always advise you to use CUSIP numbers whenever possible, as these are the truly unique identifiers that can be used to source multiple databases at once. In our case, these are the following items: item5601: Ticker Symbol item6008: ISIN item6004: CUSIP item6038: IBES For the use case here, we have a comprehensive ticker dataset with the companies that we previously used for earlier exercises. We will use this as identifier for our securities. 6.5.3.1 Fetch the Swiss Data for Stock Characteristics through datastream equitites # Get the stock market data (shares outstanding, adjusted and unadjusted stock prices, volume of trades) res &lt;- dbSendQuery(wrds, &quot;select a.dscode, a.marketdate,a.adjclose,a.close,a.volume, a.numshrs, a.ri, a.currency, a.region, b.isin, b.ticker, b.dslocalcode from tr_ds_equities.wrds_ds2dsf a left join tr_ds_equities.wrds_ds_names b on a.dscode = b.dscode where a.marketdate BETWEEN &#39;1990-01-01&#39; and &#39;2021-12-31&#39; AND a.region = &#39;CH&#39;&quot;) data_2 &lt;- dbFetch(res) # Split the ISIN into two parts # This is done to identify Swiss companies with the characteristic &quot;CH&quot; in front of the ISIN number CH_data &lt;- data_2 %&gt;% mutate(country = substr(data_2$isin,1,2)) %&gt;% subset(country == &quot;CH&quot; &amp; currency == &quot;CHF&quot;) # Get the calender week CH_data &lt;- CH_data %&gt;% mutate(Cal_Week = lubridate::week(marketdate)) # Expand the date CH_data_stock &lt;- separate(CH_data, &quot;marketdate&quot;, c(&quot;Year&quot;, &quot;Month&quot;, &quot;Day&quot;), sep = &quot;-&quot;) # Delete duplicated rows CH_data_stock &lt;- CH_data_stock[!duplicated(CH_data_stock[c(&quot;Year&quot;, &quot;Month&quot;, &quot;Day&quot;, &#39;ticker&#39;, &#39;dscode&#39;)]),] # Monthly Data: Get only the last date of each month CH_data_stock_monthly &lt;- CH_data_stock %&gt;% # Group by each year-month and company group_by(dscode, Year, Month) %&gt;% # Only get the maximum day value (the last day of each month per company and year) filter(Day == max(Day)) %&gt;% # Recreate the actual last day date mutate(Date_actual = as.Date(paste(Year,Month,Day,sep=&quot;-&quot;)), # Since this is only the last observed date, we need to transform them into the last date of each month (e.g. if last observed day was 2000-06-28 we # need to transform this to 2000-06-30 to match the relationship afterwards) Date_t = lubridate::ceiling_date(Date_actual, &quot;month&quot;) - 1) %&gt;% ungroup() %&gt;% select(Date_t, ticker, isin, adjclose, close, numshrs) # Weekly Data: Get only the last date of each week (same logic as above, but this time with weeks and not months) CH_data_stock_weekly &lt;- CH_data_stock %&gt;% group_by(dscode, Year, Month, Cal_Week) %&gt;% filter(Day == max(Day)) %&gt;% mutate(Date_t = as.Date(paste(Year,Month,Day,sep=&quot;-&quot;))) %&gt;% ungroup() %&gt;% select(Date_t, ticker, isin, adjclose, close, numshrs) 6.5.3.2 Fetch the Swiss data for company fundamanetals characteristics through worldscope # First, we define the ticker symbol tic &lt;- unique(CH_data_stock_monthly$ticker) # Then, we define the variables to be retrieved column_a &lt;- list(&#39;year_&#39;, &#39;seq&#39;, &#39;code&#39;, &#39;item5350&#39;, # Date and sequence codes &quot;item5601&quot;, &quot;item6008&quot;, &quot;item6004&quot;, &quot;item6038&quot;, # The identifiers &quot;item3501&quot;, &quot;item2999&quot;, &quot;item3351&quot;, # Common Equity, Total Assets, Total Liabs &quot;item1001&quot;, &quot;item1051&quot;, &quot;item1101&quot;, &quot;item1100&quot;, &quot;item1249&quot;, &quot;item1251&quot;) # Total Revenue, COGS, SGA, Gross Income, Operating Exp, Interest Exp) # This is to connect the both databases without duplicating their output in the end column_sql = list(&#39;year_&#39;, &#39;seq&#39;, &#39;code&#39;) # This is the second database, used for the keys from the securities monthly list column_b = list(&#39;item8001&#39;, &#39;item8004&#39;) # Market Cap, Market Cap Public # Get the quarterly data on company financials query = query_sql(dataset_a = &quot;tr_worldscope.wrds_ws_funda&quot;, dataset_b = &quot;tr_worldscope.wrds_ws_stock&quot;, multi_function = F, reuters_ds = T, column_a = column_a, column_b = column_b, column_sql = column_sql, gvkey = gvkey, sic = sic, tic = tic, cusip = cusip, isin = isin, datafmt = NULL, consol = &#39;C&#39;, indfmt = &#39;INDL&#39;, start = &#39;1990&#39;, end = &#39;2022&#39;) res &lt;- dbSendQuery(wrds, query) ## Warning in result_create(conn@ptr, statement, immediate): Closing open result set, cancelling previous query data &lt;- dbFetch(res) colnames(data) &lt;- c(&#39;Year_t&#39;, &#39;Seq&#39;, &#39;Code&#39;, &quot;Fiscal_Period_End_Date&quot;, &quot;Ticker&quot;, &quot;ISIN&quot;, &quot;CUSIP&quot;, &quot;IBES&quot;, &quot;Total_Eq_t&quot;, &quot;Total_Assets_t&quot;, &quot;Total_Liab_t&quot;, &quot;Revenue_Tot_t&quot;, &quot;COGS_t&quot;, &quot;SGA_t&quot;, &quot;Gross_Inc_t&quot;, &quot;Operating_Exp_t&quot;, &quot;Interest_Exp_t&quot;) # Split the ISIN into two parts CH_data_fund &lt;- data %&gt;% mutate(country = substr(data$ISIN,1,2)) %&gt;% subset(country == &quot;CH&quot;) # Remove duplicated observations based on the Code and Year combination CH_data_fund &lt;- CH_data_fund[!duplicated(CH_data_fund[c(&#39;Year_t&#39;, &#39;Code&#39;)]),] CH_data_fund &lt;- separate(CH_data_fund, &quot;Fiscal_Period_End_Date&quot;, c(&quot;Year_Fiscal_Period_End&quot;, &quot;Month_Fiscal_Period_End&quot;, &quot;Day_Fiscal_Period_End&quot;), sep = &quot;-&quot;) # Create the monthly dataset CH_data_fund_monthly &lt;- CH_data_fund %&gt;% mutate(Dupl_Dummy = ifelse(!is.na(Total_Assets_t) | !is.na(Total_Liab_t), 1, 0)) %&gt;% subset(Dupl_Dummy == 1) %&gt;% group_by(Year_t, Code) %&gt;% slice(rep(1:n(), first(12))) %&gt;% # Expand the data from yearly to monthly while accounting for different fiscal period end dates mutate(Month_t = ifelse(Month_Fiscal_Period_End == &quot;01&quot;, c(02, 03, 04, 05, 06, 07, 08, 09, 10, 11, 12, 01), ifelse(Month_Fiscal_Period_End == &quot;03&quot;, c(04, 05, 06, 07, 08, 09, 10, 11, 12, 01, 02, 03), ifelse(Month_Fiscal_Period_End == &quot;04&quot;, c(05, 06, 07, 08, 09, 10, 11, 12, 01, 02, 03, 04), ifelse(Month_Fiscal_Period_End == &quot;05&quot;, c(06, 07, 08, 09, 10, 11, 12, 01, 02, 03, 04, 05), ifelse(Month_Fiscal_Period_End == &quot;06&quot;, c(07, 08, 09, 10, 11, 12, 01, 02, 03, 04, 05, 06), ifelse(Month_Fiscal_Period_End == &quot;07&quot;, c(08, 09, 10, 11, 12, 01, 02, 03, 04, 05, 06, 07), ifelse(Month_Fiscal_Period_End == &quot;08&quot;, c(09, 10, 11, 12, 01, 02, 03, 04, 05, 06, 07, 08), ifelse(Month_Fiscal_Period_End == &quot;09&quot;, c(10, 11, 12, 01, 02, 03, 04, 05, 06, 07, 08, 09), ifelse(Month_Fiscal_Period_End == &quot;10&quot;, c(11, 12, 01, 02, 03, 04, 05, 06, 07, 08, 09, 10), ifelse(Month_Fiscal_Period_End == &quot;11&quot;, c(12, 01, 02, 03, 04, 05, 06, 07, 08, 09, 10, 11), c(01, 02, 03, 04, 05, 06, 07, 08, 09, 10, 11, 12) ) ) ) ) ) ) ) ) ) ), Day_t = c(01, 01, 01, 01, 01, 01, 01, 01, 01, 01, 01, 01), # Create for each month the starting period Start_Date_t = as.Date(paste(Year_t,Month_t,Day_t,sep=&quot;-&quot;)), # And also the last day of the month -&gt; assign as Date column since we only work with last day of month values (They are the same anyways) Date_t = lubridate::ceiling_date(Start_Date_t, &quot;month&quot;) - 1 ) %&gt;% ungroup() %&gt;% # Select only the variables of interest select(Date_t, Ticker, ISIN, Total_Assets_t, Total_Liab_t, Gross_Inc_t, Operating_Exp_t, Interest_Exp_t, Total_Eq_t) colnames(CH_data_stock_monthly) = c(&quot;Date_t&quot;, &quot;Ticker&quot;, &quot;ISIN&quot;, &quot;Adj_Close&quot;, &quot;Close&quot;, &quot;Num_Shares&quot;) 6.5.3.3 Export the monthly and weekly datasets # Create a dataframe which incorporates all data CH_data_total_monthly &lt;- left_join(CH_data_stock_monthly, CH_data_fund_monthly, by = c(&quot;Ticker&quot; = &quot;Ticker&quot;, &quot;Date_t&quot; = &quot;Date_t&quot;)) write.csv(CH_data_total_monthly, &quot;~/Desktop/CH_data_total_monthly.csv&quot;, row.names = FALSE) # Write the weekly csv for stocks write.csv(CH_data_stock_weekly, &quot;~/Desktop/CH_data_stock_weekly.csv&quot;, row.names = FALSE) 6.5.4 Portfolio Sorts for different factors We first deploy the portfolio sorts strategy to determine the behaviour of percentile distributed portfolios based on the characteristic of interest. Therein, we will create decile portfolios which are sorted based on the factors of interest. However, before we start, we need to clarify the following. Previously, we used a specific approach to construct factors. This approach is also known as wide format approach, because we use a wide formatted dataframe in order to calculate the factor returns. Remember that a wide formatted dataframe means that each column shows the data of one specific characteristic for one specific company. Thus, if you have 300 companies for 10 years on a yearly basis, then the dataframe will have 300 columns and 10 rows. This implies that we need multiple dataframes to construct multiple factors. The other method is called the long format approach. This is because we use a long formatted dataframe. As opposed to a wide formatted dataframe, long formats are constructed such that, in each column, we have a specific accounting number. In the other two columns, we then have the respective observational period as well as the company identifier for the respective accounting measure. For instance, if you have 3 specific measures (let’s say Total Equity, Net Income as well as Turnover) for 300 companies and 10 years, then you have a dataframe consisting of 5 columns and 3000 rows (300 companies à 10 years). This implies that we only need one dataframe to construct multiple factors. We showed you the wide format approach previously because, although it appears to be computationally more complex, it actually shows all the specific steps we make when constructing factors. That is, you can trace pretty much each step back to its direct predecessor and thereby comprehend more easily how the code operates. Despite this, we now introduce the long format approach. Although it does not document each operation directly, the main advantage is that it is computationally less expensive. As such, we can retrieve the results with less code and running time. However, for one specific factor, the Jegadeesh Momentum factor, we still create the factors in both forms. This is to show you the main concepts in both ways. Especially, we want to highlight some differences in the sorting strategy that R follows which can potentially lead to slightly different results. In general, sorting is still a highly debated field with multiple different possibilities that statistics programs offer. When we use a pre-defined sorting function, we are likely to obtain different sorts compared to using a customised (own created) sorting procedure. Although this can lead to different results, it shows an additional, important caveat for scientific work. That is, we need to be clear and precise in formulating our assumptions even in more technical and less central areas of the work at hand. 6.5.4.1 Jegadeesh and Titman Momentum Factor This is the replication of the Jegadeesh and Titman Momentum factor. Therein, we will construct the factors in both a wide and long format to show you the differences that the choice of the sorting function can have on the final performance of our portfolios. Note that this is not the same Momentum factor as we will cover in the last assignment and, thus, it cannot be copied to calculate the momentum factor. 6.5.4.1.1 Wide-format factor construction # Let&#39;s run the experiment for the reduced data frame tic_sub &lt;- unique(names(read.csv(&quot;~/Desktop/Master UZH/Data/A4_dataset_04_Ex_Session.txt&quot;, header = T, sep = &quot;\\t&quot;))) # Get 347 companies CH_data_total_monthly &lt;- read.csv(&quot;~/Desktop/CH_data_total_monthly.csv&quot;, header = T, sep = &quot;,&quot;) # Create a wide data frame CH_data_total_monthly_clean &lt;- CH_data_total_monthly[!duplicated(CH_data_total_monthly[c(&quot;Date_t&quot;, &quot;Ticker&quot;)]),] CH_data_total_monthly_clean &lt;- CH_data_total_monthly_clean %&gt;% subset(Ticker %in% tic_sub) %&gt;% select(Date_t, Ticker, Adj_Close) CH_data_total_monthly_clean_wide &lt;- CH_data_total_monthly_clean %&gt;% na.omit() %&gt;% spread(key = Ticker, value = Adj_Close) # Create time series object CH_data_total_monthly_clean_wide_ts &lt;- xts(CH_data_total_monthly_clean_wide[,-1], order.by = as.Date(CH_data_total_monthly_clean_wide$Date_t)) # Calculate Returns CH_data_total_monthly_wide_ret_ts &lt;- Return.calculate(CH_data_total_monthly_clean_wide_ts, &quot;discrete&quot;) # Cut the 5% most extreme returns extreme_cutoff &lt;- matrixStats::rowQuantiles(as.matrix(CH_data_total_monthly_wide_ret_ts), probs = 0.999, na.rm = T) for (i in names(CH_data_total_monthly_wide_ret_ts)){ extreme &lt;- ifelse(CH_data_total_monthly_wide_ret_ts[,i] &lt;= extreme_cutoff, 1, NA) if (i == &quot;ABBN&quot;){ extreme_final &lt;- extreme } else { extreme_final &lt;- cbind(extreme_final, extreme) } } CH_data_total_monthly_wide_ret_ts &lt;- CH_data_total_monthly_wide_ret_ts*extreme_final # Calculate Cumulative Returns for (i in names(CH_data_total_monthly_wide_ret_ts)){ col_name &lt;- paste0(i) Log_Ret_Adj = log(1+CH_data_total_monthly_wide_ret_ts[,i]) Sum_Ret = roll_sum(Log_Ret_Adj, 6) Cum_Ret = exp(Sum_Ret) - 1 if (i == &quot;ABBN&quot;){ Cum_Ret_final &lt;- Cum_Ret } else{ Cum_Ret_final &lt;- cbind(Cum_Ret_final, Cum_Ret) } } one_to_ten &lt;- seq(1,10,1) for (i in one_to_ten){ assign(paste0(&quot;Momentum_d&quot;, i), matrixStats::rowQuantiles(as.matrix(Cum_Ret_final), probs = i/10, na.rm = T)) } for (i in names(Cum_Ret_final)) { col_name &lt;- paste0(i) Momentum_indicator_d1 = ifelse(Cum_Ret_final[,i] &lt;= Momentum_d1, 1, NA) Momentum_indicator_d2 = ifelse(Cum_Ret_final[,i] &gt; Momentum_d1 &amp; Cum_Ret_final[,i] &lt;= Momentum_d2, 1, NA) Momentum_indicator_d3 = ifelse(Cum_Ret_final[,i] &gt; Momentum_d2 &amp; Cum_Ret_final[,i] &lt;= Momentum_d3, 1, NA) Momentum_indicator_d4 = ifelse(Cum_Ret_final[,i] &gt; Momentum_d3 &amp; Cum_Ret_final[,i] &lt;= Momentum_d4, 1, NA) Momentum_indicator_d5 = ifelse(Cum_Ret_final[,i] &gt; Momentum_d4 &amp; Cum_Ret_final[,i] &lt;= Momentum_d5, 1, NA) Momentum_indicator_d6 = ifelse(Cum_Ret_final[,i] &gt; Momentum_d5 &amp; Cum_Ret_final[,i] &lt;= Momentum_d6, 1, NA) Momentum_indicator_d7 = ifelse(Cum_Ret_final[,i] &gt; Momentum_d6 &amp; Cum_Ret_final[,i] &lt;= Momentum_d7, 1, NA) Momentum_indicator_d8 = ifelse(Cum_Ret_final[,i] &gt; Momentum_d7 &amp; Cum_Ret_final[,i] &lt;= Momentum_d8, 1, NA) Momentum_indicator_d9 = ifelse(Cum_Ret_final[,i] &gt; Momentum_d8 &amp; Cum_Ret_final[,i] &lt;= Momentum_d9, 1, NA) Momentum_indicator_d10 = ifelse(Cum_Ret_final[,i] &gt; Momentum_d9, 1, NA) Return_Mom_d1 &lt;- stats::lag(Momentum_indicator_d1, n=1) * CH_data_total_monthly_wide_ret_ts[&#39;1990-01-31/2021-12-31&#39;, i] Return_Mom_d2 &lt;- stats::lag(Momentum_indicator_d2, n=1) * CH_data_total_monthly_wide_ret_ts[&#39;1990-01-31/2021-12-31&#39;, i] Return_Mom_d3 &lt;- stats::lag(Momentum_indicator_d3, n=1) * CH_data_total_monthly_wide_ret_ts[&#39;1990-01-31/2021-12-31&#39;, i] Return_Mom_d4 &lt;- stats::lag(Momentum_indicator_d4, n=1) * CH_data_total_monthly_wide_ret_ts[&#39;1990-01-31/2021-12-31&#39;, i] Return_Mom_d5 &lt;- stats::lag(Momentum_indicator_d5, n=1) * CH_data_total_monthly_wide_ret_ts[&#39;1990-01-31/2021-12-31&#39;, i] Return_Mom_d6 &lt;- stats::lag(Momentum_indicator_d6, n=1) * CH_data_total_monthly_wide_ret_ts[&#39;1990-01-31/2021-12-31&#39;, i] Return_Mom_d7 &lt;- stats::lag(Momentum_indicator_d7, n=1) * CH_data_total_monthly_wide_ret_ts[&#39;1990-01-31/2021-12-31&#39;, i] Return_Mom_d8 &lt;- stats::lag(Momentum_indicator_d8, n=1) * CH_data_total_monthly_wide_ret_ts[&#39;1990-01-31/2021-12-31&#39;, i] Return_Mom_d9 &lt;- stats::lag(Momentum_indicator_d9, n=1) * CH_data_total_monthly_wide_ret_ts[&#39;1990-01-31/2021-12-31&#39;, i] Return_Mom_d10 &lt;- stats::lag(Momentum_indicator_d10, n=1) * CH_data_total_monthly_wide_ret_ts[&#39;1990-01-31/2021-12-31&#39;, i] if(i == &quot;ABBN&quot;){ Return_Mom_d1_final &lt;- Return_Mom_d1 Return_Mom_d2_final &lt;- Return_Mom_d2 Return_Mom_d3_final &lt;- Return_Mom_d3 Return_Mom_d4_final &lt;- Return_Mom_d4 Return_Mom_d5_final &lt;- Return_Mom_d5 Return_Mom_d6_final &lt;- Return_Mom_d6 Return_Mom_d7_final &lt;- Return_Mom_d7 Return_Mom_d8_final &lt;- Return_Mom_d8 Return_Mom_d9_final &lt;- Return_Mom_d9 Return_Mom_d10_final &lt;- Return_Mom_d10 } else{ Return_Mom_d1_final &lt;- cbind(Return_Mom_d1_final, Return_Mom_d1) Return_Mom_d2_final &lt;- cbind(Return_Mom_d2_final, Return_Mom_d2) Return_Mom_d3_final &lt;- cbind(Return_Mom_d3_final, Return_Mom_d3) Return_Mom_d4_final &lt;- cbind(Return_Mom_d4_final, Return_Mom_d4) Return_Mom_d5_final &lt;- cbind(Return_Mom_d5_final, Return_Mom_d5) Return_Mom_d6_final &lt;- cbind(Return_Mom_d6_final, Return_Mom_d6) Return_Mom_d7_final &lt;- cbind(Return_Mom_d7_final, Return_Mom_d7) Return_Mom_d8_final &lt;- cbind(Return_Mom_d8_final, Return_Mom_d8) Return_Mom_d9_final &lt;- cbind(Return_Mom_d9_final, Return_Mom_d9) Return_Mom_d10_final &lt;- cbind(Return_Mom_d10_final, Return_Mom_d10) } } EW_Return_Mom_d1 &lt;- rowMeans(Return_Mom_d1_final, na.rm = T)[8:dim(Return_Mom_d1_final)[1]] EW_Return_Mom_d2 &lt;- rowMeans(Return_Mom_d2_final, na.rm = T)[8:dim(Return_Mom_d2_final)[1]] EW_Return_Mom_d3 &lt;- rowMeans(Return_Mom_d3_final, na.rm = T)[8:dim(Return_Mom_d3_final)[1]] EW_Return_Mom_d4 &lt;- rowMeans(Return_Mom_d4_final, na.rm = T)[8:dim(Return_Mom_d4_final)[1]] EW_Return_Mom_d5 &lt;- rowMeans(Return_Mom_d5_final, na.rm = T)[8:dim(Return_Mom_d5_final)[1]] EW_Return_Mom_d6 &lt;- rowMeans(Return_Mom_d6_final, na.rm = T)[8:dim(Return_Mom_d6_final)[1]] EW_Return_Mom_d7 &lt;- rowMeans(Return_Mom_d7_final, na.rm = T)[8:dim(Return_Mom_d7_final)[1]] EW_Return_Mom_d8 &lt;- rowMeans(Return_Mom_d8_final, na.rm = T)[8:dim(Return_Mom_d8_final)[1]] EW_Return_Mom_d9 &lt;- rowMeans(Return_Mom_d9_final, na.rm = T)[8:dim(Return_Mom_d9_final)[1]] EW_Return_Mom_d10 &lt;- rowMeans(Return_Mom_d10_final, na.rm = T)[8:dim(Return_Mom_d10_final)[1]] Dates &lt;- as.Date(CH_data_total_monthly_clean_wide$Date_t[8:dim(Return_Mom_d1_final)[1]]) df_mom &lt;- as.data.frame(cbind(Dates, cumprod(1+EW_Return_Mom_d1), cumprod(1+EW_Return_Mom_d2), cumprod(1+EW_Return_Mom_d3), cumprod(1+EW_Return_Mom_d4), cumprod(1+EW_Return_Mom_d5), cumprod(1+EW_Return_Mom_d6), cumprod(1+EW_Return_Mom_d7), cumprod(1+EW_Return_Mom_d8), cumprod(1+EW_Return_Mom_d9), cumprod(1+EW_Return_Mom_d10))) colnames(df_mom) &lt;- c(&quot;Date&quot;, &quot;Decile 1&quot;, &quot;Decile 2&quot;, &quot;Decile 3&quot;, &quot;Decile 4&quot;, &quot;Decile 5&quot;, &quot;Decile 6&quot;, &quot;Decile 7&quot;, &quot;Decile 8&quot;, &quot;Decile 9&quot;, &quot;Decile 10&quot;) # Transform into long format df_mom_long &lt;- df_mom %&gt;% mutate(Date = as.Date(Date)) %&gt;% gather(Decile, value, -c(Date)) colnames(df_mom_long) &lt;- c(&quot;Date_t&quot;, &quot;Decile&quot;, &quot;cum_ret&quot;) CH_data_Cum_Ret_long &lt;- df_mom_long %&gt;% mutate(Decile = as.factor(Decile)) %&gt;% mutate(Date_t = as.Date(Date_t)) CH_data_Cum_Ret_long %&gt;% ggplot() + geom_line(mapping = aes( x = Date_t , y = cum_ret , color = Decile), size = 0.5 ) + # scale_color_manual(values=c(&quot;tomato3&quot;, &quot;khaki3&quot;, &quot;lightsteelblue3&quot;, &quot;dodgerblue4&quot;, &quot;violetred4&quot;, &quot;black&quot;)) + ylab(&quot;Cumulative Returns&quot;) + xlab(&quot;Time&quot;) + ggtitle(&quot;Jegadeesh Titman Momentum Strategy with wide formatted dataset&quot;) + theme(plot.title= element_text(size=14, color=&quot;grey26&quot;, hjust=0.5, lineheight=1.2), panel.background = element_rect(fill=&quot;#f7f7f7&quot;), panel.grid.major.y = element_line(size = 0.5, linetype = &quot;solid&quot;, color = &quot;grey&quot;), panel.grid.minor = element_blank(), panel.grid.major.x = element_blank(), plot.background = element_rect(fill=&quot;#f7f7f7&quot;, color = &quot;#f7f7f7&quot;), axis.title.x = element_text(color=&quot;grey26&quot;, size=12), axis.title.y = element_text(color=&quot;grey26&quot;, size=12), axis.line = element_line(color = &quot;black&quot;)) Finally, let’s calculate the summary statistics df_mom &lt;- as.data.frame(cbind(Dates, EW_Return_Mom_d1, EW_Return_Mom_d2, EW_Return_Mom_d3, EW_Return_Mom_d4, EW_Return_Mom_d5, EW_Return_Mom_d6, EW_Return_Mom_d7, EW_Return_Mom_d8, EW_Return_Mom_d9, EW_Return_Mom_d10)) colnames(df_mom) &lt;- c(&quot;Date&quot;, &quot;Decile 1&quot;, &quot;Decile 2&quot;, &quot;Decile 3&quot;, &quot;Decile 4&quot;, &quot;Decile 5&quot;, &quot;Decile 6&quot;, &quot;Decile 7&quot;, &quot;Decile 8&quot;, &quot;Decile 9&quot;, &quot;Decile 10&quot;) # Transform into long format df_mom_long &lt;- df_mom %&gt;% mutate(Date = as.Date(Date)) %&gt;% gather(Decile, value, -c(Date)) colnames(df_mom_long) &lt;- c(&quot;Date_t&quot;, &quot;Decile&quot;, &quot;EW_Ret_mean_t&quot;) df_mom_long %&gt;% subset(Decile != &quot;NA&quot;) %&gt;% group_by(Decile) %&gt;% mutate(Annualised_Mean_Return = round((1+mean(EW_Ret_mean_t))^12 -1, 5), Annualised_SD = round(sd(EW_Ret_mean_t)*sqrt(12), 5), Sharpe_Ratio = round(Annualised_Mean_Return/Annualised_SD,5)) %&gt;% select(Decile, Annualised_Mean_Return, Annualised_SD, Sharpe_Ratio) %&gt;% unique() ## # A tibble: 10 × 4 ## # Groups: Decile [10] ## Decile Annualised_Mean_Return Annualised_SD Sharpe_Ratio ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Decile 1 -0.0997 0.283 -0.352 ## 2 Decile 2 0.0111 0.202 0.0549 ## 3 Decile 3 0.0119 0.188 0.0635 ## 4 Decile 4 0.0720 0.177 0.407 ## 5 Decile 5 0.0670 0.158 0.423 ## 6 Decile 6 0.0824 0.150 0.548 ## 7 Decile 7 0.0850 0.146 0.581 ## 8 Decile 8 0.124 0.150 0.824 ## 9 Decile 9 0.129 0.156 0.828 ## 10 Decile 10 0.172 0.179 0.962 6.5.4.1.2 Long-format factor construction # Let&#39;s run the experiment for the reduced data frame tic_sub &lt;- unique(names(read.csv(&quot;~/Desktop/Master UZH/Data/A4_dataset_04_Ex_Session.txt&quot;, header = T, sep = &quot;\\t&quot;))) # Get 347 companies CH_data_total_monthly &lt;- read.csv(&quot;~/Desktop/CH_data_total_monthly.csv&quot;, header = T, sep = &quot;,&quot;) CH_data_total_monthly_clean &lt;- CH_data_total_monthly[!duplicated(CH_data_total_monthly[c(&quot;Date_t&quot;, &quot;Ticker&quot;)]),] # Only get the respective columns CH_data_total_mont &lt;- CH_data_total_monthly_clean %&gt;% subset(Ticker %in% tic_sub) %&gt;% select(Date_t, Ticker, Adj_Close) %&gt;% mutate(Date_t = as.Date(Date_t), Date_t = as.yearmon(Date_t)) # Control for extreme values CH_data_total_monthly_sub &lt;- CH_data_total_mont %&gt;% arrange(Date_t) %&gt;% select(Date_t, Ticker, Adj_Close) %&gt;% group_by(Ticker) %&gt;% mutate(Ret_Adj = (Adj_Close - lag(Adj_Close, 1))/lag(Adj_Close,1)) %&gt;% na.omit() %&gt;% ungroup() %&gt;% group_by(Date_t) %&gt;% mutate(Extreme_Values = quantile(Ret_Adj, p = 0.999), Extreme_Indicator = ifelse(Ret_Adj &lt;= Extreme_Values, 1, NA), Ret_Adj = Ret_Adj * Extreme_Indicator) %&gt;% ungroup() # Get the lagged values to ensure that we have a HPR of 6 periods! # Since we go 6 periods behind and take the cum ret from period -6 to period -1 to obtain the HPR from period -5 to 0. # Idea: If we do it like this, we account for gaps in the data. E.g. if two observational periods were on Aug 1990 and then on Aug 1991, the gap would be 12 periods. Thus, this would not constitute a HPR of 6 periods, but 12. In order to ensure we only ever get HPR of 6 periods, we need to create the indicator which shows how many periods (in months) two dates are apart from one another. This must equal 6 and not more! CH_data_total_monthly_sub_lag &lt;- CH_data_total_monthly_sub %&gt;% group_by(Ticker) %&gt;% mutate(lag6 = round(12*(Date_t - lag(Date_t, n=5)))/5) # Get the adjustred returns to form the PF on CH_data_total_monthly_sub_cumret &lt;- CH_data_total_monthly_sub_lag %&gt;% group_by(Ticker) %&gt;% mutate(Log_Ret_Adj = lag(log(1+Ret_Adj), n = 1), Sum_Ret = roll_sum(Log_Ret_Adj, 6), Cum_Ret = exp(Sum_Ret) - 1) %&gt;% na.omit() %&gt;% ungroup() # Assign rankings CH_data_total_monthly_sub_rank &lt;- CH_data_total_monthly_sub_cumret %&gt;% filter(., lag6 ==1) %&gt;% group_by(Date_t) %&gt;% # Here we form the groups (ranks) mutate(Decile = as.numeric(cut(Cum_Ret, breaks = quantile(Cum_Ret, probs = seq(0, 1, length = 11), na.rm = TRUE, type = 4), include.lowest = TRUE)), p1 = quantile(Cum_Ret, 0.1), p2 = quantile(Cum_Ret, 0.2), p3 = quantile(Cum_Ret, 0.3), p4 = quantile(Cum_Ret, 0.4), p5 = quantile(Cum_Ret, 0.5), p6 = quantile(Cum_Ret, 0.6), p7 = quantile(Cum_Ret, 0.7), p8 = quantile(Cum_Ret, 0.8), p9 = quantile(Cum_Ret, 0.9)) %&gt;% ungroup() # Select only certain columns CH_data_total_monthly_sub_rank_2_lag &lt;- CH_data_total_monthly_sub_rank %&gt;% select(Date_t, Ticker, Decile) # Merge two df CH_data_total_monthly_sub_rank_tot &lt;- CH_data_total_monthly_sub_lag %&gt;% ungroup() %&gt;% filter(., lag6 == 1 ) %&gt;% left_join(CH_data_total_monthly_sub_rank_2_lag, by = c(&quot;Ticker&quot;=&quot;Ticker&quot;, &quot;Date_t&quot; = &quot;Date_t&quot;)) %&gt;% select(Ticker,Date_t,Ret_Adj,Decile) %&gt;% arrange(Ticker,Date_t) %&gt;% group_by(Date_t,Decile) %&gt;% mutate(momr = mean(Ret_Adj)) %&gt;% ungroup() %&gt;% select(Date_t,Decile,momr) %&gt;% unique() # Create mean returns CH_data_EW_Ret &lt;- CH_data_total_monthly_sub_rank_tot %&gt;% group_by(Date_t, Decile) %&gt;% mutate(EW_Ret_mean_t = mean(momr, na.rm = T), EW_Ret_sd_t = sd(momr, na.rm = T)) %&gt;% select(Date_t,Decile,EW_Ret_mean_t,EW_Ret_sd_t) %&gt;% ungroup() %&gt;% unique() %&gt;% arrange(Date_t,Decile) CH_data_Cum_Ret &lt;- CH_data_EW_Ret %&gt;% group_by(Decile) %&gt;% mutate(cum_ret = cumprod(1+EW_Ret_mean_t)) ewretdat2 &lt;- CH_data_Cum_Ret %&gt;% mutate(Decile = as.factor(Decile)) %&gt;% mutate(Date_t = as.Date(Date_t)) %&gt;% subset(!is.na(cum_ret) &amp; !is.na(Decile)) ggplot(data = ewretdat2) + geom_line(mapping = aes(x =Date_t ,y = cum_ret ,color = Decile), size = 0.5 ) + ggtitle(&quot;Jegadeesh Titman Momentum Strategy with long formatted dataset&quot;) + ylab(&quot;Cumulative Returns&quot;) + xlab(&quot;Time&quot;) + theme(plot.title= element_text(size=14, color=&quot;grey26&quot;, hjust=0.5, lineheight=1.2), panel.background = element_rect(fill=&quot;#f7f7f7&quot;), panel.grid.major.y = element_line(size = 0.5, linetype = &quot;solid&quot;, color = &quot;grey&quot;), panel.grid.minor = element_blank(), panel.grid.major.x = element_blank(), plot.background = element_rect(fill=&quot;#f7f7f7&quot;, color = &quot;#f7f7f7&quot;), axis.title.x = element_text(color=&quot;grey26&quot;, size=12), axis.title.y = element_text(color=&quot;grey26&quot;, size=12), axis.line = element_line(color = &quot;black&quot;)) Finally, let’s calculate the summary statistics CH_data_EW_Ret %&gt;% subset(Decile != &quot;NA&quot;) %&gt;% group_by(Decile) %&gt;% mutate(Annualised_Mean_Return = round((1+mean(EW_Ret_mean_t))^12 -1,5), Annualised_SD = round(sd(EW_Ret_mean_t)*sqrt(12), 5), Sharpe_Ratio = round(Annualised_Mean_Return/Annualised_SD,5)) %&gt;% select(Decile, Annualised_Mean_Return, Annualised_SD, Sharpe_Ratio) %&gt;% unique() ## # A tibble: 10 × 4 ## # Groups: Decile [10] ## Decile Annualised_Mean_Return Annualised_SD Sharpe_Ratio ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 -0.103 0.287 -0.359 ## 2 2 0.0137 0.199 0.0685 ## 3 3 0.00573 0.189 0.0304 ## 4 4 0.0742 0.178 0.416 ## 5 5 0.0638 0.157 0.406 ## 6 6 0.0826 0.149 0.553 ## 7 7 0.0858 0.143 0.600 ## 8 8 0.122 0.149 0.817 ## 9 9 0.129 0.153 0.843 ## 10 10 0.177 0.177 0.999 6.5.4.2 The Size Factor This is the long-format construction of the size factor based on the data downloaded. # Let&#39;s run the experiment for the reduced data frame tic_sub &lt;- unique(names(read.csv(&quot;~/Desktop/Master UZH/Data/A4_dataset_04_Ex_Session.txt&quot;, header = T, sep = &quot;\\t&quot;))) # Get 347 companies CH_data_total_monthly &lt;- read.csv(&quot;~/Desktop/CH_data_total_monthly.csv&quot;, header = T, sep = &quot;,&quot;) CH_data_total_monthly_clean &lt;- CH_data_total_monthly[!duplicated(CH_data_total_monthly[c(&quot;Date_t&quot;, &quot;Ticker&quot;)]),] # Only get the respective columns CH_data_total_mont &lt;- CH_data_total_monthly_clean %&gt;% subset(Ticker %in% tic_sub) %&gt;% select(Date_t, Ticker, Adj_Close, Close, Num_Shares) # Control for extreme values CH_data_total_monthly_sub_Size &lt;- CH_data_total_mont %&gt;% arrange(Date_t) %&gt;% group_by(Ticker) %&gt;% mutate(Ret_Adj = (Adj_Close - lag(Adj_Close, 1))/lag(Adj_Close,1)) %&gt;% na.omit() %&gt;% ungroup() %&gt;% group_by(Date_t) %&gt;% mutate(Extreme_Values = quantile(Ret_Adj, p = 0.999), Extreme_Indicator = ifelse(Ret_Adj &lt;= Extreme_Values, 1, NA), Ret_Adj = Ret_Adj * Extreme_Indicator) %&gt;% ungroup() CH_data_total_monthly_sub_cumret_Size &lt;- CH_data_total_monthly_sub_Size %&gt;% group_by(Ticker) %&gt;% mutate(Shares_Out_lagged = lag(Num_Shares, n = 1), Price_close_lagged = lag(Close, n = 1), Market_Cap = Shares_Out_lagged * Price_close_lagged) %&gt;% na.omit() %&gt;% ungroup() # Assign rankings CH_data_total_monthly_sub_rank_Size &lt;- CH_data_total_monthly_sub_cumret_Size %&gt;% group_by(Date_t) %&gt;% # Here we form the groups (ranks) mutate(Decile = as.numeric(cut(Market_Cap, breaks = quantile(Market_Cap, probs = seq(0, 1, length = 11), na.rm = TRUE, type = 4), include.lowest = TRUE)), p1 = quantile(Market_Cap, 0.1), p2 = quantile(Market_Cap, 0.2), p3 = quantile(Market_Cap, 0.3), p4 = quantile(Market_Cap, 0.4), p5 = quantile(Market_Cap, 0.5), p6 = quantile(Market_Cap, 0.6), p7 = quantile(Market_Cap, 0.7), p8 = quantile(Market_Cap, 0.8), p9 = quantile(Market_Cap, 0.9)) %&gt;% ungroup() # Create mean returns CH_data_EW_Ret_Size &lt;- CH_data_total_monthly_sub_rank_Size %&gt;% group_by(Date_t, Decile) %&gt;% mutate(EW_Ret_mean_t = mean(Ret_Adj, na.rm = T), EW_Ret_sd_t = sd(Ret_Adj, na.rm = T)) %&gt;% select(Date_t,Decile,EW_Ret_mean_t,EW_Ret_sd_t) %&gt;% ungroup() %&gt;% unique() %&gt;% arrange(Date_t,Decile) CH_data_Cum_Ret_Size &lt;- CH_data_EW_Ret_Size %&gt;% group_by(Decile) %&gt;% mutate(cum_ret = cumprod(1+EW_Ret_mean_t)) EW_Size &lt;- CH_data_Cum_Ret_Size %&gt;% mutate(Decile = as.factor(Decile)) %&gt;% mutate(Date_t = as.Date(Date_t)) ggplot(data = EW_Size) + geom_line(mapping = aes(x =Date_t ,y = cum_ret ,color = Decile), size = 0.5 ) + ggtitle(&quot;Size Strategy with long formatted dataset&quot;) + ylab(&quot;Cumulative Returns&quot;) + xlab(&quot;Time&quot;) + theme(plot.title= element_text(size=14, color=&quot;grey26&quot;, hjust=0.5, lineheight=1.2), panel.background = element_rect(fill=&quot;#f7f7f7&quot;), panel.grid.major.y = element_line(size = 0.5, linetype = &quot;solid&quot;, color = &quot;grey&quot;), panel.grid.minor = element_blank(), panel.grid.major.x = element_blank(), plot.background = element_rect(fill=&quot;#f7f7f7&quot;, color = &quot;#f7f7f7&quot;), axis.title.x = element_text(color=&quot;grey26&quot;, size=12), axis.title.y = element_text(color=&quot;grey26&quot;, size=12), axis.line = element_line(color = &quot;black&quot;)) Finally, let’s calculate the summary statistics CH_data_EW_Ret_Size %&gt;% group_by(Decile) %&gt;% mutate(Annualised_Mean_Return = (1+mean(EW_Ret_mean_t))^12 -1, Annualised_SD = sd(EW_Ret_mean_t)*sqrt(12), Sharpe_Ratio = Annualised_Mean_Return/Annualised_SD) %&gt;% select(Decile, Annualised_Mean_Return, Annualised_SD, Sharpe_Ratio) %&gt;% unique() ## # A tibble: 10 × 4 ## # Groups: Decile [10] ## Decile Annualised_Mean_Return Annualised_SD Sharpe_Ratio ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 -0.0679 0.210 -0.324 ## 2 2 0.0332 0.171 0.194 ## 3 3 0.0488 0.162 0.302 ## 4 4 0.0627 0.165 0.381 ## 5 5 0.0750 0.177 0.423 ## 6 6 0.0555 0.161 0.346 ## 7 7 0.0780 0.160 0.489 ## 8 8 0.0609 0.171 0.356 ## 9 9 0.0889 0.180 0.492 ## 10 10 0.0857 0.174 0.492 6.5.4.3 The Value Factor This is the long-format construction of the value factor based on the data downloaded. # Let&#39;s run the experiment for the reduced data frame tic_sub &lt;- unique(names(read.csv(&quot;~/Desktop/Master UZH/Data/A4_dataset_04_Ex_Session.txt&quot;, header = T, sep = &quot;\\t&quot;))) # Get 347 companies CH_data_total_monthly &lt;- read.csv(&quot;~/Desktop/CH_data_total_monthly.csv&quot;, header = T, sep = &quot;,&quot;) CH_data_total_monthly_clean &lt;- CH_data_total_monthly[!duplicated(CH_data_total_monthly[c(&quot;Date_t&quot;, &quot;Ticker&quot;)]),] # Only get the respective columns CH_data_total_mont &lt;- CH_data_total_monthly_clean %&gt;% subset(Ticker %in% tic_sub) %&gt;% select(Date_t, Ticker, Adj_Close, Close, Num_Shares, Total_Assets_t, Total_Liab_t, Total_Eq_t) # Control for extreme values CH_data_total_monthly_sub_Value &lt;- CH_data_total_mont %&gt;% arrange(Date_t) %&gt;% group_by(Ticker) %&gt;% mutate(Ret_Adj = (Adj_Close - lag(Adj_Close, 1))/lag(Adj_Close,1)) %&gt;% na.omit() %&gt;% ungroup() %&gt;% group_by(Date_t) %&gt;% mutate(Extreme_Values = quantile(Ret_Adj, p = 0.999), Extreme_Indicator = ifelse(Ret_Adj &lt;= Extreme_Values, 1, NA), Ret_Adj = Ret_Adj * Extreme_Indicator) %&gt;% ungroup() CH_data_total_monthly_sub_cumret_Value &lt;- CH_data_total_monthly_sub_Value %&gt;% group_by(Ticker) %&gt;% mutate(Shares_Out_lagged = lag(Num_Shares, n = 1), Price_close_lagged = lag(Close, n = 1), Market_Cap = Shares_Out_lagged * Price_close_lagged, Total_Eq_t = Total_Assets_t - Total_Liab_t, B2M = lag(Total_Eq_t, n = 7) /Market_Cap) %&gt;% na.omit() %&gt;% ungroup() # Assign rankings CH_data_total_monthly_sub_rank_Value &lt;- CH_data_total_monthly_sub_cumret_Value %&gt;% group_by(Date_t) %&gt;% # Here we form the groups (ranks) mutate(Decile = as.numeric(cut(B2M, breaks = quantile(B2M, probs = seq(0, 1, length = 11), na.rm = TRUE, type = 4), include.lowest = TRUE)), p1 = quantile(B2M, 0.1), p2 = quantile(B2M, 0.2), p3 = quantile(B2M, 0.3), p4 = quantile(B2M, 0.4), p5 = quantile(B2M, 0.5), p6 = quantile(B2M, 0.6), p7 = quantile(B2M, 0.7), p8 = quantile(B2M, 0.8), p9 = quantile(B2M, 0.9)) %&gt;% ungroup() # Create mean returns CH_data_EW_Ret_Value &lt;- CH_data_total_monthly_sub_rank_Value %&gt;% group_by(Date_t, Decile) %&gt;% mutate(EW_Ret_mean_t = mean(Ret_Adj, na.rm = T), EW_Ret_sd_t = sd(Ret_Adj, na.rm = T)) %&gt;% select(Date_t,Decile,EW_Ret_mean_t,EW_Ret_sd_t) %&gt;% ungroup() %&gt;% unique() %&gt;% arrange(Date_t,Decile) CH_data_Cum_Ret_Value &lt;- CH_data_EW_Ret_Value %&gt;% group_by(Decile) %&gt;% mutate(cum_ret = cumprod(1+EW_Ret_mean_t)) EW_Value &lt;- CH_data_Cum_Ret_Value %&gt;% mutate(Decile = as.factor(Decile)) %&gt;% mutate(Date_t = as.Date(Date_t)) ggplot(data = EW_Value) + geom_line(mapping = aes(x =Date_t ,y = cum_ret ,color = Decile), size = 0.5 ) + ggtitle(&quot;Value Strategy with long formatted dataset&quot;) + ylab(&quot;Cumulative Returns&quot;) + xlab(&quot;Time&quot;) + theme(plot.title= element_text(size=14, color=&quot;grey26&quot;, hjust=0.5, lineheight=1.2), panel.background = element_rect(fill=&quot;#f7f7f7&quot;), panel.grid.major.y = element_line(size = 0.5, linetype = &quot;solid&quot;, color = &quot;grey&quot;), panel.grid.minor = element_blank(), panel.grid.major.x = element_blank(), plot.background = element_rect(fill=&quot;#f7f7f7&quot;, color = &quot;#f7f7f7&quot;), axis.title.x = element_text(color=&quot;grey26&quot;, size=12), axis.title.y = element_text(color=&quot;grey26&quot;, size=12), axis.line = element_line(color = &quot;black&quot;)) Finally, let’s calculate the summary statistics CH_data_EW_Ret_Value %&gt;% group_by(Decile) %&gt;% mutate(Annualised_Mean_Return = (1+mean(EW_Ret_mean_t))^12 -1, Annualised_SD = sd(EW_Ret_mean_t)*sqrt(12), Sharpe_Ratio = Annualised_Mean_Return/Annualised_SD) %&gt;% select(Decile, Annualised_Mean_Return, Annualised_SD, Sharpe_Ratio) %&gt;% unique() ## # A tibble: 10 × 4 ## # Groups: Decile [10] ## Decile Annualised_Mean_Return Annualised_SD Sharpe_Ratio ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 -0.00841 0.209 -0.0403 ## 2 2 0.0903 0.173 0.521 ## 3 3 0.0730 0.186 0.392 ## 4 4 0.0535 0.173 0.309 ## 5 5 0.0602 0.168 0.358 ## 6 6 0.0697 0.166 0.421 ## 7 7 0.0715 0.161 0.445 ## 8 8 0.0558 0.164 0.340 ## 9 9 0.0984 0.171 0.574 ## 10 10 0.0782 0.162 0.483 6.5.4.4 The Profitability Factor This is the long-format construction of the profitability factor based on the data downloaded. # Let&#39;s run the experiment for the reduced data frame tic_sub &lt;- unique(names(read.csv(&quot;~/Desktop/Master UZH/Data/A4_dataset_04_Ex_Session.txt&quot;, header = T, sep = &quot;\\t&quot;))) # Get 347 companies CH_data_total_monthly &lt;- read.csv(&quot;~/Desktop/CH_data_total_monthly.csv&quot;, header = T, sep = &quot;,&quot;) CH_data_total_monthly_clean &lt;- CH_data_total_monthly[!duplicated(CH_data_total_monthly[c(&quot;Date_t&quot;, &quot;Ticker&quot;)]),] # Only get the respective columns CH_data_total_mont &lt;- CH_data_total_monthly_clean %&gt;% subset(Ticker %in% tic_sub) %&gt;% select(Date_t, Ticker, Adj_Close, Gross_Inc_t, Operating_Exp_t, Interest_Exp_t, Total_Eq_t) # Control for extreme values CH_data_total_monthly_sub_Profit &lt;- CH_data_total_mont %&gt;% arrange(Date_t) %&gt;% group_by(Ticker) %&gt;% mutate(Ret_Adj = (Adj_Close - lag(Adj_Close, 1))/lag(Adj_Close,1)) %&gt;% na.omit() %&gt;% ungroup() %&gt;% group_by(Date_t) %&gt;% mutate(Extreme_Values = quantile(Ret_Adj, p = 0.999), Extreme_Indicator = ifelse(Ret_Adj &lt;= Extreme_Values, 1, NA), Ret_Adj = Ret_Adj * Extreme_Indicator) %&gt;% ungroup() CH_data_total_monthly_sub_cumret_Profit &lt;- CH_data_total_monthly_sub_Profit %&gt;% group_by(Ticker) %&gt;% mutate(Profit_lead = (Gross_Inc_t - Operating_Exp_t - Interest_Exp_t)/Total_Eq_t, Profit = lag(Profit_lead, n = 1)) %&gt;% na.omit() %&gt;% ungroup() # Assign rankings CH_data_total_monthly_sub_rank_Profit &lt;- CH_data_total_monthly_sub_cumret_Profit %&gt;% group_by(Date_t) %&gt;% # Here we form the groups (ranks) mutate(Decile = as.numeric(cut(Profit, breaks = quantile(Profit, probs = seq(0, 1, length = 11), na.rm = TRUE, type = 4), include.lowest = TRUE)), p1 = quantile(Profit, 0.1), p2 = quantile(Profit, 0.2), p3 = quantile(Profit, 0.3), p4 = quantile(Profit, 0.4), p5 = quantile(Profit, 0.5), p6 = quantile(Profit, 0.6), p7 = quantile(Profit, 0.7), p8 = quantile(Profit, 0.8), p9 = quantile(Profit, 0.9)) %&gt;% ungroup() # Create mean returns CH_data_EW_Ret_Profit &lt;- CH_data_total_monthly_sub_rank_Profit %&gt;% group_by(Date_t, Decile) %&gt;% mutate(EW_Ret_mean_t = mean(Ret_Adj, na.rm = T), EW_Ret_sd_t = sd(Ret_Adj, na.rm = T)) %&gt;% select(Date_t,Decile,EW_Ret_mean_t,EW_Ret_sd_t) %&gt;% ungroup() %&gt;% unique() %&gt;% arrange(Date_t,Decile) CH_data_Cum_Ret_Profit &lt;- CH_data_EW_Ret_Profit %&gt;% group_by(Decile) %&gt;% mutate(cum_ret = cumprod(1+EW_Ret_mean_t)) EW_Profit &lt;- CH_data_Cum_Ret_Profit %&gt;% mutate(Decile = as.factor(Decile)) %&gt;% mutate(Date_t = as.Date(Date_t)) ggplot(data = EW_Profit) + geom_line(mapping = aes(x =Date_t ,y = cum_ret ,color = Decile), size = 0.5 ) + ggtitle(&quot;Operating Profitability Strategy with long formatted dataset&quot;) + ylab(&quot;Cumulative Returns&quot;) + xlab(&quot;Time&quot;) + theme(plot.title= element_text(size=14, color=&quot;grey26&quot;, hjust=0.5, lineheight=1.2), panel.background = element_rect(fill=&quot;#f7f7f7&quot;), panel.grid.major.y = element_line(size = 0.5, linetype = &quot;solid&quot;, color = &quot;grey&quot;), panel.grid.minor = element_blank(), panel.grid.major.x = element_blank(), plot.background = element_rect(fill=&quot;#f7f7f7&quot;, color = &quot;#f7f7f7&quot;), axis.title.x = element_text(color=&quot;grey26&quot;, size=12), axis.title.y = element_text(color=&quot;grey26&quot;, size=12), axis.line = element_line(color = &quot;black&quot;)) Finally, let’s calculate the summary statistics CH_data_EW_Ret_Profit %&gt;% group_by(Decile) %&gt;% mutate(Annualised_Mean_Return = (1+mean(EW_Ret_mean_t))^12 -1, Annualised_SD = sd(EW_Ret_mean_t)*sqrt(12), Sharpe_Ratio = Annualised_Mean_Return/Annualised_SD) %&gt;% select(Decile, Annualised_Mean_Return, Annualised_SD, Sharpe_Ratio) %&gt;% unique() ## # A tibble: 10 × 4 ## # Groups: Decile [10] ## Decile Annualised_Mean_Return Annualised_SD Sharpe_Ratio ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 -0.0411 0.227 -0.181 ## 2 2 0.0256 0.191 0.134 ## 3 3 0.0100 0.221 0.0455 ## 4 4 0.0676 0.201 0.336 ## 5 5 0.0469 0.194 0.241 ## 6 6 0.0434 0.186 0.234 ## 7 7 0.0893 0.163 0.547 ## 8 8 0.0763 0.186 0.411 ## 9 9 0.0793 0.175 0.453 ## 10 10 0.0565 0.147 0.386 6.5.4.5 The Investment Factor This is the long-format construction of the investment factor based on the data downloaded. # Let&#39;s run the experiment for the reduced data frame tic_sub &lt;- unique(names(read.csv(&quot;~/Desktop/Master UZH/Data/A4_dataset_04_Ex_Session.txt&quot;, header = T, sep = &quot;\\t&quot;))) # Get 347 companies CH_data_total_monthly &lt;- read.csv(&quot;~/Desktop/CH_data_total_monthly.csv&quot;, header = T, sep = &quot;,&quot;) CH_data_total_monthly_clean &lt;- CH_data_total_monthly[!duplicated(CH_data_total_monthly[c(&quot;Date_t&quot;, &quot;Ticker&quot;)]),] # Only get the respective columns CH_data_total_mont &lt;- CH_data_total_monthly_clean %&gt;% subset(Ticker %in% tic_sub) %&gt;% select(Date_t, Ticker, Adj_Close, Total_Assets_t) # Control for extreme values CH_data_total_monthly_sub_Invest &lt;- CH_data_total_mont %&gt;% arrange(Date_t) %&gt;% group_by(Ticker) %&gt;% mutate(Ret_Adj = (Adj_Close - lag(Adj_Close, 1))/lag(Adj_Close,1)) %&gt;% na.omit() %&gt;% ungroup() %&gt;% group_by(Date_t) %&gt;% mutate(Extreme_Values = quantile(Ret_Adj, p = 0.999), Extreme_Indicator = ifelse(Ret_Adj &lt;= Extreme_Values, 1, NA), Ret_Adj = Ret_Adj * Extreme_Indicator) %&gt;% ungroup() CH_data_total_monthly_sub_cumret_Invest &lt;- CH_data_total_monthly_sub_Invest %&gt;% group_by(Ticker) %&gt;% mutate(Invest_lead = (Total_Assets_t - lag(Total_Assets_t, n = 12))/lag(Total_Assets_t, n = 12), Invest = lag(Invest_lead, n = 1)) %&gt;% na.omit() %&gt;% ungroup() # Assign rankings CH_data_total_monthly_sub_rank_Invest &lt;- CH_data_total_monthly_sub_cumret_Invest %&gt;% group_by(Date_t) %&gt;% # Here we form the groups (ranks) mutate(Decile = as.numeric(cut(Invest, breaks = quantile(Invest, probs = seq(0, 1, length = 11), na.rm = TRUE, type = 4), include.lowest = TRUE)), p1 = quantile(Invest, 0.1), p2 = quantile(Invest, 0.2), p3 = quantile(Invest, 0.3), p4 = quantile(Invest, 0.4), p5 = quantile(Invest, 0.5), p6 = quantile(Invest, 0.6), p7 = quantile(Invest, 0.7), p8 = quantile(Invest, 0.8), p9 = quantile(Invest, 0.9)) %&gt;% ungroup() # Create mean returns CH_data_EW_Ret_Invest &lt;- CH_data_total_monthly_sub_rank_Invest %&gt;% group_by(Date_t, Decile) %&gt;% mutate(EW_Ret_mean_t = mean(Ret_Adj, na.rm = T), EW_Ret_sd_t = sd(Ret_Adj, na.rm = T)) %&gt;% select(Date_t,Decile,EW_Ret_mean_t,EW_Ret_sd_t) %&gt;% ungroup() %&gt;% unique() %&gt;% arrange(Date_t,Decile) CH_data_Cum_Ret_Invest &lt;- CH_data_EW_Ret_Invest %&gt;% group_by(Decile) %&gt;% mutate(cum_ret = cumprod(1+EW_Ret_mean_t)) EW_Invest &lt;- CH_data_Cum_Ret_Invest %&gt;% mutate(Decile = as.factor(Decile)) %&gt;% mutate(Date_t = as.Date(Date_t)) ggplot(data = EW_Invest) + geom_line(mapping = aes(x =Date_t ,y = cum_ret ,color = Decile), size = 0.5 ) + ggtitle(&quot;Investment Growth Strategy with long formatted dataset&quot;) + ylab(&quot;Cumulative Returns&quot;) + xlab(&quot;Time&quot;) + theme(plot.title= element_text(size=14, color=&quot;grey26&quot;, hjust=0.5, lineheight=1.2), panel.background = element_rect(fill=&quot;#f7f7f7&quot;), panel.grid.major.y = element_line(size = 0.5, linetype = &quot;solid&quot;, color = &quot;grey&quot;), panel.grid.minor = element_blank(), panel.grid.major.x = element_blank(), plot.background = element_rect(fill=&quot;#f7f7f7&quot;, color = &quot;#f7f7f7&quot;), axis.title.x = element_text(color=&quot;grey26&quot;, size=12), axis.title.y = element_text(color=&quot;grey26&quot;, size=12), axis.line = element_line(color = &quot;black&quot;)) Finally, let’s calculate the summary statistics CH_data_EW_Ret_Invest %&gt;% group_by(Decile) %&gt;% mutate(Annualised_Mean_Return = (1+mean(EW_Ret_mean_t))^12 -1, Annualised_SD = sd(EW_Ret_mean_t)*sqrt(12), Sharpe_Ratio = Annualised_Mean_Return/Annualised_SD) %&gt;% select(Decile, Annualised_Mean_Return, Annualised_SD, Sharpe_Ratio) %&gt;% unique() ## # A tibble: 10 × 4 ## # Groups: Decile [10] ## Decile Annualised_Mean_Return Annualised_SD Sharpe_Ratio ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 -0.115 0.233 -0.494 ## 2 2 -0.0110 0.191 -0.0576 ## 3 3 0.0423 0.184 0.230 ## 4 4 0.0699 0.157 0.445 ## 5 5 0.0578 0.153 0.377 ## 6 6 0.0885 0.164 0.539 ## 7 7 0.122 0.145 0.842 ## 8 8 0.112 0.159 0.705 ## 9 9 0.150 0.164 0.913 ## 10 10 0.148 0.183 0.809 6.5.4.6 The Betting against Beta Factor # Let&#39;s run the experiment for the reduced data frame tic_sub &lt;- unique(names(read.csv(&quot;~/Desktop/Master UZH/Data/A4_dataset_04_Ex_Session.txt&quot;, header = T, sep = &quot;\\t&quot;))) # Get 347 companies CH_data_total_monthly &lt;- read.csv(&quot;~/Desktop/CH_data_total_monthly.csv&quot;, header = T, sep = &quot;,&quot;) CH_data_total_monthly_clean &lt;- CH_data_total_monthly[!duplicated(CH_data_total_monthly[c(&quot;Date_t&quot;, &quot;Ticker&quot;)]),] # Only get the respective columns CH_data_total_mont &lt;- CH_data_total_monthly_clean %&gt;% subset(Ticker %in% tic_sub) %&gt;% select(Date_t, Ticker, Adj_Close, Total_Assets_t) # Control for extreme values CH_data_total_monthly_sub_BAB &lt;- CH_data_total_mont %&gt;% arrange(Date_t) %&gt;% group_by(Ticker) %&gt;% mutate(Ret_Adj = (Adj_Close - lag(Adj_Close, 1))/lag(Adj_Close,1)) %&gt;% na.omit() %&gt;% ungroup() %&gt;% group_by(Date_t) %&gt;% mutate(Extreme_Values = quantile(Ret_Adj, p = 1), Extreme_Indicator = ifelse(Ret_Adj &lt;= Extreme_Values, 1, NA), Ret_Adj = Ret_Adj * Extreme_Indicator) %&gt;% ungroup() # Let&#39;s also get the data on the Swiss Market Index as well as the risk free rate rf &lt;- read.csv(&quot;~/Desktop/Master UZH/Data/A2_dataset_02_Ex_Session.txt&quot;, header = T, sep = &quot;\\t&quot;) rf_ts &lt;- rf %&gt;% subset((Date &gt;= &#39;1989-12-01&#39;) &amp; (Date &lt;= &#39;2019-12-31&#39;)) %&gt;% mutate(rf_annual = SWISS.CONFEDERATION.BOND.1.YEAR...RED..YIELD / 1000, rf_monthly = (1 + rf_annual)^(1/12) - 1) %&gt;% select(Date, rf_monthly) SMI &lt;- read.csv(&quot;~/Desktop/Master UZH/Data/A2_dataset_03_Ex_Session.txt&quot;, header = T, sep = &quot;\\t&quot;) SMI_ts &lt;- SMI %&gt;% subset((Date &gt;= &#39;1989-12-01&#39;) &amp; (Date &lt;= &#39;2019-12-31&#39;)) %&gt;% mutate(SMI_ret = (SMI - lag(SMI, 1))/lag(SMI, 1), rf_monthly = rf_ts$rf_monthly, SMI_rf_excess_ret = SMI_ret - rf_monthly) # Now, let&#39;s calculate the excess returns SMI_rf_ts_ret &lt;- SMI_ts_ret - rf_ts_monthly CH_data_total_monthly_sub_excess_ret_BAB &lt;- left_join(SMI_ts, CH_data_total_monthly_sub_BAB, by = c(&quot;Date&quot; = &quot;Date_t&quot;)) %&gt;% mutate(Excess_Ret_Adj = Ret_Adj - rf_monthly) # Based on this, we can now have rolling regressions of the correlation as well as the standard deviations. CH_data_total_monthly_weighted_BAB &lt;- CH_data_total_monthly_sub_excess_ret_BAB %&gt;% group_by(Ticker) %&gt;% mutate(sd_roll_1_year = roll_sd(Excess_Ret_Adj, width = 12), corr_roll_5_year = roll_cor(Excess_Ret_Adj, SMI_rf_excess_ret, width = 60), sd_roll_1_year_Market = roll_sd(SMI_rf_excess_ret, width = 12), Beta_Est_raw = sd_roll_1_year/sd_roll_1_year_Market*corr_roll_5_year, Beta_Est_Adj = Beta_Est_raw*0.4 + 0.6*1) %&gt;% ungroup() %&gt;% subset(!is.na(Beta_Est_Adj)) %&gt;% group_by(Date) %&gt;% mutate(ranks_beta = rank(Beta_Est_Adj), mean_rank_beta = mean(ranks_beta, na.rm = T), portfolio_indicator_BAB =ifelse(ranks_beta &gt; mean_rank_beta, 1, 0)) %&gt;% ungroup() %&gt;% group_by(Date, portfolio_indicator_BAB) %&gt;% mutate(abs_dev = abs(ranks_beta - mean_rank_beta), sum_abs_dev = sum(abs_dev, na.rm = T), k = 2 / sum_abs_dev, weights_BAB = k*(ranks_beta - mean_rank_beta), Beta_Est_Weight_Adj = Beta_Est_Adj * weights_BAB) %&gt;% ungroup() # Assign rankings CH_data_total_monthly_rank_BAB &lt;- CH_data_total_monthly_weighted_BAB %&gt;% group_by(Date) %&gt;% mutate(Min_PF = n()) %&gt;% subset(Min_PF &gt;= 10) %&gt;% mutate(Decile = as.numeric(cut(Beta_Est_Weight_Adj, breaks = quantile(Beta_Est_Weight_Adj, probs = seq(0, 1, length = 10), na.rm = TRUE, type = 4), include.lowest = TRUE)), p1 = quantile(Beta_Est_Weight_Adj, 0.1), p2 = quantile(Beta_Est_Weight_Adj, 0.2), p3 = quantile(Beta_Est_Weight_Adj, 0.3), p4 = quantile(Beta_Est_Weight_Adj, 0.4), p5 = quantile(Beta_Est_Weight_Adj, 0.5), p6 = quantile(Beta_Est_Weight_Adj, 0.6), p7 = quantile(Beta_Est_Weight_Adj, 0.7), p8 = quantile(Beta_Est_Weight_Adj, 0.8), p9 = quantile(Beta_Est_Weight_Adj, 0.9)) %&gt;% ungroup() # Create mean returns Decile CH_data_EW_Ret_BAB &lt;- CH_data_total_monthly_rank_BAB %&gt;% group_by(Date, Decile) %&gt;% mutate(EW_Ret_mean_t = 1/sum(Beta_Est_Weight_Adj)*sum(weights_BAB*Excess_Ret_Adj)) %&gt;% select(Date, Decile, EW_Ret_mean_t) %&gt;% ungroup() %&gt;% unique() %&gt;% arrange(Date, Decile) # Create the cumulative returns CH_data_Cum_Ret_BAB &lt;- CH_data_EW_Ret_BAB %&gt;% group_by(Decile) %&gt;% mutate(cum_ret = cumprod(1+EW_Ret_mean_t)) EW_BAB &lt;- CH_data_Cum_Ret_BAB %&gt;% mutate(Decile = as.factor(Decile)) %&gt;% mutate(Date = as.Date(Date)) EW_BAB %&gt;% subset(Decile != 5) %&gt;% ggplot() + geom_line(mapping = aes(x =Date ,y = cum_ret ,color = Decile), size = 0.5) + ggtitle(&quot;BAB Strategy with long formatted dataset&quot;) + ylab(&quot;Cumulative Returns&quot;) + xlab(&quot;Time&quot;) + theme(plot.title= element_text(size=14, color=&quot;grey26&quot;, hjust=0.5, lineheight=1.2), panel.background = element_rect(fill=&quot;#f7f7f7&quot;), panel.grid.major.y = element_line(size = 0.5, linetype = &quot;solid&quot;, color = &quot;grey&quot;), panel.grid.minor = element_blank(), panel.grid.major.x = element_blank(), plot.background = element_rect(fill=&quot;#f7f7f7&quot;, color = &quot;#f7f7f7&quot;), axis.title.x = element_text(color=&quot;grey26&quot;, size=12), axis.title.y = element_text(color=&quot;grey26&quot;, size=12), axis.line = element_line(color = &quot;black&quot;)) Finally, let’s calculate the summary statistics CH_data_EW_Ret_BAB %&gt;% group_by(Decile) %&gt;% mutate(Annualised_Mean_Return = (1+mean(EW_Ret_mean_t))^12 -1, Annualised_SD = sd(EW_Ret_mean_t)*sqrt(12), Sharpe_Ratio = Annualised_Mean_Return/Annualised_SD) %&gt;% select(Decile, Annualised_Mean_Return, Annualised_SD, Sharpe_Ratio) %&gt;% unique() ## # A tibble: 9 × 4 ## # Groups: Decile [9] ## Decile Annualised_Mean_Return Annualised_SD Sharpe_Ratio ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.0595 0.147 0.405 ## 2 2 0.0522 0.147 0.356 ## 3 3 0.134 0.199 0.674 ## 4 4 0.114 0.185 0.616 ## 5 5 -0.234 3.46 -0.0676 ## 6 6 0.0773 0.224 0.346 ## 7 7 0.0857 0.216 0.397 ## 8 8 0.0611 0.227 0.269 ## 9 9 0.0435 0.266 0.163 6.5.5 Double-sorted Portfolios We now have created the portfolio sorts for each of the factors under consideration and plotted the cumulative returns of each decile throughout time. Let’s now get one step further and calculate double-sorted portfolios based on the company size as well as the respective factor characteristic under consideration. Throughout, we will follow the approach of Fama and French (1993, 2015). That is, we will use the formulas specifically highlighted and used by both researchers during the construction for the US market and replicate their strategy for the Swiss market. The repsective factors can be found on Kenneth French’s webpage. 6.5.5.1 Small Minus Big For the SMB factor, we will use an aggregate of double-sorted portfolios based on size as well as Value, Profitability and Investment, respectively. That is, we will double-sort the stocks based on size and each of the three distinct accounting measures and then create a long-short strategy for each double sort. Lastly, we will aggregate the three risk factors by creating an equal-weighted SMB factor consisting of all three sub-factors. # Let&#39;s run the experiment for the reduced data frame tic_sub &lt;- unique(names(read.csv(&quot;~/Desktop/Master UZH/Data/A4_dataset_04_Ex_Session.txt&quot;, header = T, sep = &quot;\\t&quot;))) # Get 347 companies CH_data_total_monthly &lt;- read.csv(&quot;~/Desktop/CH_data_total_monthly.csv&quot;, header = T, sep = &quot;,&quot;) CH_data_total_monthly_clean &lt;- CH_data_total_monthly[!duplicated(CH_data_total_monthly[c(&quot;Date_t&quot;, &quot;Ticker&quot;)]),] # Only get the respective columns CH_data_total_mont &lt;- CH_data_total_monthly_clean %&gt;% subset(Ticker %in% tic_sub) %&gt;% select(Date_t, Ticker, Adj_Close, Close, Num_Shares, Total_Eq_t, Gross_Inc_t, Operating_Exp_t, Interest_Exp_t, Total_Assets_t, Total_Liab_t) # Control for extreme values CH_data_total_monthly_sub_Size &lt;- CH_data_total_mont %&gt;% arrange(Date_t) %&gt;% group_by(Ticker) %&gt;% mutate(Ret_Adj = (Adj_Close - lag(Adj_Close, 1))/lag(Adj_Close,1)) %&gt;% na.omit() %&gt;% ungroup() %&gt;% group_by(Date_t) %&gt;% mutate(Extreme_Values = quantile(Ret_Adj, p = 1), Extreme_Indicator = ifelse(Ret_Adj &lt;= Extreme_Values, 1, NA), Ret_Adj = Ret_Adj * Extreme_Indicator) %&gt;% ungroup() # Create the variables for Size and Value CH_data_total_monthly_sub_cumret_Size_Value &lt;- CH_data_total_monthly_sub_Size %&gt;% group_by(Ticker) %&gt;% mutate(# Create the Market Cap (Size) characteristic Shares_Out_lagged = lag(Num_Shares, n = 1), Price_close_lagged = lag(Close, n = 1), Market_Cap = Shares_Out_lagged * Price_close_lagged, Total_Eq_t = Total_Assets_t - Total_Liab_t, # Create the B2M (Value) characteristic B2M = lag(Total_Eq_t, n = 7) / Market_Cap) %&gt;% na.omit() %&gt;% ungroup() # Create the variables for Size and Operating Profitability CH_data_total_monthly_sub_cumret_Size_Profit &lt;- CH_data_total_monthly_sub_Size %&gt;% group_by(Ticker) %&gt;% mutate(# Create the Market Cap (Size) characteristic Shares_Out_lagged = lag(Num_Shares, n = 1), Price_close_lagged = lag(Close, n = 1), Market_Cap = Shares_Out_lagged * Price_close_lagged, # Create the B2M (Value) characteristic Profit_lead = (Gross_Inc_t - Operating_Exp_t - Interest_Exp_t)/Total_Eq_t, Profit = lag(Profit_lead, n = 1)) %&gt;% na.omit() %&gt;% ungroup() # Create the variables for Size and Investment Growth CH_data_total_monthly_sub_cumret_Size_Invest &lt;- CH_data_total_monthly_sub_Size %&gt;% group_by(Ticker) %&gt;% mutate(# Create the Market Cap (Size) characteristic Shares_Out_lagged = lag(Num_Shares, n = 1), Price_close_lagged = lag(Close, n = 1), Market_Cap = Shares_Out_lagged * Price_close_lagged, # Create the B2M (Value) characteristic Invest_lead = (Total_Assets_t - lag(Total_Assets_t, n = 12))/lag(Total_Assets_t, n = 12), Invest = lag(Invest_lead, n = 1)) %&gt;% na.omit() %&gt;% ungroup() # Assign Rankings for Size and Value CH_data_total_monthly_sub_rank_Size_Value &lt;- CH_data_total_monthly_sub_cumret_Size_Value %&gt;% group_by(Date_t) %&gt;% # Here we form the groups (ranks) mutate(# Create Small and Big Rank Size_Ranks = as.numeric(cut(Market_Cap, breaks = quantile(Market_Cap, probs= 0:2/2, na.rm = TRUE, type = 4), include.lowest = TRUE)), # Create Terciles based on B2M (Low, Neutral, High) Ranks = as.numeric(cut(B2M, breaks = quantile(B2M, probs= seq(0, 1, length = 11), na.rm = TRUE, type = 4), include.lowest = TRUE)), Value_Ranks = ifelse(Ranks &lt;= 3, 1, ifelse(Ranks &gt; 3 &amp; Ranks &lt;= 7, 2, ifelse(Ranks &gt; 7, 3, 0))) ) %&gt;% ungroup() # Assign Rankings for Size and Operating Profitability CH_data_total_monthly_sub_rank_Size_Profit &lt;- CH_data_total_monthly_sub_cumret_Size_Profit %&gt;% group_by(Date_t) %&gt;% # Here we form the groups (ranks) mutate(# Create Small and Big Rank Size_Ranks = as.numeric(cut(Market_Cap, breaks = quantile(Market_Cap, probs= 0:2/2, na.rm = TRUE, type = 4), include.lowest = TRUE)), # Create Terciles based on Oper Profit (Low, Neutral, High) Ranks = as.numeric(cut(Profit, breaks = quantile(Profit, probs= seq(0, 1, length = 11), na.rm = TRUE, type = 4), include.lowest = TRUE)), Profit_Ranks = ifelse(Ranks &lt;= 3, 1, ifelse(Ranks &gt; 3 &amp; Ranks &lt;= 7, 2, ifelse(Ranks &gt; 7, 3, 0))) ) %&gt;% ungroup() # Assign Rankings for Size and Investment Growth CH_data_total_monthly_sub_rank_Size_Invest &lt;- CH_data_total_monthly_sub_cumret_Size_Invest %&gt;% group_by(Date_t) %&gt;% # Here we form the groups (ranks) mutate(# Create Small and Big Rank Size_Ranks = as.numeric(cut(Market_Cap, breaks = quantile(Market_Cap, probs= 0:2/2, na.rm = TRUE, type = 4), include.lowest = TRUE)), # Create Terciles based on Investment Growth (Low, Neutral, High) Ranks = as.numeric(cut(Invest, breaks = quantile(Invest, probs= seq(0, 1, length = 11), na.rm = TRUE, type = 4), include.lowest = TRUE)), Invest_Ranks = ifelse(Ranks &lt;= 3, 1, ifelse(Ranks &gt; 3 &amp; Ranks &lt;= 7, 2, ifelse(Ranks &gt; 7, 3, 0))) ) %&gt;% ungroup() # Create the average returns per group for Size and Value CH_data_EW_Ret_Size_Value &lt;- CH_data_total_monthly_sub_rank_Size_Value %&gt;% group_by(Date_t, Size_Ranks, Value_Ranks) %&gt;% mutate(EW_Ret_mean_t_Size_Value = mean(Ret_Adj, na.rm = T), EW_Ret_sd_t_Size_Value = sd(Ret_Adj, na.rm = T)) %&gt;% select(Date_t,Size_Ranks, Value_Ranks, EW_Ret_mean_t_Size_Value,EW_Ret_sd_t_Size_Value) %&gt;% ungroup() %&gt;% unique() %&gt;% arrange(Date_t,Size_Ranks, Value_Ranks) %&gt;% group_by(Date_t, Size_Ranks) %&gt;% mutate(Large_Value = ifelse(Size_Ranks == 2, mean(EW_Ret_mean_t_Size_Value), 0), Small_Value = ifelse(Size_Ranks == 1, mean(EW_Ret_mean_t_Size_Value), 0)) %&gt;% ungroup() %&gt;% select(Date_t, Large_Value, Small_Value) %&gt;% unique() %&gt;% group_by(Date_t) %&gt;% mutate(SMB_BM = sum(Large_Value) - sum(Small_Value)) %&gt;% select(Date_t, SMB_BM) %&gt;% unique() # Create the average returns per group for Size and Operating Profitability CH_data_EW_Ret_Size_Profit &lt;- CH_data_total_monthly_sub_rank_Size_Profit %&gt;% group_by(Date_t, Size_Ranks, Profit_Ranks) %&gt;% mutate(EW_Ret_mean_t_Size_Profit = mean(Ret_Adj, na.rm = T), EW_Ret_sd_t_Size_Profit = sd(Ret_Adj, na.rm = T)) %&gt;% select(Date_t,Size_Ranks, Profit_Ranks, EW_Ret_mean_t_Size_Profit,EW_Ret_sd_t_Size_Profit) %&gt;% ungroup() %&gt;% unique() %&gt;% arrange(Date_t,Size_Ranks, Profit_Ranks) %&gt;% group_by(Date_t, Size_Ranks) %&gt;% mutate(Large_OP = ifelse(Size_Ranks == 2, mean(EW_Ret_mean_t_Size_Profit), 0), Small_OP = ifelse(Size_Ranks == 1, mean(EW_Ret_mean_t_Size_Profit), 0)) %&gt;% ungroup() %&gt;% select(Date_t, Large_OP, Small_OP) %&gt;% unique() %&gt;% group_by(Date_t) %&gt;% mutate(SMB_OP = sum(Large_OP) - sum(Small_OP)) %&gt;% select(Date_t, SMB_OP) %&gt;% unique() # Create the average returns per group for Size and Investment Growth CH_data_EW_Ret_Size_Invest &lt;- CH_data_total_monthly_sub_rank_Size_Invest %&gt;% group_by(Date_t, Size_Ranks, Invest_Ranks) %&gt;% mutate(EW_Ret_mean_t_Size_Invest = mean(Ret_Adj, na.rm = T), EW_Ret_sd_t_Size_Invest = sd(Ret_Adj, na.rm = T)) %&gt;% select(Date_t,Size_Ranks, Invest_Ranks, EW_Ret_mean_t_Size_Invest,EW_Ret_sd_t_Size_Invest) %&gt;% ungroup() %&gt;% unique() %&gt;% arrange(Date_t,Size_Ranks, Invest_Ranks) %&gt;% group_by(Date_t, Size_Ranks) %&gt;% mutate(Large_INV = ifelse(Size_Ranks == 2, mean(EW_Ret_mean_t_Size_Invest), 0), Small_INV = ifelse(Size_Ranks == 1, mean(EW_Ret_mean_t_Size_Invest), 0)) %&gt;% ungroup() %&gt;% select(Date_t, Large_INV, Small_INV) %&gt;% unique() %&gt;% group_by(Date_t) %&gt;% mutate(SMB_INV = sum(Large_INV) - sum(Small_INV)) %&gt;% select(Date_t, SMB_INV) %&gt;% unique() # Combine the three individual factors ## Create three xts objects CH_data_EW_Ret_Size_Value_ts &lt;- xts(CH_data_EW_Ret_Size_Value[,-1], order.by = as.Date(CH_data_EW_Ret_Size_Value$Date_t)) CH_data_EW_Ret_Size_Profit_ts &lt;- xts(CH_data_EW_Ret_Size_Profit[,-1], order.by = as.Date(CH_data_EW_Ret_Size_Profit$Date_t)) CH_data_EW_Ret_Size_Invest_ts &lt;- xts(CH_data_EW_Ret_Size_Invest[,-1], order.by = as.Date(CH_data_EW_Ret_Size_Invest$Date_t)) SMB &lt;- merge.xts(CH_data_EW_Ret_Size_Value_ts, CH_data_EW_Ret_Size_Profit_ts, CH_data_EW_Ret_Size_Invest_ts) SMB$SMB &lt;- rowMeans(SMB, na.rm = T) cumprod(1+na.omit(SMB[&quot;1991-02-28/2020-12-31&quot;])) %&gt;% tidy() %&gt;% ggplot(aes(x =index ,y = value ,color = series)) + geom_line( size = 0.5 ) + ggtitle(&quot;Size Strategy with long formatted dataset&quot;) + ylab(&quot;Cumulative Returns&quot;) + xlab(&quot;Time&quot;) + theme(plot.title= element_text(size=14, color=&quot;grey26&quot;, hjust=0.5, lineheight=1.2), panel.background = element_rect(fill=&quot;#f7f7f7&quot;), panel.grid.major.y = element_line(size = 0.5, linetype = &quot;solid&quot;, color = &quot;grey&quot;), panel.grid.minor = element_blank(), panel.grid.major.x = element_blank(), plot.background = element_rect(fill=&quot;#f7f7f7&quot;, color = &quot;#f7f7f7&quot;), axis.title.x = element_text(color=&quot;grey26&quot;, size=12), axis.title.y = element_text(color=&quot;grey26&quot;, size=12), axis.line = element_line(color = &quot;black&quot;)) As we can see, the SMB factor has declining cumulative returns throughout time, implying that there appears to be no market anomaly related to this specific factor. Finally, let’s calculate the summary statistics # Calculate the three performance metrics Annualised_Mean_Return &lt;- Return.annualized(SMB) Annualised_SD &lt;- sd.annualized(SMB) Sharpe_Ratio &lt;- Annualised_Mean_Return/Annualised_SD # Put it together df_SMB &lt;- as.data.frame(rbind(Annualised_Mean_Return, Annualised_SD, Sharpe_Ratio)) rownames(df_SMB) &lt;- c(&quot;Annualised Return&quot;, &quot;Standard Deviation&quot;, &quot;Sharpe Ratio&quot;) df_SMB ## SMB_BM SMB_OP SMB_INV SMB ## Annualised Return -0.03587035 -0.05180128 -0.05900121 -0.04904802 ## Standard Deviation 0.11646017 0.10833291 0.11875724 0.10969154 ## Sharpe Ratio -0.30800529 -0.47816754 -0.49682205 -0.44714498 6.5.5.2 High Minus Low, Robust Minus Weak and Conservative Minus Aggressive Having created the SMB factor, let’s now replicate the HML, RMW as well as CMA factors. We will follow the same approach as Fama and French here again. # Load the old dataset for book value Book &lt;- read.csv(&quot;~/Desktop/Master UZH/Data/A4_dataset_02_Ex_Session.txt&quot;, header = T, sep = &quot;\\t&quot;) Book_check &lt;- Book %&gt;% gather(Ticker,value,NESN:X692395) %&gt;% mutate(Date_t = lubridate::ceiling_date(dmy(Date), &quot;month&quot;) - 1) %&gt;% select(-Date) colnames(Book_check) &lt;- c(&quot;Ticker&quot;, &quot;Total_Eq_Real&quot;, &quot;Date_t&quot;) # Only sub select the companies under consideration tic_sub &lt;- unique(names(read.csv(&quot;~/Desktop/Master UZH/Data/A4_dataset_04_Ex_Session.txt&quot;, header = T, sep = &quot;\\t&quot;))) # Get 347 companies CH_data_total_monthly &lt;- read.csv(&quot;~/Desktop/CH_data_total_monthly.csv&quot;, header = T, sep = &quot;,&quot;) CH_data_total_monthly_clean &lt;- CH_data_total_monthly[!duplicated(CH_data_total_monthly[c(&quot;Date_t&quot;, &quot;Ticker&quot;)]),] # Only get the respective columns CH_data_total_mont &lt;- CH_data_total_monthly_clean %&gt;% subset(Ticker %in% tic_sub) %&gt;% select(Date_t, Ticker, Adj_Close, Close, Num_Shares, Total_Eq_t, Gross_Inc_t, Operating_Exp_t, Interest_Exp_t, Total_Assets_t, Total_Liab_t) %&gt;% mutate(Total_Eq_t = Total_Assets_t - Total_Liab_t, Num_Shares = Num_Shares, Date_t = as.Date(Date_t)) %&gt;% group_by(Ticker) %&gt;% mutate(Close = lag(Close, n=1), Num_Shares = lag(Num_Shares, 1)) %&gt;% ungroup() CH_data_total_mont_check &lt;- left_join(CH_data_total_mont, Book_check, by = c(&quot;Ticker&quot; = &quot;Ticker&quot;, &quot;Date_t&quot; = &quot;Date_t&quot;)) # Control for extreme values CH_data_total_monthly_sub_Size &lt;- CH_data_total_mont_check %&gt;% arrange(Date_t) %&gt;% group_by(Ticker) %&gt;% mutate(Ret_Adj = (Adj_Close - lag(Adj_Close, 1))/lag(Adj_Close,1)) %&gt;% na.omit() %&gt;% ungroup() %&gt;% group_by(Date_t) %&gt;% mutate(Extreme_Values = quantile(Ret_Adj, p = 1), Extreme_Indicator = ifelse(Ret_Adj &lt;= Extreme_Values, 1, NA), Ret_Adj = Ret_Adj * Extreme_Indicator) %&gt;% ungroup() # Create the variables for Size and Value CH_data_total_monthly_sub_cumret_Size_Value &lt;- CH_data_total_monthly_sub_Size %&gt;% group_by(Ticker) %&gt;% mutate(# Create the Market Cap (Size) characteristic Shares_Out_lagged = lag(Num_Shares, n = 1), Price_close_lagged = lag(Close, n = 1), Market_Cap = Shares_Out_lagged * Price_close_lagged, # Create the B2M (Value) characteristic Total_Eq_t = Total_Assets_t - Total_Liab_t, B2M = lag(Total_Eq_t, n = 6) / Market_Cap) %&gt;% na.omit() %&gt;% ungroup() # Create the variables for Size and old Value CH_data_total_monthly_sub_cumret_Size_Value_old &lt;- CH_data_total_monthly_sub_Size %&gt;% group_by(Ticker) %&gt;% mutate(# Create the Market Cap (Size) characteristic Shares_Out_lagged = lag(Num_Shares, n = 1), Price_close_lagged = lag(Close, n = 1), Market_Cap = Shares_Out_lagged * Price_close_lagged, # Create the B2M (Value) characteristic B2M_old = lag(Total_Eq_Real, n = 6) / Market_Cap) %&gt;% na.omit() %&gt;% ungroup() # Create the variables for Size and Operating Profitability CH_data_total_monthly_sub_cumret_Size_Profit &lt;- CH_data_total_monthly_sub_Size %&gt;% group_by(Ticker) %&gt;% mutate(# Create the Market Cap (Size) characteristic Shares_Out_lagged = lag(Num_Shares, n = 1), Price_close_lagged = lag(Close, n = 1), Market_Cap = Shares_Out_lagged * Price_close_lagged, # Create the B2M (Value) characteristic Profit_lead = (Gross_Inc_t - Operating_Exp_t - Interest_Exp_t)/Total_Eq_t, Profit = lag(Profit_lead, n = 1)) %&gt;% na.omit() %&gt;% ungroup() # Create the variables for Size and Investment Growth CH_data_total_monthly_sub_cumret_Size_Invest &lt;- CH_data_total_monthly_sub_Size %&gt;% group_by(Ticker) %&gt;% mutate(# Create the Market Cap (Size) characteristic Shares_Out_lagged = lag(Num_Shares, n = 1), Price_close_lagged = lag(Close, n = 1), Market_Cap = Shares_Out_lagged * Price_close_lagged, # Create the B2M (Value) characteristic Invest_lead = (lag(Total_Assets_t, n =1) - lag(Total_Assets_t, n = 12))/lag(Total_Assets_t, n = 12), Invest = lag(Invest_lead, n = 1)) %&gt;% na.omit() %&gt;% ungroup() # Assign Rankings for Size and Value CH_data_total_monthly_sub_rank_Size_Value &lt;- CH_data_total_monthly_sub_cumret_Size_Value %&gt;% group_by(Date_t) %&gt;% # Here we form the groups (ranks) mutate(# Create Small and Big Rank Size_Ranks = as.numeric(cut(Market_Cap, breaks = quantile(Market_Cap, probs= 0:2/2, na.rm = TRUE, type = 4), include.lowest = TRUE)), # Create Terciles based on B2M (Low, Neutral, High) Ranks = as.numeric(cut(B2M, breaks = quantile(B2M, probs= seq(0, 1, length = 11), na.rm = TRUE, type = 4), include.lowest = TRUE)), Value_Ranks = ifelse(Ranks &lt;= 3, 1, ifelse(Ranks &gt; 3 &amp; Ranks &lt;= 7, 2, ifelse(Ranks &gt; 7, 3, 0))) ) %&gt;% ungroup() # Assign Rankings for Size and Old Value CH_data_total_monthly_sub_rank_Size_Value_old &lt;- CH_data_total_monthly_sub_cumret_Size_Value_old %&gt;% group_by(Date_t) %&gt;% # Here we form the groups (ranks) mutate(# Create Small and Big Rank Size_Ranks = as.numeric(cut(Market_Cap, breaks = quantile(Market_Cap, probs= 0:2/2, na.rm = TRUE, type = 4), include.lowest = TRUE)), # Create Terciles based on B2M (Low, Neutral, High) Ranks = as.numeric(cut(B2M_old, breaks = quantile(B2M_old, probs= seq(0, 1, length = 11), na.rm = TRUE, type = 4), include.lowest = TRUE)), Value_old_Ranks = ifelse(Ranks &lt;= 3, 1, ifelse(Ranks &gt; 3 &amp; Ranks &lt;= 7, 2, ifelse(Ranks &gt; 7, 3, 0))) ) %&gt;% ungroup() # Assign Rankings for Size and Operating Profitability CH_data_total_monthly_sub_rank_Size_Profit &lt;- CH_data_total_monthly_sub_cumret_Size_Profit %&gt;% group_by(Date_t) %&gt;% # Here we form the groups (ranks) mutate(# Create Small and Big Rank Size_Ranks = as.numeric(cut(Market_Cap, breaks = quantile(Market_Cap, probs= 0:2/2, na.rm = TRUE, type = 4), include.lowest = TRUE)), # Create Terciles based on Oper Profit (Low, Neutral, High) Ranks = as.numeric(cut(Profit, breaks = quantile(Profit, probs= seq(0, 1, length = 11), na.rm = TRUE, type = 4), include.lowest = TRUE)), Profit_Ranks = ifelse(Ranks &lt;= 3, 1, ifelse(Ranks &gt; 3 &amp; Ranks &lt;= 7, 2, ifelse(Ranks &gt; 7, 3, 0))) ) %&gt;% ungroup() # Assign Rankings for Size and Investment Growth CH_data_total_monthly_sub_rank_Size_Invest &lt;- CH_data_total_monthly_sub_cumret_Size_Invest %&gt;% group_by(Date_t) %&gt;% # Here we form the groups (ranks) mutate(# Create Small and Big Rank Size_Ranks = as.numeric(cut(Market_Cap, breaks = quantile(Market_Cap, probs= 0:2/2, na.rm = TRUE, type = 4), include.lowest = TRUE)), # Create Terciles based on Investment Growth (Low, Neutral, High) Ranks = ntile(Total_Assets_t, 10), Invest_Ranks = ifelse(Ranks &lt;= 3, 1, ifelse(Ranks &gt; 3 &amp; Ranks &lt;= 7, 2, ifelse(Ranks &gt; 7, 3, 0))) ) %&gt;% ungroup() # Create the average returns per group for Size and Value CH_data_EW_Ret_HML &lt;- CH_data_total_monthly_sub_rank_Size_Value %&gt;% group_by(Date_t, Size_Ranks, Value_Ranks) %&gt;% mutate(EW_Ret_mean_t_Size_Value = mean(Ret_Adj, na.rm = T), EW_Ret_sd_t_Size_Value = sd(Ret_Adj, na.rm = T)) %&gt;% select(Date_t,Size_Ranks, Value_Ranks, EW_Ret_mean_t_Size_Value,EW_Ret_sd_t_Size_Value) %&gt;% ungroup() %&gt;% unique() %&gt;% arrange(Date_t,Size_Ranks, Value_Ranks) %&gt;% group_by(Date_t, Value_Ranks) %&gt;% mutate(High_Size = ifelse(Value_Ranks == 3, mean(EW_Ret_mean_t_Size_Value), 0), Low_Size = ifelse(Value_Ranks == 1, mean(EW_Ret_mean_t_Size_Value), 0)) %&gt;% ungroup() %&gt;% select(Date_t, High_Size, Low_Size) %&gt;% unique() %&gt;% group_by(Date_t) %&gt;% mutate(HML = sum(High_Size) - sum(Low_Size)) %&gt;% select(Date_t, HML) %&gt;% unique() # Create the average returns per group for Size and Old Value CH_data_EW_Ret_HML_old &lt;- CH_data_total_monthly_sub_rank_Size_Value_old %&gt;% group_by(Date_t, Size_Ranks, Value_old_Ranks) %&gt;% mutate(EW_Ret_mean_t_Size_Value_old = mean(Ret_Adj, na.rm = T), EW_Ret_sd_t_Size_Value_old = sd(Ret_Adj, na.rm = T)) %&gt;% select(Date_t,Size_Ranks, Value_old_Ranks, EW_Ret_mean_t_Size_Value_old,EW_Ret_sd_t_Size_Value_old) %&gt;% ungroup() %&gt;% unique() %&gt;% arrange(Date_t,Size_Ranks, Value_old_Ranks) %&gt;% group_by(Date_t, Value_old_Ranks) %&gt;% mutate(High_Size = ifelse(Value_old_Ranks == 3, mean(EW_Ret_mean_t_Size_Value_old), 0), Low_Size = ifelse(Value_old_Ranks == 1, mean(EW_Ret_mean_t_Size_Value_old), 0)) %&gt;% ungroup() %&gt;% select(Date_t, High_Size, Low_Size) %&gt;% unique() %&gt;% group_by(Date_t) %&gt;% mutate(HML_old = sum(High_Size) - sum(Low_Size)) %&gt;% select(Date_t, HML_old) %&gt;% unique() # Create the average returns per group for Size and Operating Profitability CH_data_EW_Ret_Size_RMW &lt;- CH_data_total_monthly_sub_rank_Size_Profit %&gt;% group_by(Date_t, Size_Ranks, Profit_Ranks) %&gt;% mutate(EW_Ret_mean_t_Size_Profit = mean(Ret_Adj, na.rm = T), EW_Ret_sd_t_Size_Profit = sd(Ret_Adj, na.rm = T)) %&gt;% select(Date_t,Size_Ranks, Profit_Ranks, EW_Ret_mean_t_Size_Profit,EW_Ret_sd_t_Size_Profit) %&gt;% ungroup() %&gt;% unique() %&gt;% arrange(Date_t,Size_Ranks, Profit_Ranks) %&gt;% group_by(Date_t, Profit_Ranks) %&gt;% mutate(Robust_Size = ifelse(Profit_Ranks == 3, mean(EW_Ret_mean_t_Size_Profit), 0), Weak_Size = ifelse(Profit_Ranks == 1, mean(EW_Ret_mean_t_Size_Profit), 0)) %&gt;% ungroup() %&gt;% select(Date_t, Robust_Size, Weak_Size) %&gt;% unique() %&gt;% group_by(Date_t) %&gt;% mutate(RMW = sum(Robust_Size) - sum(Weak_Size)) %&gt;% select(Date_t, RMW) %&gt;% unique() # Create the average returns per group for Size and Investment Growth CH_data_EW_Ret_CMA &lt;- CH_data_total_monthly_sub_rank_Size_Invest %&gt;% group_by(Date_t, Size_Ranks, Invest_Ranks) %&gt;% mutate(EW_Ret_mean_t_Size_Invest = mean(Ret_Adj, na.rm = T), EW_Ret_sd_t_Size_Invest = sd(Ret_Adj, na.rm = T)) %&gt;% select(Date_t,Size_Ranks, Invest_Ranks, EW_Ret_mean_t_Size_Invest,EW_Ret_sd_t_Size_Invest) %&gt;% ungroup() %&gt;% unique() %&gt;% arrange(Date_t,Size_Ranks, Invest_Ranks) %&gt;% group_by(Date_t, Invest_Ranks) %&gt;% mutate(Aggressive_Ranks = ifelse(Invest_Ranks == 3, mean(EW_Ret_mean_t_Size_Invest), 0), Conservative_Ranks = ifelse(Invest_Ranks == 1, mean(EW_Ret_mean_t_Size_Invest), 0)) %&gt;% ungroup() %&gt;% select(Date_t, Aggressive_Ranks, Conservative_Ranks) %&gt;% unique() %&gt;% group_by(Date_t) %&gt;% mutate(CMA = sum(Conservative_Ranks) - sum(Aggressive_Ranks)) %&gt;% select(Date_t, CMA) %&gt;% unique() # Combine the three individual factors ## Create three xts objects CH_data_EW_Ret_HML_ts &lt;- xts(CH_data_EW_Ret_HML[,-1], order.by = as.Date(CH_data_EW_Ret_HML$Date_t)) CH_data_EW_Ret_HML_old_ts &lt;- xts(CH_data_EW_Ret_HML_old[,-1], order.by = as.Date(CH_data_EW_Ret_HML_old$Date_t)) CH_data_EW_Ret_Size_RMW_ts &lt;- xts(CH_data_EW_Ret_Size_RMW[,-1], order.by = as.Date(CH_data_EW_Ret_Size_RMW$Date_t)) CH_data_EW_Ret_Size_CMA_ts &lt;- xts(CH_data_EW_Ret_CMA[,-1], order.by = as.Date(CH_data_EW_Ret_CMA$Date_t)) Factors &lt;- merge.xts(SMB$SMB, CH_data_EW_Ret_HML_ts, CH_data_EW_Ret_Size_RMW_ts, CH_data_EW_Ret_Size_CMA_ts) cumprod(1+na.omit(Factors[&quot;1991-02-28/2020-12-31&quot;])) %&gt;% tidy() %&gt;% ggplot(aes(x =index ,y = value ,color = series)) + geom_line( size = 0.5 ) + ggtitle(&quot;Size, Value, Profitability and Investment Growth Strategy&quot;) + ylab(&quot;Cumulative Returns&quot;) + xlab(&quot;Time&quot;) + theme(plot.title= element_text(size=14, color=&quot;grey26&quot;, hjust=0.5, lineheight=1.2), panel.background = element_rect(fill=&quot;#f7f7f7&quot;), panel.grid.major.y = element_line(size = 0.5, linetype = &quot;solid&quot;, color = &quot;grey&quot;), panel.grid.minor = element_blank(), panel.grid.major.x = element_blank(), plot.background = element_rect(fill=&quot;#f7f7f7&quot;, color = &quot;#f7f7f7&quot;), axis.title.x = element_text(color=&quot;grey26&quot;, size=12), axis.title.y = element_text(color=&quot;grey26&quot;, size=12), axis.line = element_line(color = &quot;black&quot;)) As we can see, the Profitability factor appears to be the strongest anomaly of the factors under consideration. The value factor also shows significant, positive, albeit less pronounced, returns for the period under consideration. On the other hand, both the Investment as well as the size factor appear to be no material anomaly in the Swiss market from the observational time horizon. Finally, let’s calculate the summary statistics # Calculate the three performance metrics Annualised_Mean_Return &lt;- Return.annualized(Factors) Annualised_SD &lt;- sd.annualized(Factors) Sharpe_Ratio &lt;- Annualised_Mean_Return/Annualised_SD # Put it together df_factors &lt;- as.data.frame(rbind(Annualised_Mean_Return, Annualised_SD, Sharpe_Ratio)) rownames(df_factors) &lt;- c(&quot;Annualised Return&quot;, &quot;Standard Deviation&quot;, &quot;Sharpe Ratio&quot;) df_factors ## SMB HML RMW CMA ## Annualised Return -0.04904802 0.0331400 0.05414061 -0.03357213 ## Standard Deviation 0.10969154 0.1285831 0.10801873 0.19642474 ## Sharpe Ratio -0.44714498 0.2577321 0.50121499 -0.17091602 6.5.5.3 Up Minus Down Lastly, we will show you the performance of the Momentum factor, or UMD. We will create the factor based on both the Jegadeesh and Titman (1995) approach (the original approach of their paper) as well as the more static approach used by Carharrt (1996). We will first create the approach of Carharrt (1996). # Load the old datasets for check up Prices_Adj &lt;- read.csv(&quot;~/Desktop/Master UZH/Data/A4_dataset_01_Ex_Session.txt&quot;, header = T, sep = &quot;\\t&quot;) Prices_Unadj_check &lt;- Prices_Adj %&gt;% gather(Ticker,value,NESN:X692395) %&gt;% mutate(Date_t = lubridate::ceiling_date(dmy(Date), &quot;month&quot;) - 1) %&gt;% select(-Date) colnames(Prices_Unadj_check) &lt;- c(&quot;Ticker&quot;, &quot;Close_Real&quot;, &quot;Date_t&quot;) # Get only selected companies tic_sub &lt;- unique(names(read.csv(&quot;~/Desktop/Master UZH/Data/A4_dataset_04_Ex_Session.txt&quot;, header = T, sep = &quot;\\t&quot;))) # Get 347 companies CH_data_total_monthly &lt;- read.csv(&quot;~/Desktop/CH_data_total_monthly.csv&quot;, header = T, sep = &quot;,&quot;) CH_data_total_monthly_clean &lt;- CH_data_total_monthly[!duplicated(CH_data_total_monthly[c(&quot;Date_t&quot;, &quot;Ticker&quot;)]),] # Only get the respective columns CH_data_total_mont &lt;- CH_data_total_monthly_clean %&gt;% subset(Ticker %in% tic_sub) %&gt;% select(Date_t, Ticker, Adj_Close, Close, Num_Shares, Total_Eq_t, Gross_Inc_t, Operating_Exp_t, Interest_Exp_t, Total_Assets_t) %&gt;% mutate(Total_Eq_t = Total_Eq_t, Num_Shares = Num_Shares, Date_t = as.Date(Date_t)) %&gt;% group_by(Ticker) %&gt;% mutate(Close = lag(Close, n=1), Total_Eq_t = lag(Total_Eq_t, 1), Num_Shares = lag(Num_Shares, 1)) %&gt;% ungroup() CH_data_total_mont_check &lt;- left_join(CH_data_total_mont, Prices_Unadj_check, by = c(&quot;Ticker&quot; = &quot;Ticker&quot;, &quot;Date_t&quot; = &quot;Date_t&quot;)) # Control for extreme values CH_data_total_monthly_sub_Size &lt;- CH_data_total_mont_check %&gt;% arrange(Date_t) %&gt;% group_by(Ticker) %&gt;% mutate(Ret_Adj = (Adj_Close - lag(Adj_Close, 1))/lag(Adj_Close,1), Ret_Adj_old = (Close_Real - lag(Close_Real, 1))/lag(Close_Real,1)) %&gt;% na.omit() %&gt;% ungroup() %&gt;% group_by(Date_t) %&gt;% mutate(Extreme_Values = quantile(Ret_Adj, p = 1), Extreme_Indicator = ifelse(Ret_Adj &lt;= Extreme_Values, 1, NA), Ret_Adj = Ret_Adj * Extreme_Indicator) %&gt;% ungroup() # Create the variables for Size and Momentum CH_data_total_monthly_sub_cumret_Size_Mom &lt;- CH_data_total_monthly_sub_Size %&gt;% group_by(Ticker) %&gt;% mutate(# Create the Market Cap (Size) characteristic Shares_Out_lagged = lag(Num_Shares, n = 1), Price_close_lagged = lag(Close, n = 1), Market_Cap = Shares_Out_lagged * Price_close_lagged, # Create the Cumulative Returns (Momentum) characteristic Log_Ret_Adj = lag(log(1+Ret_Adj), n = 1), Sum_Ret = roll_sum(Log_Ret_Adj, 6), Cum_Ret = exp(Sum_Ret) - 1, ) %&gt;% na.omit() %&gt;% ungroup() # Assign Rankings for Size and Momentum CH_data_total_monthly_sub_rank_Size_Mom &lt;- CH_data_total_monthly_sub_cumret_Size_Mom %&gt;% group_by(Date_t) %&gt;% # Here we form the groups (ranks) mutate(# Create Small and Big Rank Size_Ranks = as.numeric(cut(Market_Cap, breaks = quantile(Market_Cap, probs= 0:2/2, na.rm = TRUE, type = 4), include.lowest = TRUE)), # Create Terciles based on B2M (Low, Neutral, High) Ranks = as.numeric(cut(Cum_Ret, breaks = quantile(Cum_Ret, probs= seq(0, 1, length = 11), na.rm = TRUE, type = 4), include.lowest = TRUE)), Mom_Ranks = ifelse(Ranks &lt;= 3, 1, ifelse(Ranks &gt; 3 &amp; Ranks &lt;= 7, 2, ifelse(Ranks &gt; 7, 3, 0))) ) %&gt;% ungroup() # Create the average returns per group for Size and Momentum CH_data_EW_Ret_Mom &lt;- CH_data_total_monthly_sub_rank_Size_Mom %&gt;% group_by(Date_t, Size_Ranks, Mom_Ranks) %&gt;% mutate(EW_Ret_mean_t_Size_Mom = mean(Ret_Adj, na.rm = T), EW_Ret_sd_t_Size_Mom = sd(Ret_Adj, na.rm = T)) %&gt;% select(Date_t,Size_Ranks, Mom_Ranks, EW_Ret_mean_t_Size_Mom,EW_Ret_sd_t_Size_Mom) %&gt;% ungroup() %&gt;% unique() %&gt;% arrange(Date_t,Size_Ranks, Mom_Ranks) %&gt;% group_by(Date_t, Mom_Ranks) %&gt;% mutate(Up_Size = ifelse(Mom_Ranks == 3, mean(EW_Ret_mean_t_Size_Mom), 0), Down_Size = ifelse(Mom_Ranks == 1, mean(EW_Ret_mean_t_Size_Mom), 0)) %&gt;% ungroup() %&gt;% select(Date_t, Up_Size, Down_Size) %&gt;% unique() %&gt;% group_by(Date_t) %&gt;% mutate(MOM = sum(Up_Size) - sum(Down_Size)) %&gt;% select(Date_t, MOM) %&gt;% unique() colnames(CH_data_EW_Ret_Mom) &lt;- c(&quot;Date_t&quot;, &quot;Momentum_Car&quot;) # Combine the three individual factors ## Create the xts objects CH_data_EW_Ret_MOM_ts &lt;- xts(CH_data_EW_Ret_Mom[,-1], order.by = as.Date(CH_data_EW_Ret_Mom$Date_t)) Now, we will replicate the approach of Jegadeesh and Titman (1995). # Let&#39;s run the experiment for the reduced data frame tic_sub &lt;- unique(names(read.csv(&quot;~/Desktop/Master UZH/Data/A4_dataset_04_Ex_Session.txt&quot;, header = T, sep = &quot;\\t&quot;))) # Get 347 companies CH_data_total_monthly &lt;- read.csv(&quot;~/Desktop/CH_data_total_monthly.csv&quot;, header = T, sep = &quot;,&quot;) CH_data_total_monthly_clean &lt;- CH_data_total_monthly[!duplicated(CH_data_total_monthly[c(&quot;Date_t&quot;, &quot;Ticker&quot;)]),] # Only get the respective columns CH_data_total_mont &lt;- CH_data_total_monthly_clean %&gt;% subset(Ticker %in% tic_sub) %&gt;% select(Date_t, Ticker, Adj_Close, Num_Shares, Close) %&gt;% mutate(Date_t = as.Date(Date_t), Date_t = as.yearmon(Date_t)) # Control for extreme values CH_data_total_monthly_sub &lt;- CH_data_total_mont %&gt;% arrange(Date_t) %&gt;% select(Date_t, Ticker, Adj_Close, Num_Shares, Close) %&gt;% group_by(Ticker) %&gt;% mutate(Ret_Adj = (Adj_Close - lag(Adj_Close, 1))/lag(Adj_Close,1)) %&gt;% na.omit() %&gt;% ungroup() %&gt;% group_by(Date_t) %&gt;% mutate(Extreme_Values = quantile(Ret_Adj, p = 1), Extreme_Indicator = ifelse(Ret_Adj &lt;= Extreme_Values, 1, NA), Ret_Adj = Ret_Adj * Extreme_Indicator) %&gt;% ungroup() # Get the lagged values to ensure that we have a HPR of 6 periods! # Since we go 6 periods behind and take the cum ret from period -6 to period -1 to obtain the HPR from period -5 to 0. # Idea: If we do it like this, we account for gaps in the data. E.g. if two observational periods were on Aug 1990 and then on Aug 1991, the gap would be 12 periods. Thus, this would not constitute a HPR of 6 periods, but 12. In order to ensure we only ever get HPR of 6 periods, we need to create the indicator which shows how many periods (in months) two dates are apart from one another. This must equal 6 and not more! CH_data_total_monthly_sub_lag &lt;- CH_data_total_monthly_sub %&gt;% group_by(Ticker) %&gt;% mutate(lag6 = round(12*(Date_t - lag(Date_t, n=5)))/5) # Get the adjustred returns to form the PF on CH_data_total_monthly_sub_cumret &lt;- CH_data_total_monthly_sub_lag %&gt;% group_by(Ticker) %&gt;% mutate(Shares_Out_lagged = lag(Num_Shares, n = 1), Price_close_lagged = lag(Close, n = 1), Market_Cap = Shares_Out_lagged * Price_close_lagged, # Create the Cumulative Returns (Momentum) characteristic Log_Ret_Adj = lag(log(1+Ret_Adj), n = 1), Sum_Ret = roll_sum(Log_Ret_Adj, 6), Cum_Ret = exp(Sum_Ret) - 1) %&gt;% na.omit() %&gt;% ungroup() # Assign Rankings for Size and Momentum CH_data_total_monthly_sub_rank_lag &lt;- CH_data_total_monthly_sub_cumret %&gt;% group_by(Date_t) %&gt;% # Here we form the groups (ranks) mutate(# Create Small and Big Rank Size_Ranks = as.numeric(cut(Market_Cap, breaks = quantile(Market_Cap, probs= 0:2/2, na.rm = TRUE, type = 4), include.lowest = TRUE)), # Create Terciles based on B2M (Low, Neutral, High) Ranks = as.numeric(cut(Cum_Ret, breaks = quantile(Cum_Ret, probs= seq(0, 1, length = 11), na.rm = TRUE, type = 4), include.lowest = TRUE)), Mom_Ranks = ifelse(Ranks &lt;= 3, 1, ifelse(Ranks &gt; 3 &amp; Ranks &lt;= 7, 2, ifelse(Ranks &gt; 7, 3, 0))) ) %&gt;% ungroup() # Select only certain columns CH_data_total_monthly_sub_rank_2_lag &lt;- CH_data_total_monthly_sub_rank_lag %&gt;% select(Date_t, Ticker, Mom_Ranks, Size_Ranks) # Create the average returns per group for Size and Momentum CH_data_total_monthly_sub_rank_tot &lt;- CH_data_total_monthly_sub_lag %&gt;% ungroup() %&gt;% filter(., lag6 == 1 ) %&gt;% left_join(CH_data_total_monthly_sub_rank_2_lag, by = c(&quot;Ticker&quot;=&quot;Ticker&quot;, &quot;Date_t&quot; = &quot;Date_t&quot;)) %&gt;% select(Ticker,Date_t,Ret_Adj,Mom_Ranks, Size_Ranks) %&gt;% arrange(Ticker,Date_t) %&gt;% group_by(Date_t,Mom_Ranks) %&gt;% mutate(momr = mean(Ret_Adj, na.rm = T)) %&gt;% ungroup() %&gt;% select(Date_t,Mom_Ranks,Size_Ranks, momr) %&gt;% ungroup() %&gt;% unique() %&gt;% arrange(Date_t,Size_Ranks, Mom_Ranks) %&gt;% group_by(Date_t, Mom_Ranks) %&gt;% mutate(Up_Size = ifelse(Mom_Ranks == 3, mean(momr), 0), Down_Size = ifelse(Mom_Ranks == 1, mean(momr), 0)) %&gt;% ungroup() %&gt;% select(Date_t, Up_Size, Down_Size) %&gt;% unique() %&gt;% group_by(Date_t) %&gt;% mutate(MOM = sum(Up_Size, na.rm = T) - sum(Down_Size, na.rm = T), Date_t = lubridate::ceiling_date(my(Date_t), &quot;month&quot;) - 1) %&gt;% select(Date_t, MOM) %&gt;% unique() colnames(CH_data_total_monthly_sub_rank_tot) &lt;- c(&quot;Date_t&quot;, &quot;Momentum_JT&quot;) # Combine the three individual factors ## Create the xts objects CH_data_EW_Ret_MOM_JT_ts &lt;- xts(CH_data_total_monthly_sub_rank_tot[,-1], order.by = as.Date(CH_data_total_monthly_sub_rank_tot$Date_t)) Now, we can combine both approaches and see the differences in cumulative returns. # Create a merged xts object Factors &lt;- merge.xts(CH_data_EW_Ret_MOM_ts, CH_data_EW_Ret_MOM_JT_ts) # Plot the relationship cumprod(1+na.omit(Factors[&quot;1991-02-28/2020-12-31&quot;])) %&gt;% tidy() %&gt;% ggplot(aes(x =index ,y = value ,color = series)) + geom_line( size = 0.5 ) + ggtitle(&quot;Momentum Strategies&quot;) + ylab(&quot;Cumulative Returns&quot;) + xlab(&quot;Time&quot;) + theme(plot.title= element_text(size=14, color=&quot;grey26&quot;, hjust=0.5, lineheight=1.2), panel.background = element_rect(fill=&quot;#f7f7f7&quot;), panel.grid.major.y = element_line(size = 0.5, linetype = &quot;solid&quot;, color = &quot;grey&quot;), panel.grid.minor = element_blank(), panel.grid.major.x = element_blank(), plot.background = element_rect(fill=&quot;#f7f7f7&quot;, color = &quot;#f7f7f7&quot;), axis.title.x = element_text(color=&quot;grey26&quot;, size=12), axis.title.y = element_text(color=&quot;grey26&quot;, size=12), axis.line = element_line(color = &quot;black&quot;)) As we can see, there are slight differences in the cumulative return structures of both strategies, whereas it appears as if the factor construction of Jegadeesh and Titman (JT) delivers, on average, higher cumulative returns, especially after 2012. Finally, let’s calculate the summary statistics # Calculate the three performance metrics Annualised_Mean_Return &lt;- Return.annualized(Factors) Annualised_SD &lt;- sd.annualized(Factors) Sharpe_Ratio &lt;- Annualised_Mean_Return/Annualised_SD # Put it together df_MOM &lt;- as.data.frame(rbind(Annualised_Mean_Return, Annualised_SD, Sharpe_Ratio)) rownames(df_MOM) &lt;- c(&quot;Annualised Return&quot;, &quot;Standard Deviation&quot;, &quot;Sharpe Ratio&quot;) df_MOM ## Momentum_Car Momentum_JT ## Annualised Return 0.1327952 0.1587250 ## Standard Deviation 0.1716485 0.1797972 ## Sharpe Ratio 0.7736459 0.8828004 6.5.5.4 Betting Against Beta # Let&#39;s run the experiment for the reduced data frame tic_sub &lt;- unique(names(read.csv(&quot;~/Desktop/Master UZH/Data/A4_dataset_04_Ex_Session.txt&quot;, header = T, sep = &quot;\\t&quot;))) # Get 347 companies CH_data_total_monthly &lt;- read.csv(&quot;~/Desktop/CH_data_total_monthly.csv&quot;, header = T, sep = &quot;,&quot;) CH_data_total_monthly_clean &lt;- CH_data_total_monthly[!duplicated(CH_data_total_monthly[c(&quot;Date_t&quot;, &quot;Ticker&quot;)]),] # Only get the respective columns CH_data_total_mont &lt;- CH_data_total_monthly_clean %&gt;% subset(Ticker %in% tic_sub) %&gt;% select(Date_t, Ticker, Adj_Close, Total_Assets_t) # Control for extreme values CH_data_total_monthly_sub_BAB &lt;- CH_data_total_mont %&gt;% arrange(Date_t) %&gt;% group_by(Ticker) %&gt;% mutate(Ret_Adj = (Adj_Close - lag(Adj_Close, 1))/lag(Adj_Close,1)) %&gt;% na.omit() %&gt;% ungroup() %&gt;% group_by(Date_t) %&gt;% mutate(Extreme_Values = quantile(Ret_Adj, p = 1), Extreme_Indicator = ifelse(Ret_Adj &lt;= Extreme_Values, 1, NA), Ret_Adj = Ret_Adj * Extreme_Indicator) %&gt;% ungroup() # Let&#39;s also get the data on the Swiss Market Index as well as the risk free rate rf &lt;- read.csv(&quot;~/Desktop/Master UZH/Data/A2_dataset_02_Ex_Session.txt&quot;, header = T, sep = &quot;\\t&quot;) rf_ts &lt;- rf %&gt;% subset((Date &gt;= &#39;1989-12-01&#39;) &amp; (Date &lt;= &#39;2019-12-31&#39;)) %&gt;% mutate(rf_annual = SWISS.CONFEDERATION.BOND.1.YEAR...RED..YIELD / 1000, rf_monthly = (1 + rf_annual)^(1/12) - 1) %&gt;% select(Date, rf_monthly) SMI &lt;- read.csv(&quot;~/Desktop/Master UZH/Data/A2_dataset_03_Ex_Session.txt&quot;, header = T, sep = &quot;\\t&quot;) SMI_ts &lt;- SMI %&gt;% subset((Date &gt;= &#39;1989-12-01&#39;) &amp; (Date &lt;= &#39;2019-12-31&#39;)) %&gt;% mutate(SMI_ret = (SMI - lag(SMI, 1))/lag(SMI, 1), rf_monthly = rf_ts$rf_monthly, SMI_rf_excess_ret = SMI_ret - rf_monthly) # Now, let&#39;s calculate the excess returns SMI_rf_ts_ret &lt;- SMI_ts_ret - rf_ts_monthly CH_data_total_monthly_sub_excess_ret_BAB &lt;- left_join(SMI_ts, CH_data_total_monthly_sub_BAB, by = c(&quot;Date&quot; = &quot;Date_t&quot;)) %&gt;% mutate(Excess_Ret_Adj = Ret_Adj - rf_monthly) # Based on this, we can now have rolling regressions of the correlation as well as the standard deviations. CH_data_total_monthly_weighted_BAB &lt;- CH_data_total_monthly_sub_excess_ret_BAB %&gt;% group_by(Ticker) %&gt;% mutate(sd_roll_1_year = roll_sd(Excess_Ret_Adj, width = 12), corr_roll_5_year = roll_cor(Excess_Ret_Adj, SMI_rf_excess_ret, width = 60), sd_roll_1_year_Market = roll_sd(SMI_rf_excess_ret, width = 12), Beta_Est_raw = sd_roll_1_year/sd_roll_1_year_Market*corr_roll_5_year, Beta_Est_Adj = Beta_Est_raw*0.6 + 0.4*1) %&gt;% ungroup() %&gt;% subset(!is.na(Beta_Est_Adj)) %&gt;% group_by(Date) %&gt;% mutate(ranks_beta = rank(Beta_Est_Adj), mean_rank_beta = mean(ranks_beta, na.rm = T), portfolio_indicator_BAB =ifelse(ranks_beta &gt; mean_rank_beta, 1, 0)) %&gt;% ungroup() %&gt;% group_by(Date, portfolio_indicator_BAB) %&gt;% mutate(abs_dev = abs(ranks_beta - mean_rank_beta), sum_abs_dev = sum(abs_dev, na.rm = T), k = 2 / sum_abs_dev, weights_BAB = k*(ranks_beta - mean_rank_beta), Beta_Est_Weight_Adj = Beta_Est_Adj * weights_BAB) %&gt;% ungroup() # Assign rankings CH_data_total_monthly_rank_BAB &lt;- CH_data_total_monthly_weighted_BAB %&gt;% group_by(Date) %&gt;% mutate(Decile = ntile(Beta_Est_Weight_Adj, 2)) %&gt;% ungroup() # Create mean returns Decile CH_data_EW_Ret_BAB &lt;- CH_data_total_monthly_rank_BAB %&gt;% group_by(Date, Decile) %&gt;% mutate(EW_Ret_mean_t = 1/sum(Beta_Est_Weight_Adj)*sum(weights_BAB*Excess_Ret_Adj)) %&gt;% select(Date, Decile, EW_Ret_mean_t) %&gt;% ungroup() %&gt;% unique() %&gt;% arrange(Date, Decile) %&gt;% spread(key = Decile, value = EW_Ret_mean_t) %&gt;% mutate(BAB = `1` - `2`, BAB_cr = cumprod(1+BAB)) CH_data_EW_Ret_BAB %&gt;% ggplot() + geom_line(mapping = aes(x =as.Date(Date),y = BAB_cr), size = 0.5, color = &quot;goldenrod&quot;) + ggtitle(&quot;BAB Strategy with long formatted dataset&quot;) + ylab(&quot;Cumulative Returns&quot;) + xlab(&quot;Time&quot;) + theme(plot.title= element_text(size=14, color=&quot;grey26&quot;, hjust=0.5, lineheight=1.2), panel.background = element_rect(fill=&quot;#f7f7f7&quot;), panel.grid.major.y = element_line(size = 0.5, linetype = &quot;solid&quot;, color = &quot;grey&quot;), panel.grid.minor = element_blank(), panel.grid.major.x = element_blank(), plot.background = element_rect(fill=&quot;#f7f7f7&quot;, color = &quot;#f7f7f7&quot;), axis.title.x = element_text(color=&quot;grey26&quot;, size=12), axis.title.y = element_text(color=&quot;grey26&quot;, size=12), axis.line = element_line(color = &quot;black&quot;)) Finally, let’s calculate the summary statistics CH_data_EW_Ret_BAB_ts &lt;- xts(CH_data_EW_Ret_BAB$BAB, order.by = as.Date(CH_data_EW_Ret_BAB$Date)) # Calculate the three performance metrics Annualised_Mean_Return &lt;- Return.annualized(CH_data_EW_Ret_BAB_ts) Annualised_SD &lt;- sd.annualized(CH_data_EW_Ret_BAB_ts) Sharpe_Ratio &lt;- Annualised_Mean_Return/Annualised_SD # Put it together df_BAB&lt;- as.data.frame(rbind(Annualised_Mean_Return, Annualised_SD, Sharpe_Ratio)) rownames(df_BAB) &lt;- c(&quot;Annualised Return&quot;, &quot;Standard Deviation&quot;, &quot;Sharpe Ratio&quot;) colnames(df_BAB) &lt;- &quot;BAB&quot; df_BAB ## BAB ## Annualised Return 0.03227748 ## Standard Deviation 0.15029547 ## Sharpe Ratio 0.21476018 6.5.6 The importance of data selection We have now seen how to calculate five of the most widespread factors in asset management. Lastly, we deem it necessary to show you the importance of data accessibility. This follows the fact that identical databases provide different data based on the sub-section of the respective database. Since the data is different, the exactly identical approach to construct the factor at hand will, inadvertedly, lead to different end results. This should hightlight both the discrepancy of data availability and stress the need to always thoroughly double-check and cross-reference your data in order to circumvent the issue of incoherent data mining. To do so, we retrieved data to calculate the Value factor from Thomson Reuters Datastream as well as Bloomberg, two of the largest data providers. We calculate the Value factor in an identical way and plot both cumulative returns to show the (substantial) differences when using different sets of data. # Load the old dataset for book value Book &lt;- read.csv(&quot;~/Desktop/Master UZH/Data/A4_dataset_02_Ex_Session.txt&quot;, header = T, sep = &quot;\\t&quot;) Book_check &lt;- Book %&gt;% gather(Ticker,value,NESN:X692395) %&gt;% mutate(Date_t = lubridate::ceiling_date(dmy(Date), &quot;month&quot;) - 1) %&gt;% select(-Date) colnames(Book_check) &lt;- c(&quot;Ticker&quot;, &quot;Total_Eq_Real&quot;, &quot;Date_t&quot;) # Only sub select the companies under consideration tic_sub &lt;- unique(names(read.csv(&quot;~/Desktop/Master UZH/Data/A4_dataset_04_Ex_Session.txt&quot;, header = T, sep = &quot;\\t&quot;))) # Get 347 companies CH_data_total_monthly &lt;- read.csv(&quot;~/Desktop/CH_data_total_monthly.csv&quot;, header = T, sep = &quot;,&quot;) CH_data_total_monthly_clean &lt;- CH_data_total_monthly[!duplicated(CH_data_total_monthly[c(&quot;Date_t&quot;, &quot;Ticker&quot;)]),] # Only get the respective columns CH_data_total_mont &lt;- CH_data_total_monthly_clean %&gt;% subset(Ticker %in% tic_sub) %&gt;% select(Date_t, Ticker, Adj_Close, Close, Num_Shares, Total_Eq_t, Gross_Inc_t, Operating_Exp_t, Interest_Exp_t, Total_Assets_t, Total_Liab_t) %&gt;% mutate(Total_Eq_t = Total_Assets_t - Total_Liab_t, Num_Shares = Num_Shares, Date_t = as.Date(Date_t)) %&gt;% group_by(Ticker) %&gt;% mutate(Close = lag(Close, n=1), Num_Shares = lag(Num_Shares, 1)) %&gt;% ungroup() CH_data_total_mont_check &lt;- left_join(CH_data_total_mont, Book_check, by = c(&quot;Ticker&quot; = &quot;Ticker&quot;, &quot;Date_t&quot; = &quot;Date_t&quot;)) # Control for extreme values CH_data_total_monthly_sub_Size &lt;- CH_data_total_mont_check %&gt;% arrange(Date_t) %&gt;% group_by(Ticker) %&gt;% mutate(Ret_Adj = (Adj_Close - lag(Adj_Close, 1))/lag(Adj_Close,1)) %&gt;% na.omit() %&gt;% ungroup() %&gt;% group_by(Date_t) %&gt;% mutate(Extreme_Values = quantile(Ret_Adj, p = 1), Extreme_Indicator = ifelse(Ret_Adj &lt;= Extreme_Values, 1, NA), Ret_Adj = Ret_Adj * Extreme_Indicator) %&gt;% ungroup() # Create the variables for Size Value TR CH_data_total_monthly_sub_cumret_Size_Value &lt;- CH_data_total_monthly_sub_Size %&gt;% group_by(Ticker) %&gt;% mutate(# Create the Market Cap (Size) characteristic Shares_Out_lagged = lag(Num_Shares, n = 1), Price_close_lagged = lag(Close, n = 1), Market_Cap = Shares_Out_lagged * Price_close_lagged, # Create the B2M (Value) characteristic Total_Eq_t = Total_Assets_t - Total_Liab_t, B2M = lag(Total_Eq_t, n = 6) / Market_Cap) %&gt;% na.omit() %&gt;% ungroup() # Create the variables for Size and Value BL CH_data_total_monthly_sub_cumret_Size_Value_old &lt;- CH_data_total_monthly_sub_Size %&gt;% group_by(Ticker) %&gt;% mutate(# Create the Market Cap (Size) characteristic Shares_Out_lagged = lag(Num_Shares, n = 1), Price_close_lagged = lag(Close, n = 1), Market_Cap = Shares_Out_lagged * Price_close_lagged, # Create the B2M (Value) characteristic B2M_old = lag(Total_Eq_Real, n = 6) / Market_Cap) %&gt;% na.omit() %&gt;% ungroup() # Assign Rankings for Size and Value TR CH_data_total_monthly_sub_rank_Size_Value &lt;- CH_data_total_monthly_sub_cumret_Size_Value %&gt;% group_by(Date_t) %&gt;% # Here we form the groups (ranks) mutate(# Create Small and Big Rank Size_Ranks = as.numeric(cut(Market_Cap, breaks = quantile(Market_Cap, probs= 0:2/2, na.rm = TRUE, type = 4), include.lowest = TRUE)), # Create Terciles based on B2M (Low, Neutral, High) Ranks = as.numeric(cut(B2M, breaks = quantile(B2M, probs= seq(0, 1, length = 11), na.rm = TRUE, type = 4), include.lowest = TRUE)), Value_Ranks = ifelse(Ranks &lt;= 3, 1, ifelse(Ranks &gt; 3 &amp; Ranks &lt;= 7, 2, ifelse(Ranks &gt; 7, 3, 0))) ) %&gt;% ungroup() # Assign Rankings for Size and Value BL CH_data_total_monthly_sub_rank_Size_Value_old &lt;- CH_data_total_monthly_sub_cumret_Size_Value_old %&gt;% group_by(Date_t) %&gt;% # Here we form the groups (ranks) mutate(# Create Small and Big Rank Size_Ranks = as.numeric(cut(Market_Cap, breaks = quantile(Market_Cap, probs= 0:2/2, na.rm = TRUE, type = 4), include.lowest = TRUE)), # Create Terciles based on B2M (Low, Neutral, High) Ranks = as.numeric(cut(B2M_old, breaks = quantile(B2M_old, probs= seq(0, 1, length = 11), na.rm = TRUE, type = 4), include.lowest = TRUE)), Value_old_Ranks = ifelse(Ranks &lt;= 3, 1, ifelse(Ranks &gt; 3 &amp; Ranks &lt;= 7, 2, ifelse(Ranks &gt; 7, 3, 0))) ) %&gt;% ungroup() # Create the average returns per group for Size and Value TR CH_data_EW_Ret_HML &lt;- CH_data_total_monthly_sub_rank_Size_Value %&gt;% group_by(Date_t, Size_Ranks, Value_Ranks) %&gt;% mutate(EW_Ret_mean_t_Size_Value = mean(Ret_Adj, na.rm = T), EW_Ret_sd_t_Size_Value = sd(Ret_Adj, na.rm = T)) %&gt;% select(Date_t,Size_Ranks, Value_Ranks, EW_Ret_mean_t_Size_Value,EW_Ret_sd_t_Size_Value) %&gt;% ungroup() %&gt;% unique() %&gt;% arrange(Date_t,Size_Ranks, Value_Ranks) %&gt;% group_by(Date_t, Value_Ranks) %&gt;% mutate(High_Size = ifelse(Value_Ranks == 3, mean(EW_Ret_mean_t_Size_Value), 0), Low_Size = ifelse(Value_Ranks == 1, mean(EW_Ret_mean_t_Size_Value), 0)) %&gt;% ungroup() %&gt;% select(Date_t, High_Size, Low_Size) %&gt;% unique() %&gt;% group_by(Date_t) %&gt;% mutate(HML = sum(High_Size) - sum(Low_Size)) %&gt;% select(Date_t, HML) %&gt;% unique() # Create the average returns per group for Size and Value BL CH_data_EW_Ret_HML_old &lt;- CH_data_total_monthly_sub_rank_Size_Value_old %&gt;% group_by(Date_t, Size_Ranks, Value_old_Ranks) %&gt;% mutate(EW_Ret_mean_t_Size_Value_old = mean(Ret_Adj, na.rm = T), EW_Ret_sd_t_Size_Value_old = sd(Ret_Adj, na.rm = T)) %&gt;% select(Date_t,Size_Ranks, Value_old_Ranks, EW_Ret_mean_t_Size_Value_old,EW_Ret_sd_t_Size_Value_old) %&gt;% ungroup() %&gt;% unique() %&gt;% arrange(Date_t,Size_Ranks, Value_old_Ranks) %&gt;% group_by(Date_t, Value_old_Ranks) %&gt;% mutate(High_Size = ifelse(Value_old_Ranks == 3, mean(EW_Ret_mean_t_Size_Value_old), 0), Low_Size = ifelse(Value_old_Ranks == 1, mean(EW_Ret_mean_t_Size_Value_old), 0)) %&gt;% ungroup() %&gt;% select(Date_t, High_Size, Low_Size) %&gt;% unique() %&gt;% group_by(Date_t) %&gt;% mutate(HML_old = sum(High_Size) - sum(Low_Size)) %&gt;% select(Date_t, HML_old) %&gt;% unique() colnames(CH_data_EW_Ret_HML) &lt;- c(&quot;Date_t&quot;, &quot;HML_TR&quot;) colnames(CH_data_EW_Ret_HML_old) &lt;- c(&quot;Date_t&quot;, &quot;HML_BL&quot;) # Combine the three individual factors ## Create three xts objects CH_data_EW_Ret_HML_TR_ts &lt;- xts(CH_data_EW_Ret_HML[,-1], order.by = as.Date(CH_data_EW_Ret_HML$Date_t)) CH_data_EW_Ret_HML_BL_ts &lt;- xts(CH_data_EW_Ret_HML_old[,-1], order.by = as.Date(CH_data_EW_Ret_HML_old$Date_t)) Factors &lt;- merge.xts(CH_data_EW_Ret_HML_TR_ts, CH_data_EW_Ret_HML_BL_ts) cumprod(1+na.omit(Factors[&quot;1991-02-28/2020-12-31&quot;])) %&gt;% tidy() %&gt;% ggplot(aes(x =index ,y = value ,color = series)) + geom_line( size = 0.5 ) + ggtitle(&quot;Value strategy based on different databases (Bloomberg vs. Reuters)&quot;) + ylab(&quot;Cumulative Returns&quot;) + xlab(&quot;Time&quot;) + theme(plot.title= element_text(size=14, color=&quot;grey26&quot;, hjust=0.5, lineheight=1.2), panel.background = element_rect(fill=&quot;#f7f7f7&quot;), panel.grid.major.y = element_line(size = 0.5, linetype = &quot;solid&quot;, color = &quot;grey&quot;), panel.grid.minor = element_blank(), panel.grid.major.x = element_blank(), plot.background = element_rect(fill=&quot;#f7f7f7&quot;, color = &quot;#f7f7f7&quot;), axis.title.x = element_text(color=&quot;grey26&quot;, size=12), axis.title.y = element_text(color=&quot;grey26&quot;, size=12), axis.line = element_line(color = &quot;black&quot;)) As we can see, although both appear to be correlated and follow the same paths throughout time, we can see that the data provided from Bloomberg (BL) delivers substantially larger factor returns than the data from Thompson Reuters. Let’s look at where these differences arise. # Let&#39;s check why the two HML differentiate that much CH_data_total_mont_check %&gt;% mutate(Total_Eq_t = Total_Eq_t/1000) %&gt;% select(Ticker, Date_t, Total_Eq_t, Total_Eq_Real) %&gt;% mutate(Diff_Equity = round(Total_Eq_t - Total_Eq_Real,3), Diff_Equity_Ratio = round(Diff_Equity / ((Total_Eq_t + Total_Eq_Real)/2),3), Same_identifier = ifelse(Total_Eq_t == Total_Eq_Real, 1, 0)) %&gt;% subset(Same_identifier == 0) %&gt;% # As we can see, there are 16&#39;040 observations where the Total Equity value is different from one another subset( abs(Diff_Equity_Ratio) &gt; 0.1) %&gt;% # If we now only consider large discrepancies (more than 10% deviation), we obtain 2&#39;255 observations subset(abs(Diff_Equity_Ratio) &gt; 0.1) %&gt;% # If we only consider the companies which have at at least one point in time a deviation of more than 10%, we would get 51 companies with a deviation of # more than 10%! select(Ticker) %&gt;% unique() %&gt;% arrange(Ticker) ## # A tibble: 115 × 1 ## Ticker ## &lt;chr&gt; ## 1 ABBN ## 2 ACUN ## 3 ADEN ## 4 ADVN ## 5 ADXN ## 6 AEVS ## 7 AIRE ## 8 AIRN ## 9 ALCN ## 10 ALPH ## # … with 105 more rows If we look at the HML factor, we understand that discrepancies can either arise in the Book Value or Market Capitalisation, whereas the Market Capitalisation consists of the Number of Shares as well as the unadjusted Share price. Although we don’t show it here, the Market Capitalisation data is very similar for both data providers, which implies that the discrepancies in returns arise based on the company Book Value. If we drill down on this metric, we can observe that for 16’400 observations the Book Value (Total Equity) is different depending on the database. Let’s now assume that the cumulative return discrepancies are mainly caused by large differences (e.g. &gt;10%) in Book Value. If we consider this, then we understand that 2’255 observations have a discrepancy of more than 10% for the book value, and that, overall, 115 companies show to have a deviation of more than 10% for the book value at least at one observational period. This is striking, as it shows that, even for two of the most mainstream data providers there appears to be large differences in data availability and calculation of basic accounting numbers such as a company’s book value. Consequently, we can highlight the importance of cross-referencing and double checks with this simple exercise. Therefore, it is advisable to always stick to at least two calculation as well as database options when constructing factors. Based on that, you can then decide which strategy appears more reasonable for the task at hand. "],["statistical-properties.html", "Chapter 7 Statistical Properties 7.1 Random Variables and Probability Distributions: Introdcution 7.2 Matrix Algebra: Introduction", " Chapter 7 Statistical Properties The first topic covers mathematical and statistical properties that most modern finance is built upon. These properties serve as cornerstones required to comprehend the foundations of financial economics, risk management as well as asset management. ALthough widely used in many areas, their comprehension is key in identifying and retracing financial concepts in any of the afore-mentioned areas. In this chapter, we will cover the mathematical foundations and statistical properties that are often used within empirical finance contexts. Based upon the topics taught in Statistics and Mathematics courses as well as Empirical Methods, we will cover properties related to \\(\\textbf{Probability Theory}\\) and \\(\\textbf{Matrix Algebra}\\). This section should serve as a repetition to the topics already discusses in Empirical Methods and as such students should at least be familiar with the subjects at hand. However, as the course will rely substantially on these properties, a coherent discussion of them is a necessary prerequisite to be able to expect the baselines of empirical finance. 7.1 Random Variables and Probability Distributions: Introdcution In this chapter, we will repeat the fundamentals of probability as well as probability distributions. We dive into what random variables are, how they are used in financial applications, how they can be related to probability measures, such as distributions, what the distributions tell us, how they can be related to financial concepts and how they can be calculated using R. The chapter is outlined as follows: Section 1 introduces the concept of random variables within a countable space. Next, in Section 2, we look at discrete probability distributions, such as Bernoulli, Poisson or Mutlinomials. In Section 3 we look at continuous probability distributions, before we dive into continuous distributions with appealing properties, such as normal or log-normal distributions. Lastly, we will look at continuous functions that can deal with extreme events. 7.1.1 The concept of Probability: Important Notions and Definitions To understand the concept of random variables more thoroughly, we need to define some concepts first. The concepts we discuss are . For that, let’s use a dice throwing example. A dice can take up six values when being rolled, ranging from 1 to 6, with, theoretical, probability of 1/6 for each outcome. 7.1.1.1 Outcomes, Spaces and Events, Measurable and Immeasurable Spaces \\(\\textbf{Definition 2.1: Outcome}\\) Outcomes are just all possible, or feasible, values that a certain experiment can render. It is denoted by \\(\\omega\\). In the case of throwing a dice, this is just all numbers that the dice can show, e.g. 1 to 6. We write this accordingly as: \\[ \\omega_1 = 1, \\omega_2 = 2, \\omega_3 = 3, \\omega_4 = 4, \\omega_5 = 5, \\omega_6 = 6 \\] \\(\\textbf{Definition 2.2: Space}\\) The set of all feasible outcomes is called space. It is denoted by \\(\\Omega\\). In a dice experiment, this is just all values of \\(\\omega\\) defined previously. We write this as: \\[ \\Omega = [\\omega_1, \\omega_2, \\omega_3, \\omega_4, \\omega_5, \\omega_6] \\] Each Space \\(\\Omega\\) can be distributed into certain parts. For instance, in the dice example we can be interested in whether the rolled number is odd or even, defining a set of either all odd or all even numbers. In general, the \\(2^\\Omega\\) comprises of all possible subsets of a space \\(\\Omega\\), including the \\(\\emptyset\\) and the space set \\(\\Omega\\). With the aid of this power set, we are able to describe . Another neat property of the power set is that it includes each union of arbitrarily many events as well as any intersection of arbitrarily many events. The power set also contains the complements to all events. $ \\(\\sigma\\)-$ The \\(\\sigma\\)-algebra, denoted as \\(\\mathbb{A}\\), is the collection of events that are subsets of \\(\\Omega\\) with the following properties: \\[ \\text{(I) } \\Omega \\in \\mathbb{A} \\text{ and } \\emptyset \\in \\mathbb{A}\\\\ \\text{(II) }\\text{if event } E \\in \\mathbb{A} \\text{ then }\\hat{E} \\in \\mathbb{A} \\\\ \\text{(III) }\\text{If the countable sequence of events } E_1, \\dots, E_n \\in \\mathbb{A}, \\text{ then } \\cup^\\infty_{i=1}E_i \\in \\mathbb{A} \\text{ and } \\cap^\\infty_{i=1}E_i \\in \\mathbb{A} \\] Which defines that (I) both the space and the empty set, (II) the complements of any event and (III) both the intersection as well as the union of any event(s) are included. In the case of the dice rolling experiment, this would include all potential values as well as their intersections, combinations and complements. \\(\\sigma\\)- The Borel \\(\\sigma\\)-algebra is mostly used in uncountable spaces. That is, where \\(\\Omega\\) is no longer finite, or countable, implying we have uncountably many potential outcomes. Suppose that we are analyzing the daily logarithmic returns for a common stock or common stock index. Theoretically, any real number is a feasible outcome for a particular day’s return, although we might expect some capping above and below certain values. So, events are characterized by singular values as well as closed or open intervals, such as being interested if the return is at least 10 percent, and each potential outcome in the real space. To design our set of events of the uncountable space \\(\\Omega\\), we take the following approach. We first (I) include “any real number,” which is the space itself, \\(\\Omega\\), as well as the empty space \\(\\emptyset\\). Next, one includes (II) all events of the form “less than or equal to a”, for any real number a. Accordingly, we consider all possible half-open intervals given by \\((-\\infty, a]\\) for any a \\(\\in \\mathbb{R}\\). For each of these half-open intervals, we then add (III) its complement \\((a, \\infty)\\) which expresses the event “greater than a.” Lastly, we include (IV) all possible unions and intersections of everything already in the set of events as well as (V) of the resulting unions and intersections themselves. IN total, the Borel \\(\\sigma\\)-algebra consists of all these sets, intersections, unions and complements for an immeasurable space. It is denoted by \\(\\mathbb{B}\\). 7.1.1.2 Probability Measure There are some formal definitions a probability measure needs to satisfy: : A probability measure should assign each event E from our \\(\\sigma\\)-algebra a nonnegative value corresponding to the chance of this event occurring.\\ : The chance that the empty set occurs should be zero since, by definition, it is the improbable event of “no value.”\\ : The event that “any value” might occur (i.e., 1) should be 1 or, equivalently, 100% since some outcome has to be observable.\\ : If we have two or more events that have nothing to do with one another that are pairwise disjoint or , and create a new event by , the should equal the . More formally, this means: \\[ \\text{(I) } P(\\emptyset) = 0\\\\ \\text{(II) } P(\\Omega) = 1\\\\ \\text{(III) } \\text{ For a countable sequence of events } E_1, \\dots, E_n \\in \\mathbb{A} \\\\\\text{ that are mutually exclusive we have that } P(\\cup_{i=1}^\\infty E_i = \\scriptstyle\\sum_{i=1}^\\infty \\textstyle P(E_i)) \\] 7.1.1.3 Modelling Randomness and Chance: The Probability Space Now, we defined all individual constituents needed to model randomness and chance. By understanding what the space, \\(\\Omega\\), the subsets of events with certain properties, \\(\\sigma\\)-algebra, and the probability measure, P, we defined the triplet {\\(\\Omega\\), \\(\\sigma\\)-algebra, P} that forms the so called . 7.1.1.4 Modelling Randomness and Chance: Probability Measure in Countable and Uncountable Spaces Understanding the differences of P in cases of countability vs. uncountability is key in understanding the important implications for the . Suppose first a countable space \\(\\Omega\\). Here, the probability of any event E in the \\(\\sigma\\)-algebra \\(\\mathbb{A}\\) can be computed by adding the probabilities of all outcomes associated with E. That is: \\[ P(E) = \\scriptstyle\\sum_{\\omega_i \\in E}\\textstyle p(i) \\] where \\(P(\\Omega) = 1\\). In case of the dice rolling experiment, each outcome is associated with a probability of 1/6, formally: \\[ p(\\omega_i) = 1/6 \\] Now, suppose we are in an uncountably large space, given by \\(\\Omega = \\mathbb{R}\\). Here, the \\(\\sigma\\)-algebra is given by the Borel \\(\\sigma\\)-algebra \\(\\mathbb{B}\\) and we can no longer pin the probability of the events E in the space down by simply following the same approach as before. Doing so, we require the use of the : \\(\\textbf{Definition 2.6: Distribution function of P}\\) A function F is a distribution function of the probability measure P if it satisfies the following properties: : F is right-continuous\\ : F is nondecreasing\\ : \\(\\lim_{x\\rightarrow -\\infty} = 0\\) and \\(\\lim_{x\\rightarrow \\infty} = 1\\)\\ : For any \\(x \\in \\mathbb{R}\\), we have \\(F(x) = P((-\\infty, x])\\) This is exactly the foundation of the probability distributions we use in statistics and how to calculate each probability therein. Because, it follows that, for any interval (x,y], we compute the according to: \\[ F(y) - F(x) = P((x,y]) \\] So, in this case we have a function F uniquely related to P from which we derive the probability of any event in \\(\\mathbb{B}\\). Now, if we understand or define which distribution a variable follows, we can pin down the area under the distribution to understand the probability of certain events. To illustrate, the probability of the S&amp;P 500 log return being between -1% and 1% is: \\[ F(0.01) - F(-0.01) = P((–0.01,0.01]) \\] Whereas F is the function given by the probability distribution related to the probability measure, which consists of the space, all sub-spaces (Borel) as well as the probability measure, P. 7.1.2 Random Variables When we refer to some quantity as being a random variable, we want to express that its value is subject to uncertainty, or randomness. Strictly speaking, any random variable of interest is called stochatic. This is in contrast to a deterministic quantity whose value is determined with absolute certainty. As opposed to this, the random variable value is unknown until an outcome of an experiment is observable. A straight-forward way to think about a random variable is the following. Suppose we have a random experiment where some outcome \\(\\omega\\) from the space \\(\\Omega\\) occurs. Depending on this value, the random variables takes some value \\(X(\\omega) = x\\) where \\(\\omega\\) is an input to X. What we observe, finally, is the value x, which is only a consequence of the outcome \\(\\omega\\) of the underlying random experiment. Consequently, a random variable is a function that is completely deterministic and depends on the outcome \\(\\omega\\) of some experiment. As such, we understand random variables as . Mostly, we define random variables as measurable function. Let {\\(\\Omega\\), \\(\\mathbb{A}\\)} and {\\(\\Omega&#39;\\), \\(\\mathbb{A}&#39;\\)} be two measurable spaces and their corresponding \\(\\sigma\\)-algebrae, respectively. Then, a function \\(X: \\Omega \\rightarrow \\Omega&#39;\\) is \\(\\mathbb{A}-\\mathbb{A}&#39;\\)-measurable if, for any set \\(E&#39; \\in \\mathbb{A}&#39;\\), we have: \\[ X^{-1}(E&#39;) \\in \\mathbb{A} \\] In words, a function from one space to another is measurable if \\[ \\text{(I) you can map outcomes } \\omega \\text{ from } \\Omega \\text{ with values } X(\\omega) = x \\text{ in } \\Omega&#39;\\\\ \\text{(II) you can map events } E^{-1} \\text{ in the state space back with } \\sigma-\\text{algebra }, \\\\\\mathbb{A&#39;}, \\text{ to the corresponding origin of } E^{-1} \\text{ in } \\sigma-\\text{algebra } \\mathbb{A} \\text{ of the original probability space} \\] In essence, for each for each event in the state space \\(\\sigma\\)-algebra, \\(\\mathbb{A&#39;}\\), we have a corresponding event in the \\(\\sigma\\)-algebra of the domain space, \\(\\mathbb{A}\\). 7.1.2.1 Discrete and Continuous Random Variables Discrete Random Variables are variables that can take up a limited, or countably large, number of outcomes, \\(\\omega\\), such that \\(\\omega \\in {\\omega_1, \\dots, \\omega_n}\\). As such, with discrete random variables, we are in a countable origin space. As opposed to this, a Continuous Random Variable is a variable that can take on any real number value. That is, we understand that \\(\\omega \\in \\mathbb{R}\\). Based on the definitions from earlier, we are in an infinite, or uncountable, origin space. 7.1.3 Discrete Random Variables and Distributions We now consider the random variables in countably finite spaces and their distributions. The random variables on the countable space will be referred to as discrete random variables. 7.1.3.1 Random Variables in the countable space In cases of discrete random variables, the corresponding probability distribution function (PDF) is denoted as p(x) \\[ p(x) = \\sum_{w_i \\in E} p_i \\] whereas \\(p_i\\) is the probability of the individual outcome \\(\\omega_i\\) in E. Let’s quickly assume and recreate a potential discrete distribution. For that, let’s assume return based probabilities we generate: ret = c(-0.2, 0, 0.15, 0.35, 0.7) probabs = c(0.05, 0.14, 0.46, 0.25, 0.1) plot(ret, probabs, lwd=4, xlab=&quot;Return&quot;, ylab=&quot;Probability&quot;, xaxt=&quot;n&quot;) axis(1, at=ret) 7.1.3.2 Bernoulli Distribution Suppose, we have a random variable X with two possible outcomes. As such, the state space is \\(\\Omega&#39; \\in {x_1, x_2}\\). In general, the Bernoulli distribution is associated with random variables that assume the values \\(x_1 = 1\\) and \\(x_2 = 0\\). The distribution of X is given by the probability for the two outcomes, that is: \\[ p(X = 1) = p_1 = \\pi\\\\ p(X=0) = p_2 = (1-\\pi) \\] Having both the probability and values, we can describe the model as: \\[ p(x) = \\pi^x(1-\\pi)^{1-x} \\] Consequently, the mean of the Bernoulli Distribution is: \\[ 0*(1-\\pi) + 1*\\pi = \\pi \\] And its variance is gien by: \\[ (1-p)^2p+(0-p)^2(1-p) = p(1-p) \\] 7.1.3.3 Binomial Distribution A binomial distribution is basically n linked single Bernoulli trials. In other words, we perform a random experiment with n “independent” and identically distributed Bernoulli random variables, which we denote by B(p). We just assumed Independence and Identical Distribution. This is also known as “IID” assumption. Although we do not cover this in detail, it’s important to understand that means that the outcome of a certain item does not influence the outcome of any others. By we mean that the two random variables’ distributions are the same. This experiment is as if one draws an item from a bin and replaces it into the bin before drawing the next item. As such, we speak of . In general, a binomial random variable X counts the number of “successes” in n repeated Bernoulli trials, denoted as \\(X \\sim B(n,\\pi)\\). To define the probability of X being equal to k, we need to define two concepts. The first determines many different samples of size n are there to yield a i realizations of the outcome. It is called as the and is given by: \\[ \\begin{pmatrix} n \\\\ k \\end{pmatrix} = \\frac{n!}{(n-k)!k!} \\] The second defines the probability measure. Since in each sample the n individual B(p) distributed items are drawn independently, the probability of the sum over these n items is the product of the probabilities of the outcomes of the individual items, given by: \\[ \\pi^k(1-\\pi)^{n-k} \\] Combined, we obtain the probability under a Binomial Distribution, as product of both terms: \\[ P(x = k) = \\begin{pmatrix} n \\\\ k \\end{pmatrix} \\pi^k(1-\\pi)^{n-k} \\] The mean of a Binomial random variable is: \\[ E(x) = np \\] and its variance is: \\[ var(x) = np(1− p) \\] We can easily extend this idea to financial applications. Let’s assume that in each period the stock price can either increase or decrease by i = 10%. Here, probability of increase is given by 0.6 and probability of decline by 0.4. We start with an initial price of 20. According to this outcome, the stock price in t+1 will either be 20(1+0.1) = 22 or 20(1-0.1) = 18. In the third period, the stock price will further deviate according to the same principle and thus we will obtain: \\[ 22*1.1 = 24.20\\\\ 22*0.9 = 19.80\\\\ 18*1.1 = 19.80\\\\ 18*0.9 = 16.20 \\] At t=2, we obtain a new state space, \\(\\Omega&#39;\\), consisting of {16.2, 19.8, 24.2}. In that case, the probability distribution of \\(S_2\\) is given as follows: \\[ P(S_2 = 24.20) = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}\\pi^2(1-\\pi)^0 = 0.6^2 = 0.36 \\\\ P(S_2 = 19.80) = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}\\pi^1(1-\\pi)^{2-1} = 0.48 \\\\ P(S_2 = 16.20) = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}\\pi^0(1-\\pi)^{2-0} = 0.4^2 = 0.16 \\] To get the respective stock returns in t=2, we can use the formula: \\[ S_2 = S_0*1.1^n*0.9^{n-k} \\] \\[ S_t = S_0*(1+i)^k*(1-i)^{n-k} \\] 7.1.3.4 Multinomial Distribution A multinomial distribution follows the same concept as a binomial distribution, with the difference that the outcomes are more than 2. In general cases, we follow n outcomes. Formally, we have that \\(x = {x_1, \\dots, x_n}\\). Whereas the respective probabilities are denoted as \\(p(x) = {p(x_1), \\dots, p(x_n)}\\). As with the Binomial Distribution, we have two distinct components. The first is the and it is given by: \\[ \\begin{pmatrix} &amp; &amp; n\\\\ n_1 &amp; n_2 &amp; n_3 &amp; \\dots &amp; n_k \\end{pmatrix} \\] The second term is again the probability of each event occurring. However, we can no longer find the complement(s), as only one probability of events can be expressed by the others. Thus, we just work with occurrences: \\[ \\pi_1^{n_1} * \\pi_2^{n_2} * \\pi_3^{n_3} * \\dots * \\pi_k^{n_k} \\] Together, we obtain the Multinomial probability for a given event: \\[ P(x_1 = n_1, x_2 = n_2, x_3 = n_3, \\dots, x_k = n_k) = \\begin{pmatrix} &amp; &amp; n\\\\ n_1 &amp; n_2 &amp; n_3 &amp; \\dots &amp; n_k \\end{pmatrix} \\pi_1^{n_1} * \\pi_2^{n_2} * \\pi_3^{n_3} * \\dots * \\pi_k^{n_k} \\] Here, the respective Expected Value is: \\[ E(x_k) = p_k*n \\] and the correspoding Variance: \\[ var(x_k) = p_k*(1-p_k)*n \\] We can easily replicate the ideas formed in the stock price movements to multinomial perspectives. For that, let’s assume that we have now three distinct outcomes. That is, the stock can either increase by 10%, stay the same or decline by 10%. As such, we define the respective movements as \\(Y_u = 1.1, Y_s = 1.0, Y_d = 0.9\\). The respective probabilities are said to be \\(p_u = 0.25, p_s = 0.5, p_d = 0.25\\). Our new state space consists of six possible outcomes: \\[ \\Omega &#39; = [(u,s,d)] = [(2,0,0), (0,2,0), (0,0,2), (1,1,0), (1,0,1), (0,1,1)] \\] And the corresponding prices are: \\[ S_2 = S_0*p_u^{n_u}*p_s^{n_s}*p_d^{n_d} \\in [16.2, 18, 19.8, 20, 22, 24.2] \\] These are the multinomial coefficients we use for calculation of the probability for x being equal to some value. Consequently, we get the following probabilities: \\[ P(S = 24.4) = \\begin{pmatrix} &amp; 2 \\\\ 2 &amp; 0 &amp; 0\\end{pmatrix}p_up_u = 0.0625 \\\\ P(S = 22) = \\begin{pmatrix} &amp; 2 \\\\ 1 &amp; 1 &amp; 0\\end{pmatrix}p_up_s = 0.25 \\\\ P(S = 20) = \\begin{pmatrix} &amp; 2 \\\\ 0 &amp; 2 &amp; 0\\end{pmatrix}p_sp_s = 0.25 \\\\ P(S = 19.8) = \\begin{pmatrix} &amp; 2 \\\\ 1 &amp; 0 &amp; 1\\end{pmatrix}p_up_d = 0.125 \\\\ P(S = 18) = \\begin{pmatrix} &amp; 2 \\\\ 0 &amp; 1 &amp; 1 \\end{pmatrix}p_sp_d = 0.25 \\\\ P(S = 16.2) = \\begin{pmatrix} &amp; 2 \\\\ 0 &amp; 0 &amp; 2 \\end{pmatrix}p_dp_d = 0.0625 \\\\ \\] 7.1.4 Continuous Random Variables and Distributions As previously mentioned, within the scope of continuous distributions, we no longer have a countable space \\(\\Omega\\) we can rely on. That is, the different outcomes, \\(\\omega\\) are uncountable. Technically, without limitations caused by rounding to a certain number of digits, we could imagine that any real number could provide a feasible outcome, thereby the subsets is given by the Borel \\(\\sigma\\)-algebra, \\(\\mathbb{B}\\), which is based on all half-open intervals from \\((-\\infty, a]\\) for any \\(a \\in \\mathbb{R}\\). As the space set is uncountable, we need a unique way to assign a probability to a certain event. Recall that, as just described, the subsets in an uncountable space are given by all half-open intervals from \\((-\\infty, a]\\). We can make use of this property by introducing a which expresses the \\((-\\infty, a]\\) occurs. That is, the probability that a . In said case, F(a) states the . To be a little more concise, we assume that the Continuous Distribution Function, F(a), has the following properties: \\(\\lim_{x \\rightarrow -\\infty} \\rightarrow 0\\) \\ \\(\\lim_{x \\rightarrow \\infty} \\rightarrow 1\\) \\ \\(F(b) - F(a) \\geq 0 for b \\geq a\\) \\ \\(\\lim_{x \\downarrow a} F(x) = F(a)\\) These Properties state (I) Behaviour in Extremes (II) Monotonically Increasing behaviour (III) Right-Continuity As the set of events in real numbers are uncountably many, pinning down an exact number is zero. As such, we generally assign probabilities in the following way: \\[ P((a,b)) = F(b) - F(a) \\] Whereas \\(F(b) = P((-\\infty, b]))\\) and \\(F(a) = P((-\\infty, a]))\\). That is, the entire probability that an outcome of at most a occurs is subtracted from the greater event that an outcome of at most b occurs, implying: \\[ (a,b] = (-\\infty, b) / (-\\infty, a] \\] To assign probabilities in the continuous way, however, we need to define certain knowledge of the distribution function F. 7.1.4.1 Density Function: General Case The continuous distribution function F of a probability measure P on \\(\\mathbb{R}, \\mathbb{B}\\) is defined as follows: \\[ F(x) = \\int^x_{-\\infty}f(t) dt \\] where f(t) is the of the probability measure P. We interpret the density function equation accordingly: Since, at any real value x the distribution function uniquely equals the probability that an outcome of at most x is realized (\\(F(x) = P((-\\infty, x])\\)), the density function states that this probability is obtained by \\(-\\infty\\) . We interpret this function as the . This follows the subsequent logic. We know that with continuous distribution functions, the probability of exactly a value of x occurring is zero. However, the probability of observing a value between x and some very small step to the right, denoted as \\(\\triangle\\) x (i.e. [x, x+\\(\\triangle\\) x]), is necessarily zero. As such, between this increment of x and \\(\\triangle\\) x, the distribution function F increases by exactly this probability. That is, the increment is: \\[ F(x + \\triangle x) - F(x) = P(X \\in [x, x + \\triangle x)) \\] Now, dividing this equation by the width of the interval, denoted as \\(\\trianlge\\) x, we obtain the per unit step on this interval. If we reduce the step size \\(\\triangle\\) x to an infinitesimally small step, \\(\\delta x\\), this average approaches the , which we denote f. This is the . \\[ \\lim_{\\triangle \\rightarrow 0} \\frac{F(x+\\triangle x) - F(x)}{\\triangle x} = \\frac{\\delta F(x)}{\\delta(x)} = f(x) \\] This equation is quite fundamental for continuous probability. Here, ee divide the probability that some realization should be inside of the small interval by that interval step. And, by letting that interval shrink to width zero, we obtain the marginal rate of growth or, equivalently, the derivative of F. Hence, we call f the probability density function or simply the density function. Commonly, it is abbreviated as pdf. From the equation above, we understand that the probability of some occurrence of at most x is given by integration of the density function f over the interval \\((-\\infty, x]\\). This follows the respective steps: For a given outcome, calculate the increment of x and \\(\\triangle\\) x, and divide this equation by the width of the interval to get the marginal rate of growth At each value t, we multiply the corresponding density f(t) by the infinitesimally small interval width dt. Finally, we integrate all values of f (weighted by dt) up to x to obtain the probability for \\((-\\infty, x]\\) In the end, the integral of this marginal rate of growth of F in the interval at x is exactly how the probability \\(P((-\\infty, x])\\) is derived through integrating the marginal rate f over the interval \\((-\\infty, x]\\) with respect to the values. The resulting total probability is then given by the area under the curve in the below figure. mean=80; sd=10 lb=60; ub=100 x &lt;- seq(-4,4,length=100)*sd + mean hx &lt;- dnorm(x,mean,sd) plot(x, hx, type=&quot;n&quot;, xlab=&quot;x&quot;, ylab=&quot;pdf&quot;) i &lt;- x &gt;= lb &amp; x &lt;= ub lines(x, hx) polygon(c(lb,x[i],ub), c(0,hx[i],0), col=&quot;red&quot;) area &lt;- pnorm(mean, sd) - pnorm(lb, mean) result &lt;- paste(&quot;P(&quot;,lb,&quot;&lt; IQ &lt;&quot;,ub,&quot;) =&quot;, signif(area, digits=3)) The area representing the value of the interval is indicated by the red block. So, the probability of some occurrence of at least a and at most b is given by the area inside red. Based on the notions above, the probability of \\(X \\in {a,b}\\) is given by: \\[ P(X \\in (a,b]) = \\int^b_{a}f(t) dt \\] 7.1.5 The cumulative Distribution Before we dig into distributions with appealing properties for our statistical analysis, we first define some important concepts of distribution functions. The first is related ot the cumulative distribution. In general, the cumulative distribution function (CDF) of a random variable assigns the probability of a random variable X to be smaller than or equal to a given threshold. It can be also interpreted as a half-closed interval consisting of the entire space left to a certain threshold. Formally: \\[ F_X(x) = P(X\\leq x) \\] The most important properties are: \\[ \\text{Property 1: } \\text{If } x_1 &lt; x_2, \\text{then } F(x_1) &lt; F(x_2) \\\\ \\text{Property 2: } F_X(-\\infty) = 0 \\\\ \\text{Property 3: } F_X(\\infty) = 1 \\\\ \\text{Property 4: } P(X &gt; x) = 1 - F_X(x) \\\\ \\text{Property 5: } P(x_1 &lt; X \\leq x_2) = F_X(x_2) - F_X(x_1) \\] We can easily show an example for both discrete as well as continuous distributions: d=data.frame(x=c(0,1,2,4,5,7,8,9, 10), cdf=c(0,0.1,0.2,0.3,0.5,0.6,0.7,1, 1)) ggplot() + geom_step(data=d, mapping=aes(x=x, y=cdf), direction=&quot;vh&quot;, linetype=3) + geom_point(data=d, mapping=aes(x=x, y=cdf), color=&quot;red&quot;) + ylab(&quot;CDF&quot;) + xlab(&quot;x&quot;) + ggtitle(&quot;CDF for discrete distribution&quot;) + theme(plot.title= element_text(size=14, color=&quot;grey26&quot;, hjust=0.5,lineheight=2.4), panel.background = element_rect(fill=&quot;#f7f7f7&quot;), panel.grid.major.y = element_line(size = 0.5, linetype = &quot;solid&quot;, color = &quot;grey&quot;), panel.grid.minor = element_blank(), panel.grid.major.x = element_blank(), plot.background = element_rect(fill=&quot;#f7f7f7&quot;, color = &quot;#f7f7f7&quot;), axis.line = element_line(color = &quot;grey&quot;)) ggplot(data.frame(x = c(-5, 5)), aes(x = x)) + stat_function(fun = pnorm) + ylab(&quot;CDF&quot;) + xlab(&quot;x&quot;) + ggtitle(&quot;CDF for continuous distribution&quot;) + theme(plot.title= element_text(size=14, color=&quot;grey26&quot;, hjust=0.5,lineheight=2.4), panel.background = element_rect(fill=&quot;#f7f7f7&quot;), panel.grid.major.y = element_line(size = 0.5, linetype = &quot;solid&quot;, color = &quot;grey&quot;), panel.grid.minor = element_blank(), panel.grid.major.x = element_blank(), plot.background = element_rect(fill=&quot;#f7f7f7&quot;, color = &quot;#f7f7f7&quot;), axis.line = element_line(color = &quot;grey&quot;)) 7.1.5.1 Quantile values of Distributions : Given a random variable X with a continuous CDF F_X(x), for any \\(\\alpha\\), where 0 \\(\\leq \\alpha \\leq 1\\), the \\(100*\\alpha\\) % quantile of the distribution for X is given as the value \\(q_\\alpha\\) that satisfies: \\[ F_X(q_a) = P(X \\leq q_\\alpha) = \\alpha \\] In essence, the definition implies that the quantile distribution incorporates all values of a distribution up to a specific threshold such that exactly \\(\\alpha\\) % of the entire distribution are included within that range. Important examples that are often used in statistics include the 25% quantile, the median (50% quantile), the 75% quantile as well as minimum and maximum values. For instance, the median of the distribution, \\(q_{0.5}\\) satisfies the following: \\[ F_X(q_{0.5}) = P(X \\leq q_{0.5}) = 0.5 \\] In the case that \\(F_X\\) is invertible, then \\(q_{\\alpha}\\) can be determined as: \\[ q_\\alpha = F_X^{-1}(\\alpha) \\] That is, by using the inverse cdf \\(F_X^{-1}\\), one can determine the quantile value for a given threshold of the underlying distribution. Looking again at the median example, the 50% quantile value can be determined as: \\[ q_{0.5} = F_X^{1}(0.5) \\] This inverse is also called . Applying this in R is relatively straight-forward. Given the standard normal distribution, the quantile value can be determined by solving: \\[ q_\\alpha = \\Phi^{-1}(\\alpha) \\] Where \\(\\Phi^{-1}\\) denotes the inverse of the cdf of the standard normal distribution with the function qnorm(). Let’s use it to print the critical values of our normal distribution that we usually use for significance tests. # Define the functions critical_10 &lt;- qnorm(0.95,mean=0,sd=1) critical_5 &lt;- qnorm(0.975,mean=0,sd=1) critical_1 &lt;- qnorm(0.995,mean=0,sd=1) criticals &lt;- round(cbind(critical_10, critical_5, critical_1),2) colnames(criticals) &lt;- c(&quot;10% Significance (2 Tailed)&quot;, &quot;5% Significance (2 Tailed)&quot;, &quot;1% Significance 21 Tailed)&quot;) criticals ## 10% Significance (2 Tailed) 5% Significance (2 Tailed) 1% Significance 21 Tailed) ## [1,] 1.64 1.96 2.58 Accordingly, with the quantile function, we can obtain critical values of given distributions. 7.1.6 Continuous Distributions with Appealing Properties Next, we discuss the more commonly used distributions with appealing statistical properties that are used in finance. These are the normal distribution, the student’s t distribution, chi-2 distribution, Fisher F distribution and log-normal distribution. 7.1.6.1 The Normal Distribution The normal distribution, or Gaussian, is the most common distribution used in finance. It is defined by two parameters: its mean \\(\\mu\\) as well as its standard deviation \\(\\sigma\\). It is denoted by \\(N(\\mu, \\sigma)\\). The PDF of the normal distribution is given by: \\[ f(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-(x-\\mu)^2/2\\sigma^2} \\] We can easily print the pdf of the normal distribution using the function rnorm() dat &lt;- read.table(text = &quot;info mean sd info1 0 1 info2 1 0.5 info3 2 1 &quot;, header = TRUE) densities &lt;- apply(dat[, -1], 1, function(x) rnorm(n = 100000, mean = x[1], sd = x[2])) colnames(densities) &lt;- dat$info densities.m &lt;- melt(densities) ## Warning in melt(densities): The melt generic in data.table has been passed a matrix and will attempt to redirect to the relevant reshape2 method; ## please note that reshape2 is deprecated, and this redirection is now deprecated as well. To continue using melt methods from reshape2 while both ## libraries are attached, e.g. melt.list, you can prepend the namespace like reshape2::melt(densities). In the next version, this warning will become ## an error. #Plot densities.m %&gt;% ggplot(aes(x = value, fill = Var2, color = Var2)) + geom_density(alpha = 0.2) + theme_bw() + xlim(-5,5) ## Warning: Removed 117 rows containing non-finite values (stat_density). A problem is that the distribution function cannot be solved for analytically and therefore has to be approximated numerically. That is: \\[ P(a \\leq X \\leq b) = \\int^b_a \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}x^2}dx \\] does not have a closed form solution. In the particular case of the standard normal distribution, the values are tabulated. Standard statistical software provides the values for the standard normal distribution. Some useful approximations are: \\[ \\begin{align*} P(X \\in [\\mu \\pm \\sigma]) \\approx 0.68\\\\ P(X \\in [\\mu \\pm 2\\sigma]) \\approx 0.95\\\\ P(X \\in [\\mu \\pm 3\\sigma]) \\approx 0.99 \\end{align*} \\] The above states that approximately 68% of the probability is given to values that lie in an interval of one standard deviation around the mean of the distribution. 7.1.6.2 Chi-2 Distribution In this distribution, let Z be a standard normal random variable, in brief \\(Z \\sim N(0,1)\\), and \\(X = Z^2\\). Then X is distributed chi-square with one degree of freedom, denoted as \\(X \\sim \\chi^2(1)\\). The PDF of the Chi-2 Distribution is given as: \\[ f(X) = \\frac{1}{2^{n/2}\\Gamma(\\frac{n}{2})}e^{-\\frac{x}{2}}x^{(\\frac{n}{2}-1)} \\] for any x \\(\\geq\\) 0. Here \\(\\Gamma(z) = \\int^\\infty_0 t^{z-1}e^{-t}dt\\) denotes the gamma function. In general, Degrees of Freedom (DOF) indicate how many independently behaving standard normal random variables the resulting variable is composed of. Here, X is only composed of one, called Z. In general, this distribution is characterised by its DOF. If we have n distributed random variables that are all independent of each other, then their sum is written as: \\[ S = \\sum_{i=1}^n \\textstyle X \\sim \\chi^2(n) \\] The corresponding properties are: mean of E(x) = n and variance of Var(x) = 2n. So, the mean and variance are directly related to the degrees of freedom. An important feature of the \\(chi^2\\) distribution is the degrees of freedom equal the number of independent \\(\\chi^2 (1)\\) distributed \\(X_i\\) in the sum. Consequently, the summation of any two chi-squared distributed random variables is itself chi-square distributed. The \\(chi^2\\) distribution is drawn with the function `rchisq()`` as follows: a &lt;- rchisq(100000, df = 2) b &lt;- rchisq(100000, df = 3) c &lt;- rchisq(100000, df = 7) d &lt;- rchisq(100000, df = 10) df_chi &lt;- as.data.frame(cbind(a,b,c,d)) colnames(df_chi) = c(&quot;DOF=2&quot;, &quot;DOF=3&quot;, &quot;DOF=7&quot;, &quot;DOF=10&quot;) df_chi_melt &lt;- melt(df_chi) #Plot df_chi_melt %&gt;% ggplot(aes(x = value, fill = variable, color = variable)) + geom_density(alpha = 0.2) + theme_bw() + xlim(0,20) 7.1.6.3 Student’s t-Distribution Basically, the Student’s t-distribution has a similar shape to the normal distribution, but thicker tails. For large degrees of freedom n, the Student’s t-distribution does not significantly differ from the standard normal distribution. If X has a Student’s t distribution with degrees of freedom parameter v, then the PDF has the form: \\[ f(x) = \\frac{\\Gamma(\\frac{v+1}{2})}{\\sqrt{v\\pi}\\Gamma(\\frac{v}{2})}\\left(1 + \\frac{x^2}{v}\\right)^{-\\frac{(v+1)}{2}} \\] where \\(\\Gamma(z) = \\int^\\infty_0 t^{z-1}e^{-t}dt\\) denotes the gamma function. Especially, it has the following properties: \\[ E[X] = 0\\\\ Var(x) = \\frac{v}{v-2}, v &gt; 2 \\\\ Skew(x) = 0, v &gt; 3 \\\\ Kurt(x) = \\frac{6}{v-4}, v &gt; 4 \\] The parameter v controls the scale and tail thickness of the distribution. If v is close to four, then the kurtosis is large and the tails are thick. In general, the lower the degrees of freedom, the heavier the tails of the distribution, making extreme outcomes much more likely than for greater degrees of freedom or, in the limit, the normal distribution. The t-distribution is displayed with the function rt()` below. Note that we also add the normal distribution to show that the tails for both t distributions are fatter, but the fattest are for the t distribution with the lowest DOF. a &lt;- rnorm(100000, mean = 0, sd = 1) b &lt;- rt(100000, df = 1) c &lt;- rt(100000, df = 5) d &lt;- rt(100000, df = 10) df_norm_t &lt;- as.data.frame(cbind(a,b,c,d)) colnames(df_norm_t) = c(&quot;N(0,1)&quot;, &quot;t, DOF=1&quot;, &quot;t, DOF=5&quot;, &quot;t, DOF=10&quot;) df_norm_t_melt &lt;- melt(df_norm_t) ## Warning in melt(df_norm_t): The melt generic in data.table has been passed a data.frame and will attempt to redirect to the relevant reshape2 ## method; please note that reshape2 is deprecated, and this redirection is now deprecated as well. To continue using melt methods from reshape2 while ## both libraries are attached, e.g. melt.list, you can prepend the namespace like reshape2::melt(df_norm_t). In the next version, this warning will ## become an error. ## No id variables; using all as measure variables #Plot df_norm_t_melt %&gt;% ggplot(aes(x = value, fill = variable, color = variable)) + geom_density(alpha = 0.2) + theme_bw() + xlim(-5,5) ## Warning: Removed 13112 rows containing non-finite values (stat_density). 7.1.6.4 F-Distribution The F-Distribution is similar to the \\(\\chi^2\\) distribution, but with two variables. The PDF of the F-Distribution is defined as: \\[ f(X) = \\frac{F(\\frac{n_1 + n_2}{2})}{F(\\frac{n_1}{2}) + F(\\frac{n_2}{2})}\\cdot\\left(\\frac{n_1}{n_2}\\right)^{n_1/2} \\cdot \\frac{x^{n_1/2 - 1}}{\\left[1+x\\cdot\\frac{n_1}{2}\\right]^{\\frac{n_1+n_2}{2}}} \\] for any x \\(\\geq\\) 0. Here we let both \\(X \\sim \\chi^2(n_1)\\) and \\(Y \\sim \\chi^2(n_2)\\) and then \\(F(n_1, n_2)\\) is defined as: \\[ F(n_1, n_2) = \\frac{Y/n_1}{X/n_2} \\] This ratio has an F-distribution with \\(n_1\\) and \\(n_2\\) DOF from the underlying \\(\\chi^2\\) distribution for X and Y, respectively. Also like the chi-square distribution, the F-distribution is skewed to the right. The first two moments of the F-distribution are the following: The mean is given by E(X) = \\(\\frac{n_2}{n_2 - 2}\\) for \\(n_2 &gt; 2\\), and the variance is given by Var(X) = \\(\\frac{2n_2^2(n_1 + n_2 -2)}{n_1(n_2 - 2)^2(n_2 - 4)}\\) for \\(n_2 &gt; 4\\). The F-Distribution values can be determined with the function r(f) and looks like this: a &lt;- rf(100000, df1 = 4, df2 = 4) b &lt;- rf(100000, df1 = 4, df2 = 10) c &lt;- rf(100000, df1 = 10, df2 = 4) d &lt;- rf(100000, df1 = 10, df2 = 100) df_F &lt;- as.data.frame(cbind(a,b,c,d)) colnames(df_F) = c(&quot;n1=4, n2=4&quot;, &quot;n1=4, n2=10&quot;, &quot;n1=10, n2=4&quot;,&quot;n1=10, n2=100&quot;) df_F_melt &lt;- melt(df_F) #Plot df_F_melt %&gt;% ggplot(aes(x = value, fill = variable, color = variable)) + geom_density(alpha = 0.2) + theme_bw() + xlim(0,7) 7.1.6.5 Log-Normal Distribution The last important function we look at is the log-normal distribution. It is directly linked to the standard normal distribution. To see this, let X be a normally distributed random variable with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Then the random variable \\[ X = e^Y \\] is log-normally distributed also with mean \\(\\mu\\) and variance \\(\\sigma^2\\). This distribution is denoted as \\(X \\sim Ln(\\mu, \\sigma^2)\\). The support of the log-normal distribution is on the positive half of the real line, as the exponential function can only take up positive values. Accordingly, the PDF of the log-normal distribution is given by: \\[ f(X) = \\frac{1}{x\\sigma\\sqrt{2\\pi}}e^{\\frac{(\\ln x - \\mu)^2}{2\\sigma^2}} \\] for any x &gt; 0. The density function is also similar to the normal distribution and results in the log-normal distribution function: \\[ F(x) = \\Phi\\left(\\frac{\\ln x - \\mu}{\\sigma}\\right) \\] The log-normal distribution values can be calculate using the rlnorm() function: a &lt;- rlnorm(100000, meanlog = 0, sdlog = 1) b &lt;- rlnorm(100000, meanlog = 0, sdlog = 0.5) c &lt;- rlnorm(100000, meanlog = 0, sdlog = 2) d &lt;- rlnorm(100000, meanlog = 1, sdlog = 1) df_lnorm &lt;- as.data.frame(cbind(a,b,c,d)) colnames(df_lnorm) = c(&quot;mean=0, sd=1&quot;, &quot;mean=0, sd=0.5&quot;, &quot;mean=0, sd=2&quot;,&quot;mean=1,sd=1&quot;) df_lnorm_melt &lt;- melt(df_lnorm) #Plot df_lnorm_melt %&gt;% ggplot(aes(x = value, fill = variable, color = variable)) + geom_density(alpha = 0.2) + theme_bw() + xlim(-1,7) 7.1.6.6 Functions for distribution calculations (d, p, q, r) To create distributions of the forms above, we use distribution packages. In these distribution packages, we can make use of four distinct functions that can apply all the theory we just discussed. For that, we will look at the normal distribution. As previously, each statistical property of the distribution can be calculated using the functions dnorm(), pnorm(), qnorm() and rnorm(). We will now cover what each of these functions does. dnorm The dnorm() returns the value of of the probability density function for the distribution of interest, given the parameters x, \\(\\mu\\) and \\(\\sigma\\). That is, for a given input x with distributional moments of the mean and variance, we obtain the corresponding value on the y-axis, which indicates the density function of that value. # This is the largest density of the PDF given that we have the normal distribution with mean = 0 &amp; sd = 1 dnorm(0, mean = 0, sd = 1) ## [1] 0.3989423 pnorm The pnorm() returns the integral from \\(-\\infty\\) to q of the pdf of a certain distribution, whereas q is a z-score. That is, for a given x, pnorm() returns the value of the y axis on the cdf, also known as probability density. Consequently, with this function you obtain the cdf functional value. In general, it is the function that replaces the table of probabilities and Z-scores at the back of the statistics textbook. In general, pnorm is used to get the probability that \\(-\\infty &lt; X \\leq x\\), and, as such, it gives the p-value for a distirbution. # This is the median value of the respective distribution. As such, it comprises of exactly half the overall density of the underlying distribution. This is intuitive, given that the value -1.96 in a normal distribution with mean = 0 and sd = 1 is exactly the middle value, thereby incorporating half of the entire area under the cdf. pnorm(-1.96, mean = 0, sd = 1) ## [1] 0.0249979 We deliberately chose -1.96 b/c, in a two-sided test when assuming normal distribution, then \\(-\\infty &lt; -1.96 \\leq x\\) and \\(x \\leq -1.96 &lt; \\infty\\) constitute approximately 5% of the probability mass under the curve, or, in other words, 5% of the total probability. qnorm The qnorm() is the inverse of the pnorm() function. Looking at the quantiles part, it is the function that returns the inverse values of the cdf. You can use this function to determine the p’th quantile value of the underlying distribution (e.g. which value incorporates exactly half of the overall area under the curve). # Intuitively, pnorm() inverse for a value of 0.5 indicates what value the 50% quantile must have under the given distribution characteristics. As such, we see that the 5% quantile here must have value -1.96! qnorm(0.025, mean = 0, sd = 1) ## [1] -1.959964 Consequently, it gives us the value for a given probability. That is, here a one-sided probability mass of 2.5% would require that the corresponding value is approximately -1.96, thereby stating that -1.96 is at the 2.5’th percentile of the distribution. rnorm Lastly, the rnorm() is used to generate vector of numbers that follow a certain distribution and its characteristics. We used this function to generate an order of numbers to plot subsequently and show the plotted distributions. rnorm(10, mean = 0, sd = 1) ## [1] 0.7175373 -0.5227276 -0.8529029 0.4184272 0.5607154 -0.5526390 0.6421609 -1.8744686 0.2432041 -1.2057285 Although we just showed the functions for the normal distribution, R offers a great amount of functions for other distributions that can be used identically as the normal case. To give you an overview: Neatly, nearly all of these functions can be used with the prefixes “d,p,q,r”. Thus, for instance, just write rlnorm to get the values of a log-normal distribution. 7.1.7 Moments and Properties of bivariate distributions Bivariate and Multivariate distributions are important concepts in asset management settings. This is because each asset can be regarded as a random variable. In order to be able to form portfolios, we thus need to understand how different assets relate with each other and what their common covarying structure is. To do so, let’s look at the fundamental concepts and moments first. 7.1.7.1 Bivariate Distributions for continuous random variables The joint probability for two random variables is characterised using their (PDF), called f(x,y), such that: \\[ \\int^\\infty_{-\\infty}\\int^\\infty_{-\\infty}f(x,y)dxdy = 1 \\] Note that the joint probability distribution is plotted in a three-dimensional space. To find the joint probabilities of \\(x_1 \\leq X \\leq x_2\\) and \\(y_1 \\leq Y \\leq y_2\\), we must find the volume nder the probability surface over the grid where the intervals \\([x_1, x_2]\\) and \\([y_1, y_2]\\) are overlapping. That is: \\[ P(x_1 \\leq X \\leq x_2, y_1 \\leq Y \\leq y_2) = \\int^{x_2}_{x_1}\\int^{y_2}_{y_1}f(x,y)dxdy \\] 7.1.7.2 Standard bivariate normal distribution An important bivariate distribution constitutes the standard bivariate normal distribution. It has the fofm: \\[ f(x,y) = \\frac{1}{2\\pi}e^{-\\frac{1}{2}(x^2+y^2)} dx dy \\] 7.1.7.3 Marginal Distributions The marginal distribution treats the data as if only the one component was observed while a detailed joint distribution in connection with the other component is of no interest. In other words, the joint frequencies are projected into the frequency dimension of that particular component. The frequency of certain values of the component of interest is measured by the marginal frequency. The marginal frequency of X is calculated as sum of all frequencies of X given that Y takes on a particular value. Thus, we obtain the row sum as the marginal frequency of this component X. That is, for each value \\(X_i\\), we sum the joint frequencies over all pairs (\\(X_i\\), \\(Y_j\\)) where \\(Y_j\\) is held fix. Formally, this is: \\[ f_x(X_i) = \\sum_jf(X_i, Y_j) \\] where the sum is over all values \\(W_j\\) of the component Y. Lastly, the marginal pdf of X is found by integrating y out of the joint PDF f(X,Y): \\[ f(x) = \\int^\\infty_{-\\infty} f(x,y)dy \\] 7.1.7.4 Conditional Distributions The conditional probability that X = x given that Y = y is defined as: \\[ f(x|y) = f(X=x|Y=y) = \\frac{f(x,y)}{f(y)} \\] Whereas an analogous principle holds for the conditional probability of y on x. The use of conditional distributions reduces the original space to a subset determined by the value of the conditioning variable. In general, we need conditional distributions, or probability, to define what value x takes given that y takes a certain value. Consequently, we no longer are within a field of independence between two variables when we include conditional probability. Conditional moments are different to unconditional ones. As such, the conditional expectation and variance are defined as follows. For discrete random variables, X and Y, the conditional expectation is given as: \\[ E[X|Y=y] = \\sum_{x\\in S_X}x\\cdot P(X=x|Y=y) \\] For discrete random variables, X and Y, the conditional variance is given as: \\[ var(X|Y=y) = \\sum_{x\\in S_X}(x-E[X|Y=y])^2\\cdot P(X=x|Y=y) \\] To do so, we first look at the properties of covariance and correlation 7.1.7.5 Independence The previous discussion raised the issue that a component may have influence on the occurrence of values of the other component. This can be analyzed by comparison of the joint frequencies of x and y with the value in one component fixed, say x = X. If these frequencies vary for different values of y, then the occurrence of values x is not independent of the value of y. This is equivalent to check whether a certain value of x occurs more frequently given a certain value of y. That is, check the conditional frequency of x conditional on y, and compare this conditional frequency with the marginal frequency at this particular value of x. If the conditional frequency is not equal to the marginal frequency, then there is no independence. Formally, two random variables are independent if: \\[ f_{x|y}(x,y) = f(x)\\cdot f(y) \\] That is, the joint frequency is the mathematical product of their respective marginals. Independence is a handy feature as it allows us to compare marginal and conditional distribution properties of random variables. 7.1.7.6 Correlation and Covariance Covariance and correlation describe properties of the combined variation of two or more assets. As the term describes, they measure to what extent assets covary. Thereby, they quantify the level of similarity of of movements over time for different variables. The covariance between two random variables, X and Y, is given as: \\[ \\sigma_{XY} = cov(X,Y) =E[(X - E(X))(Y - E(Y))] \\] The correlation between two random variables, X and Y, is given as: \\[ \\rho_{XY} = cor(X,Y) = \\frac{\\sigma_{XY}}{\\sigma_X\\sigma_Y} \\] Covariance and Correlations have certain nice properties we can use. Important properties of the are: cov(X,X) = var(X) cov(X,Y) = cov(Y,X) cov(X,Y) = E[XY] -E[X]E[Y] cov(aX,bY) = abcov(X,Y) cov(X,Y) = 0 if X and Y independent Let’s quickly show the third and fourth property: \\[ \\begin{align} cov(X,Y) &amp;= E[(X-E(X))(Y-E(Y))] \\\\ &amp;= E[XY -E(X)Y -E(Y)X +E(X)E(Y)] \\\\ &amp;= E[XY] - E(X)E(Y) - E(X)E(Y) + E(X)E(Y)\\\\ &amp;= E[XY] - E(X)E(Y) \\end{align} \\] \\[ \\begin{align} cov(aX,bY) &amp;= E[(aX - aE(X))(bY - bE(Y))] \\\\ &amp;= a\\cdot b\\cdot E[(X-E(X))(Y-E(Y))] \\\\ &amp;= a\\cdot b\\cdot cov(X,Y) \\end{align} \\] Important properties of the are: \\(-1\\leq \\rho_{xy} \\leq 1\\) $_{xy} = 1 $: Perfect positive linear relation $_{xy} = -1 $: Perfect negative linear relation 7.1.7.7 Expectation and variance of the sum of two random variables Joint distributions are important when considering asset prices. They define how to compute important properties when considering multiple assets. When considering joint distributions, two important properties can be shown. The first relates to the expected value of a linear combination. Especially, it holds that, for two random variables with defined means and covariance matrices: \\[ \\begin{align} E[aX + bY] &amp;= \\sum_{x\\in S_X}\\sum_{y \\in S_Y} (ax + by)P(X=x, Y=y) \\\\ &amp;= \\sum_{x\\in S_X}\\sum_{y \\in S_Y} (ax)P(X=x, Y=y) + \\sum_{y\\in S_Y}\\sum_{x \\in S_X} (by)P(X=x, Y=y) \\\\ &amp;= a\\sum_{x\\in S_X}x\\sum_{y \\in S_Y}P(X=x, Y=y) + b\\sum_{y\\in S_Y}y\\sum_{x \\in S_X}P(X=x, Y=y)\\\\ &amp;= a\\sum_{x\\in S_X}xP(X=x) + b\\sum_{y\\in S_y}yP(Y=y) &amp;&amp; \\text{sum of all y options renders condition = 1}\\\\ &amp;= aE[X] + bE[Y] \\\\ &amp;= a\\mu_X + b\\mu_Y \\end{align} \\] This means that expectation is additive. The second result relates to the variance of a linear combination. Especially, it holds that, for two random variables with defined means and covariance matrices: \\[ \\begin{align} var(aX + bY) &amp;= E[(aX + bY - E[aX]E[bY])^2]\\\\ &amp;= E[((aX - E(aX)) + (bY - E(bY)))^2] \\\\ &amp;= E[(a(X-E(X)) + b(Y-E(Y)))^2] \\\\ &amp;= a^2E[X-E(X)]^2 + b^2(Y-E(Y))^2 + 2 ab(X-E(X))(Y-E(Y))\\\\ &amp;= a^2\\cdot var(X) + b^2\\cdot var(Y) + 2\\cdot a \\cdot b \\cdot cov(X,Y) \\end{align} \\] That is, the variance of a linear combination of random variables is itself not linear. This is due to the covariance term when computing the variance of the sum of two random variables that are not independent. This means that the variance is not additive. Both properties are inherently important when considering both portfolio return as well as risk characteristics. 7.1.8 Moments of Probability Distributions Moments of a distribution generally tell us things about the center, spread, the distribution as well as the shape behaviour of the underlying distribution. As such, they are important to understand the baseline configuration of a distribution. In our case, we will look at four moments: Expected Value (mean) Variance Skewness Kurtosis 7.1.8.1 Expected Value (Mean) The expected value of a random variable X measures the center of mass for the underlying PDF. The expected value of a random variable X is given by: \\[ \\mu_x = E[X] = \\sum x\\cdot P(X=x) \\] 7.1.8.2 Variance and Standard Deviation The variance of a random variable X measures the spread around the mean. As such, it measures the spread of the distribution. The variance and standard deviation of a random variable X are given by: \\[ \\sigma_X^2 = E[(X-\\mu_x)^2]\\\\ \\sigma_X = \\sqrt{\\sigma_X^2} \\] 7.1.8.3 Skewness The skewness of a random variable X measures the symmetry of a distribution around its mean. It is given by: \\[ skew(X) = \\frac{E[(X-\\mu_x)^3]}{\\sigma_X^3} \\] If X has a symmetric distribution, then skew(X) = 0, as values above and below the mean cancel each other out. There are two special cases: For skew(X) &gt; 0, the distribution has a long right tail. For skew(X) &lt; 0, the distribution has a long left tail. a &lt;- rsnorm(100000, mean = 0, sd = 2, xi = 2) b &lt;- rsnorm(100000, mean = 0, sd = 2, xi = -2) skew &lt;- as.data.frame(cbind(a,b)) colnames(skew) = c(&quot;Right Skewed&quot;, &quot;Left Skewed&quot;) skew_melt &lt;- melt(skew) ## Warning in melt(skew): The melt generic in data.table has been passed a data.frame and will attempt to redirect to the relevant reshape2 method; ## please note that reshape2 is deprecated, and this redirection is now deprecated as well. To continue using melt methods from reshape2 while both ## libraries are attached, e.g. melt.list, you can prepend the namespace like reshape2::melt(skew). In the next version, this warning will become an ## error. ## No id variables; using all as measure variables #Plot skew_melt %&gt;% ggplot(aes(x = value, fill = variable, color = variable)) + geom_density(alpha = 0.2) + theme_bw() + xlim(-15,15) 7.1.8.4 Kurtosis The Kurtosis of X measures the thickness in the tails of a distribution. It is given as: \\[ kurt(X) = \\frac{E[(X-\\mu_x)^4]}{\\sigma_x^4} \\] Kurtosis is the average of the standardized data raised to the fourth power. Any standardized values that are less than 1 (i.e., data within one standard deviation of the mean) contributes very little to the overall Kurtosis. This is due to the fact that raising a value less than 1 to the fourth power shrinks the value itself (e.g. 0.5^4 = 0.0625). However, since kurtosis is based on deviations from the mean raised to the fourth power, . Consequently, large Kurtosis values indicate that extreme values are likely to be present in the data. Consequently, there are three types of Kurtosis we need to be familiar with: Mesokurtic: This is the normal distribution Leptokurtic: This distribution has fatter tails and a sharper peak. The kurtosis is “positive” with a value greater than 3 Platykurtic: The distribution has a lower and wider peak and thinner tails. The kurtosis is “negative” with a value greater than 3 To visualise this, we need to a &lt;- rnorm(100000, mean = 0, sd = 1) b &lt;- rnorm(100000, mean = 0, sd = 1.45) c &lt;- rnorm(100000, mean = 0, sd = 0.55) df_kurt &lt;- as.data.frame(cbind(a,b,c)) colnames(df_kurt) = c(&quot;Mesokurtic&quot;, &quot;Platykurtic&quot;, &quot;Leptokurtic&quot;) df_kurt_melt &lt;- melt(df_kurt) ## Warning in melt(df_kurt): The melt generic in data.table has been passed a data.frame and will attempt to redirect to the relevant reshape2 method; ## please note that reshape2 is deprecated, and this redirection is now deprecated as well. To continue using melt methods from reshape2 while both ## libraries are attached, e.g. melt.list, you can prepend the namespace like reshape2::melt(df_kurt). In the next version, this warning will become ## an error. ## No id variables; using all as measure variables #Plot df_kurt_melt %&gt;% ggplot(aes(x = value, fill = variable, color = variable)) + geom_density(alpha = 0.2) + theme_bw() + xlim(-4,4) ## Warning: Removed 577 rows containing non-finite values (stat_density). 7.2 Matrix Algebra: Introduction In this chapter, we will repeat the basic matrix algebra concepts used throughout the lecture. Matrices are the simplest and most useful way to organise data sets. Using matrix algebra makes manipulation and transformation of multidimensional data sets easier, as it can summarise many steps that would be needed if we worked with each constituent individually. Especially, understanding the functioning of matrix algebra especially helps us in comprehending general concepts of the lecture. For instance, portfolio construction with either two or more assets as well as risk and return calculations can be simplified and generalised using matrix algebra. Further, systems of linear equations to define the first order conditions of mean-variance optimised portfolios can be created using matrix algebra. In general, this form of data manipulation is the most straight-forward when being applied to programming languages such as R, as the syntax of the program largely follows the syntax of textbook linear algebra discussions. Thus, copying theory into programming language is not that hard when using matrix manipulation techniques. Lastly, many R calculations can be efficiently evaluated if they are vectorized - that is, if they operate on vectors of elements instead of looping over individual elements. The chapter will be organised as follows: Section 1 introduces the basic definitions and concepts of matrix algebra, much like you have already seen in Empirical Methods. Section 2 reviews basic operations and manipulation techniques. In Section 3, we look at how to represent summation notation with matrix algebra. Section 4 presents systems of linear equations that constitute the cornerstones of portfolio math. Then, Section 5 introduces the concept of Positive Semi-Definite (PSD) matrices, before we dive into multivariate probability distribution representations using matrix algebra. We conclude the chapter by having a discusssion on portfolio mathematics using matrix algebra as well as how to use derivatives of simple matrix functions. 7.2.1 Matrices and Vectors A vector is a one-dimensional array of numbers. For instance, \\[ \\underset{n \\times 1}{\\textbf{x}} = \\begin{bmatrix} x_{1}\\\\ x_{2} \\\\ \\vdots\\\\ x_{n} \\end{bmatrix} \\] is an \\(n \\times 1\\) vector of entries x. This is also known as \\(\\textbf{Column Vector}\\) If we transpose a vector, it becomes a \\(\\textbf{row vector}\\). For instance: \\[ \\underset{n \\times 1}{\\textbf{x}&#39;} = \\begin{bmatrix} x_{1} &amp; x_2 &amp; \\dots &amp; x_n \\end{bmatrix} \\] A \\(\\textbf{matrix}\\) is a two-dimensional array of numbers. Each matrix consists of rows and columns, whereas both make up the dimension of a matrix. A general form of a matrix is the following: \\[ \\underset{n \\times k}{\\textbf{A}} = \\begin{bmatrix} a_{11} &amp; a_{12} &amp; \\dots &amp; a_{1k}\\\\ a_{21} &amp; a_{22} &amp; \\dots &amp; a_{2k} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ a_{n1} &amp; a_{n2} &amp; \\dots &amp; a_{nk} \\end{bmatrix} \\] where \\(a_{ij}\\) denotes the element in the \\(i^{th}\\) row and \\(j^{th}\\) column of the matrix \\(\\textbf{A}\\). Just as with vectors, we can also transpose the matrix: \\[ \\underset{n \\times k}{\\textbf{A}&#39;} = \\begin{bmatrix} a_{11} &amp; a_{21} &amp; \\dots &amp; a_{n1}\\\\ a_{12} &amp; a_{22} &amp; \\dots &amp; a_{n2} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ a_{1k} &amp; a_{2k} &amp; \\dots &amp; a_{nk} \\end{bmatrix} \\] An important concept in matrices is \\(\\textit{symmetric}\\) and \\(\\textit{square}\\). A \\(\\textit{symmetric}\\) matrix \\(\\textbf{A}\\) is defined such that \\(\\textbf{A} = \\textbf{A&#39;}\\). This can only be the case if the matrix is already \\(\\textit{square}\\), implying that the number of rows equals the number of columns. \\(\\textbf{Example: Creating Vectors and Matrices in R}\\) In R, to construct \\(\\textbf{vectors}\\), the easiest way is to use the combine function c(): xvec = c(1,2,3) xvec ## [1] 1 2 3 Vectors of numbers in R are of class numeric and do not have a dimension attribute: class(xvec) ## [1] &quot;numeric&quot; dim(xvec) ## NULL The elements of a vector can be assigned names using the names() function: names(xvec) = c(&quot;x1&quot;, &quot;x2&quot;, &quot;x3&quot;) xvec ## x1 x2 x3 ## 1 2 3 Lastly, to create a \\(\\textbf{matrix}\\) from a vector, we use the as.matrix() function as.matrix(xvec) ## [,1] ## x1 1 ## x2 2 ## x3 3 In R, matrix objects are created using the matrix() function. For example, we can create a \\(2 \\times 3\\) matrix using: matA = matrix(data=c(1,2,3,4,5,6),nrow=2,ncol=3,byrow=FALSE) matA ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 Looking for the “class” object, we can see if it’s really a matrix: class(matA) ## [1] &quot;matrix&quot; &quot;array&quot; If we want to transpose the matrix, we can use the byrow=TRUE command: matA = matrix(data=c(1,2,3,4,5,6),nrow=2,ncol=3,byrow=TRUE) matA ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 4 5 6 In order to get the dimension of a matrix, we use the dim() command: dim(matA) ## [1] 2 3 This indicates the \\(n \\times k\\) structure, which is \\(2 \\times 3\\). Further, we can define names of the rows and columns using the colnames() and rownames() arguments: rownames(matA) = c(&quot;row1&quot;, &quot;row2&quot;) colnames(matA) = c(&quot;col1&quot;, &quot;col2&quot;, &quot;col3&quot;) matA ## col1 col2 col3 ## row1 1 2 3 ## row2 4 5 6 Lastly, matrix manipulation starts with \\(\\textbf{slicing operations}\\). Thus, the elements of a matrix can be extracted or subsetted as follows: matA[1, 2] ## [1] 2 This defines the elements which should be extracted from the matrix. In our case, we told the program to only take the first row and second column element of the matrix. If we only want to select according to one dimension, we do so accordingly: matA[1,] ## col1 col2 col3 ## 1 2 3 This takes We can take accordingly: matA[,1] ## row1 row2 ## 1 4 Lastly, we can \\(\\textbf{transpose}\\) a matrix by using the t() function: t(matA) ## row1 row2 ## col1 1 4 ## col2 2 5 ## col3 3 6 7.2.2 Basic Matrix Operations 7.2.2.1 Addition and Subtraction Matrices are additive. That means, given the same dimensions of two matrices, you can add and subtract the respective row-column elements from each other. For instance, if we have: \\[ \\textbf{A} = \\begin{bmatrix} 3 &amp; 4\\\\ 8 &amp; 5 \\end{bmatrix}, \\textbf{B} = \\begin{bmatrix} 9 &amp; 1\\\\ 5 &amp; 2 \\end{bmatrix} \\] Then: \\[ \\textbf{A} + \\textbf{B} = \\begin{bmatrix} 3 &amp; 4\\\\ 8 &amp; 5 \\end{bmatrix} + \\begin{bmatrix} 9 &amp; 1\\\\ 5 &amp; 2 \\end{bmatrix} = \\begin{bmatrix} 3+9 &amp; 4+1\\\\ 8+5 &amp; 5+2 \\end{bmatrix} = \\begin{bmatrix} 12 &amp; 5\\\\ 13 &amp; 7 \\end{bmatrix}\\\\ \\textbf{A} - \\textbf{B} = \\begin{bmatrix} 3 &amp; 4\\\\ 8 &amp; 5 \\end{bmatrix} - \\begin{bmatrix} 9 &amp; 1\\\\ 5 &amp; 2 \\end{bmatrix} = \\begin{bmatrix} 3-9 &amp; 4-1\\\\ 8-5 &amp; 5-2 \\end{bmatrix} = \\begin{bmatrix} -6 &amp; 3\\\\ 3 &amp; 3 \\end{bmatrix} \\] In R, this is quite easily done: matA = matrix(c(3,4,8,5),2,2,byrow=TRUE) # Note that we do not indicate nrow = ... &amp; ncol = ... but just write 2 at both places. As long as the # order of the commands is correct, R automatically interprets the second entry as nrow and the third as # ncol. matB = matrix(c(9,1,5,2),2,2,byrow=TRUE) matA + matB ## [,1] [,2] ## [1,] 12 5 ## [2,] 13 7 matA - matB ## [,1] [,2] ## [1,] -6 3 ## [2,] 3 3 7.2.2.2 Scalar and Vector Multiplication Matrices are also \\(\\textbf{multiplicative}\\). That is, they can be multiplied by a scalar or by another matrix that is \\(\\textbf{conformable}\\). For instance, if we take a scalar c = 2 and use the same matrices again, we get: \\[ c * \\textbf{A} = 2* \\begin{bmatrix} 3 &amp; 4\\\\ 8 &amp; 5 \\end{bmatrix} = \\begin{bmatrix} 6 &amp; 8\\\\ 16 &amp; 10 \\end{bmatrix} \\] Matrix multiplication only applies to conformable matrices. \\(\\textbf{A}\\) and \\(\\textbf{B}\\) are said to be conformable if the \\(\\textbf{number of columns in A equals the number of rows in B}\\). If Matrix \\(\\textbf{A}\\) has the dimension of \\(n \\times k\\) and \\(\\textbf{B}\\) the dimension of \\(k \\times p\\), then they are conformable with dimension of \\(n \\times p\\). \\[ \\underset{2\\times 2}{\\textbf{A}} \\cdot\\underset{2\\times 3}{\\textbf{B}} = \\begin{bmatrix} 3 &amp; 4\\\\ 8 &amp; 5 \\end{bmatrix} \\cdot \\begin{bmatrix} 9 &amp; 1 &amp; 7\\\\ 5 &amp; 2 &amp; 3 \\end{bmatrix} = \\begin{bmatrix} 3*9 + 4*5 &amp; 3*1 + 4*2 &amp; 3*7+4*3 \\\\ 8*9+5*5 &amp; 8*1+5*2 &amp; 8*7+5*3 \\end{bmatrix}= \\begin{bmatrix} 47 &amp; 11 &amp; 33\\\\ 97 &amp; 18 &amp; 71 \\end{bmatrix} = \\underset{2\\times 3}{\\textbf{C}} \\] Here, each element of the matrix \\(\\textbf{C}\\) is the dot product of the resulting from the \\(i^{th}\\) row of \\(\\textbf{A}\\) and the \\(j^{th}\\) column of \\(\\textbf{B}\\). \\(\\textbf{Example: Matrix Multiplication in R}\\) To do this in R, we can simply use the %*% operator: matA = matrix(c(3,4,8,5),2,2,byrow=TRUE) matB = matrix(c(9,1,7,5,2,3),2,3,byrow=TRUE) matA %*% matB ## [,1] [,2] [,3] ## [1,] 47 11 33 ## [2,] 97 18 71 dim(matA %*% matB) ## [1] 2 3 As we can see, the dimensions are now \\(2 \\times 3\\). 7.2.2.3 Miscellaneous Matrix Properties Some important properties of matrices that we will use for financial applications are the \\(\\textbf{associative property}\\) as well as \\(\\textbf{transpose product property}\\). That is, if three matrices are conformable (number of columns of the first is number of rows of the latter), then: \\[ \\textbf{A}(\\textbf{B} + \\textbf{C}) = \\textbf{A}\\textbf{B} + \\textbf{A}\\textbf{C} \\] Further, the transpose of the product of two matrices is the product of the transposes in opposite order \\[ (\\textbf{A}\\textbf{B})&#39; = \\textbf{B}&#39;\\textbf{A}&#39; \\] 7.2.2.4 Identity, Diagonal as well as Lower and Upper Triangle Matrices Some pre-defined matrices are quite common in financial applications. The first is called \\(\\textbf{Identity Matrix}\\). An identity matrix is a matrix with all zero elements and only diagonal elements consisting of 1’s. In matrix algebra, pre-multiplying or post-multiplying a matrix by a conformable identity matrix gives back the matrix. Consequently, the matrix must consist of only non-zero diagonal entries. To illustrate, assume that: \\[ \\textbf{I} = \\begin{bmatrix} 1&amp;0\\\\ 0&amp;1 \\end{bmatrix} \\] is the identity matrix and \\[ \\textbf{A} = \\begin{bmatrix} a_{11}&amp;a_{12}\\\\ a_{21} &amp; a_{22} \\end{bmatrix} \\] Then, multiplying \\(\\textbf{A}\\) and \\(\\textbf{I}\\) equals: \\[ \\textbf{A} \\cdot \\textbf{I} = \\begin{bmatrix} a_{11}&amp;a_{12}\\\\ a_{21} &amp; a_{22} \\end{bmatrix} \\cdot \\begin{bmatrix} 1&amp;0\\\\ 0&amp;1 \\end{bmatrix} = \\begin{bmatrix} a_{11}*1 + a_{12} * 0 &amp; 0*a_{11}+ a_{12}*1\\\\ a_{21}*1 + a_{22} * 0 &amp; 0*a_{21}+ a_{22}*1 \\end{bmatrix} = \\begin{bmatrix} a_{11}&amp;a_{12}\\\\ a_{21} &amp; a_{22} \\end{bmatrix} = \\textbf{A} \\] In R, an identity matrix is constructed using the diag() function: matI = diag(2) matI ## [,1] [,2] ## [1,] 1 0 ## [2,] 0 1 Further, we can have a \\(\\textbf{Diagonal matrix}\\), \\(\\textbf{Upper-Triangle matrix}\\) as well as a \\(\\textbf{Lower-Triangle matrix}\\). For that, consider the following matrix: \\[ \\textbf{A} = \\begin{bmatrix} d_1 &amp; u_{12} &amp; u_{13} &amp; \\dots &amp; u_{1n} \\\\ l_{21} &amp; d_2 &amp; u_{23} &amp; \\dots &amp; u_{2n} \\\\ l_{31} &amp; l_{32} &amp; d_3 &amp; \\dots &amp; l_{3n} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ l_{n1} &amp; l_{n2} &amp; l_{n3} &amp; \\dots &amp; d_{n} \\end{bmatrix} \\] Here, d defines the diagonal, u the upper-triangle and l the lower-triangle entries. The diagonal consists of n elements, whereas both the lower- and upper-triangle consist of n(n-1)/2 entries. Then, we have the following matrices: A Diagonal Matrix \\(\\textbf{D}\\) is a \\(n \\times n\\) square matrix with \\(n \\times 1\\) vector of diagonal entries and zero else: \\[ \\textbf{D} = \\begin{bmatrix} d_1 &amp; 0 &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; d_2 &amp; 0 &amp; \\dots &amp;0\\\\ 0 &amp; 0 &amp; d_3 &amp; \\dots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; 0 &amp; \\dots &amp; d_{n} \\end{bmatrix} \\] An upper-triangle matrix \\(\\textbf{U}\\) has all values below the main diagonal equal to zero: \\[ \\textbf{U} = \\begin{bmatrix} d_1 &amp; u_{12} &amp; u_{13} &amp; \\dots &amp; u_{1n} \\\\ 0 &amp; d_2 &amp; u_{23} &amp; \\dots &amp; u_{2n} \\\\ 0 &amp; l_{32} &amp; d_3 &amp; \\dots &amp; l_{3n} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; 0 &amp; \\dots &amp; d_{n} \\end{bmatrix} \\] A lower-triangle matrix \\(\\textbf{L}\\) has all values above the main diagonal equal to zero: \\[ \\textbf{L} = \\begin{bmatrix} d_1 &amp; 0 &amp; 0 &amp; \\dots &amp; 0 \\\\ l_{21} &amp; d_2 &amp; 0 &amp; \\dots &amp; 0 \\\\ l_{31} &amp; l_{32} &amp; d_3 &amp; \\dots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ l_{n1} &amp; l_{n2} &amp; l_{n3} &amp; \\dots &amp; d_{n} \\end{bmatrix} \\] We can apply these matrices easily in R: matA = matrix(c(1,2,3,4,5,6,7,8,9), 3, 3) matA ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 # Extract the lower elements from the matrix: matA[lower.tri(matA)] ## [1] 2 3 6 # Extract the upper elements from the matrix: matA[upper.tri(matA)] ## [1] 4 7 8 7.2.3 Summation Notation in Matrix Form Imagine we have the sum: \\[ \\sum^{n}_{k=1} x_k = x_1 + ... + x_n \\] Then, this sum can be represented by a matrix multiplication of a vector \\(\\textbf{x} = [x_1,..,x_n]\\) and a $n $ vector of ones: \\[ \\textbf{x&#39;}\\textbf{1} = [x_1,..,x_n] \\cdot \\begin{bmatrix} 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix} = x_1 + \\dots + x_n = \\sum^{n}_{k=1} x_k \\] Next, we can have a squared sum: \\[ \\sum^{n}_{k=1} x_k^2 = x_1^2 + ... + x_n^2 \\] Then, this sum can be represented by a matrix multiplication: \\[ \\textbf{x&#39;}\\textbf{x} = [x_1,..,x_n] \\cdot \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_n \\end{bmatrix} = x_1^2 + \\dots + x_n^2 = \\sum^{n}_{k=1} x_k^2 \\] Last, we can have cross-products: \\[ \\sum^{n}_{k=1} x_ky_k = x_1y_1 + ... + x_ny_n \\] Then, this sum can be represented by a matrix multiplication: \\[ \\textbf{x&#39;}\\textbf{y} = [x_1,..,x_n] \\cdot \\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_n \\end{bmatrix} = x_1y_1 + \\dots + x_ny_n = \\sum^{n}_{k=1} x_ky_k \\] In R, this can easily be facilitated: xvec = c(1,2,3) onevec = rep(1,3) t(xvec)%*%onevec ## [,1] ## [1,] 6 yvec = c(4,3,5) crossprod(xvec, yvec) ## [,1] ## [1,] 25 7.2.4 Systems of Linear Equations 7.2.4.1 Inverse of a Matrix Systems of linear equations and matrix algebra are indisputably linked. One of such links comes in the form of the \\(\\textbf{inverse of a matrix}\\) properties. To see how, let’s consider the two linear equations: \\[ x + y = 1\\\\ 2x - y = 1 \\] Considering both functions, we can find their intersection points as \\(x = 2/3\\) and \\(y = 1/3\\). Note that these two linear equations can also be written in terms of matrix notation: \\[ \\begin{bmatrix} 1 &amp; 1\\\\ 2 &amp; -1 \\end{bmatrix} \\begin{bmatrix} x\\\\ y \\end{bmatrix} = \\begin{bmatrix} 1\\\\ 1 \\end{bmatrix} \\] Check yourself that with matrix multiplication you would obtain the same two equations as above. In general, this implies \\(\\textbf{A}\\cdot\\textbf{z} = \\textbf{b}\\), where \\[ \\textbf{A} = \\begin{bmatrix} 1 &amp; 1\\\\ 2 &amp; -1 \\end{bmatrix}, \\textbf{z} = \\begin{bmatrix} x\\\\ y \\end{bmatrix}, \\textbf{b} = \\begin{bmatrix} 1\\\\ 1 \\end{bmatrix} \\] If, in this equation, we had a \\(2 \\times 2\\) matrix \\(\\textbf{B}\\) with elements such that \\(\\textbf{B}\\cdot\\textbf{A} = \\textbf{I_2}\\) (\\(\\textbf{I_2}\\) being the identity matrix), then we can \\(\\textbf{solve for elements in z}\\) as follows: \\[ \\begin{align*} \\textbf{B}\\cdot\\textbf{A}\\cdot\\textbf{z} &amp;= \\textbf{B}\\cdot\\textbf{b} \\\\ \\textbf{I}\\cdot\\textbf{z} &amp;= \\textbf{B}\\cdot\\textbf{b} \\\\ \\textbf{z} &amp;= \\textbf{B}\\cdot\\textbf{b} \\end{align*} \\] or, in matrix notation: $$ = $$ If such a matrix \\(\\textbf{B}\\) exists, it is called \\(\\textbf{inverse of A}\\) and is denoted as \\(\\textbf{A}^{-1}\\). This is the same as when we want to solve the system of linear equation \\(\\textbf{A}\\textbf{x} = \\textbf{b}\\), we just take the \\(\\textbf{A}\\) to the other side and get: \\(\\textbf{x} = \\textbf{A}^{-1}\\textbf{b}\\). As long as we can determine the elements in \\(\\textbf{A}^{-1}\\), then we can solve for the values of x and y in the linear equations system of the vector \\(\\textbf{z}\\). The system of linear equations has a solution as long as the \\(\\textbf{two lines intersect}\\). If the two lines are parallel, then one of the equations is a multiple of the other. In this case, we say that \\(\\textbf{A}\\) is NOT INVERTIBLE. There are general rules in R how to solve for such a system of linear equations. One is in the form of the solve() function: matA = matrix(c(1,1,2,-1), 2, 2, byrow=TRUE) vecB = c(1,1) # First we solve for the inverse of A: A^-1 matA.inv = solve(matA) # Then, we can easily solve for the vector z: z = matA.inv%*%vecB z ## [,1] ## [1,] 0.6666667 ## [2,] 0.3333333 7.2.4.2 Linear Independence and Rank of a Matrix Consider again the \\(n \\times k\\): \\[ \\underset{n \\times k}{\\textbf{A}} = \\begin{bmatrix} a_{11} &amp; a_{12} &amp; \\dots &amp; a_{1k}\\\\ a_{21} &amp; a_{22} &amp; \\dots &amp; a_{2k} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ a_{n1} &amp; a_{n2} &amp; \\dots &amp; a_{nk} \\end{bmatrix} = [\\textbf{a}_1, \\textbf{a}_2, \\dots, \\textbf{a}_k] \\] Where each vector \\(\\textbf{a}\\) is a \\(n \\times 1\\) column vector. In that case, k vectors \\(\\textbf{a}_1, \\textbf{a}_2, \\dots \\textbf{a}_k\\) are \\(\\textbf{linearily independent}\\) if \\(\\textbf{a}_1c_1 + \\textbf{a}_2c_2 + \\dots + \\textbf{a}_kc_k = 0\\). That is, no vector can be expressed as a \\(\\textbf{non-trivial linear combination}\\) of the other vectors. For us of importance is the \\(\\textbf{Rank of a Matrix}\\). The column rank of matrix, denoted \\(\\textbf{rank()}\\), is equal to the \\(\\textbf{maximum number of linearly independent columns}\\). If rank(A) = m, then we say the matrix has \\(\\textbf{full rank}\\). In R, we figure the rank of a matrix accordingly with the rankMatrix() function: library(Matrix) Amat = matrix(c(1,3,5,2,4,6), 2, 3, byrow=TRUE) as.numeric(rankMatrix(Amat)) ## [1] 2 7.2.5 Positive Definite (PD) Matrix Another important concept in matrix algebra is positive definiteness. We consider a matrix to be if for any \\(n \\times 1\\) x \\(\\neq\\) 0: \\[ \\begin{equation} \\textbf{x}&#39;\\textbf{A}\\textbf{x} &gt; 0 \\end{equation} \\] Therein, we consider a matrix \\(\\textbf{A}\\) to be \\(\\textbf{positive semi-definite}\\) if for any \\(n \\times 1\\) x \\(\\neq\\) 0: \\[ \\begin{equation} \\textbf{x}&#39;\\textbf{A}\\textbf{x} \\geq 0 \\end{equation} \\] Hence, if a matrix is positive semi-definite then there exists some vector x such that \\(\\textbf{A}\\textbf{x} = 0\\), which implies that . 7.2.6 Multivariate Probability Distributions 7.2.6.1 Covariance and Correlation Matrix Covariance and Correlation Matrices are fundamental concepts in financial applications. The covariance matrix is denoted as a sum sign, \\(\\scriptstyle\\sum\\). It summarizes the variances and covariances of the elements of the random vector . Generally, the covariance matrix of a random vector \\(\\textbf{X}\\) with mean vector \\(\\mu\\) is defined as: \\[ \\begin{equation} cov(\\textbf{X}) = E[(\\textbf{X}- \\mu)(\\textbf{X}- \\mu)&#39;] = \\scriptstyle\\sum \\end{equation} \\] If \\(\\textbf{X}\\) has n elements, then \\(\\scriptstyle\\sum\\) will be the symmetric and positive semi-definite n \\(\\times\\) n matrix: \\[ \\scriptstyle\\sum_{n \\times n} = \\begin{bmatrix} \\sigma_1^2 &amp; \\sigma_{12} &amp; \\dots &amp; \\sigma_{1n}\\\\ \\sigma_{12} &amp; \\sigma_2^2 &amp; \\dots &amp; \\sigma_{2n}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\sigma_{1n} &amp; \\sigma_{2n} &amp; \\dots &amp; \\sigma_n^2 \\end{bmatrix} \\] To understand the mathematics behind a variance-covariance matrix, let’s quickly look at the composition of a 2 \\(\\times\\) 2 case: \\[ \\begin{align*} E[(\\textbf{X}- \\mu)(\\textbf{X}- \\mu)&#39;] &amp;= E\\left[(\\begin{matrix} X_1 - \\mu_1 \\\\ X_2 - \\mu_2\\end{matrix})(X_1 - \\mu_1, X_2 - \\mu_2)\\right]\\\\ &amp;= E\\left[(\\begin{matrix}(X_1 - \\mu_1)(X_1 - \\mu_1) &amp; (X_1 - \\mu_1)(X_2 - \\mu_2) \\\\ (X_1 - \\mu_1)(X_2 - \\mu_2) &amp; (X_2 - \\mu_2)(X_2 - \\mu_2)\\end{matrix})\\right] \\\\ &amp;= \\left(\\begin{matrix}E[(X_1 - \\mu_1)^2] &amp; E[(X_1 - \\mu_1)(X_2 - \\mu_2)] \\\\ E[(X_1 - \\mu_1)(X_2 - \\mu_2)] &amp; E[(X_2 - \\mu_2)^2\\end{matrix}\\right) \\\\ &amp;= \\left(\\begin{matrix} var(X_1) &amp; cov(X_1,X_2) \\\\ cov(X_2, X_1) &amp; var(X_2) \\end{matrix}\\right) \\\\ &amp;= \\left(\\begin{matrix} \\sigma_1^2 &amp; \\sigma_{12} \\\\ \\sigma_{12} &amp; \\sigma_2^2 \\end{matrix}\\right) \\end{align*} \\] In contrast to the covariance matrix, the correlation matrix \\(\\textbf{C}\\) summarizes all pairwise correlations between the elements of the n \\(\\times\\) 1 random vector \\(\\textbf{X}\\) is given by: \\[ \\begin{equation} \\textbf{C} = \\left( \\begin{matrix} 1 &amp; \\rho_{12} &amp; \\dots &amp; \\rho_{1n} \\\\ \\rho_{12} &amp; 1 &amp; \\dots &amp; \\rho{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\rho_{1n} &amp; \\rho_{2n} &amp; \\dots &amp; 1 \\end{matrix} \\right) \\end{equation} \\] In general, the correlation matrix can be computed from the covariance matrix. For that, we understand that: \\[ \\begin{equation} \\textbf{C} = \\textbf{D}^{-1}\\scriptstyle\\sum\\textstyle\\textbf{D}^{-1} \\end{equation} \\] Whereas \\(\\textbf{D}\\) is an n \\(\\times\\) n diagonal matrix with the standard deviations of the elements of \\(\\textbf{X}\\) along the main diagonal: \\[ \\textbf{D} = \\left( \\begin{matrix} \\sigma_1 &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; \\sigma_2 &amp; \\dots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; \\sigma_n \\end{matrix} \\right) \\] In R, a covariance matrix is calculated using usual matrix formulation: sigma1 = 2.3 sigma2 = 1.8 rho12 = 0.35 sigma12 = rho12*sigma1*sigma2 cov_m = matrix(c(sigma1^2, sigma12, sigma12, sigma2^2), 2, 2, byrow=TRUE) cov_m ## [,1] [,2] ## [1,] 5.290 1.449 ## [2,] 1.449 3.240 Then, we can transform this into a correlation matrix using the cov2cor() function: cov2cor(cov_m) ## [,1] [,2] ## [1,] 1.00 0.35 ## [2,] 0.35 1.00 Note that this makes absolutely sense, as the rho12 = 0.35, which is nothing else than the correlation between sigma1 and sigma2 as we defined it above. 7.2.6.2 Variance of a linear combination of random vectors Another important property are transformations with linear combinations. That is, we add an n \\(\\times\\) 1 vector called \\(\\textbf{a}\\) = \\((a_1, \\dots, a_n)\\) to the random vector \\(\\textbf{X}\\). If we assume that a random variables \\(\\textbf{Y}\\) exists which is a of the form \\(Y = \\textbf{a}&#39;\\textbf{X} = a_1X_1 + \\dots + a_nX_n\\), then the expected values is: \\[ \\mu_y = E[Y] = E[\\textbf{a}&#39;\\textbf{X}] = \\textbf{a}&#39;E[\\textbf{X}] = \\textbf{a}&#39;\\mu \\] and the corresponding variance is: \\[ var(Y) = var(\\textbf{a}&#39;\\textbf{X}) = E[(\\textbf{a}&#39;\\textbf{X} - \\textbf{a}&#39;\\mu)^2] = E[(\\textbf{a}&#39;(\\textbf{X} - \\mu))^2] \\] We can now use a simple definition from matrix algebra. If z is a scalar, then we know that \\(z&#39;z = zz&#39; = z^2\\). We know that \\(\\textbf{a}&#39;(\\textbf{X} - \\mu)\\) is a scalar, as such we can compute the variance as: \\[ \\begin{align*} var(Y) &amp;= E[z^2] = E[z \\cdot z&#39;] \\\\ &amp;= E[\\textbf{a}&#39;(\\textbf{X} - \\mu)\\textbf{a}(\\textbf{X} - \\mu)&#39;] \\\\ &amp;= \\textbf{a}&#39;E[(\\textbf{X} - \\mu)(\\textbf{X} - \\mu)&#39;]\\textbf{a} \\\\ &amp;= \\textbf{a}&#39;cov(\\textbf{X})\\textbf{a} \\\\ &amp;= \\textbf{a}&#39;\\scriptstyle\\sum\\textstyle\\textbf{a} \\end{align*} \\] Consequently, we know that the variance of a linear combination of a random variable and a constant is just the inverse of the constant multiplied with the covariance of the random variable multiplied with the constant. 7.2.6.3 Covariance between linear combination of two random vectors If we consider two different constants, \\(\\textbf{a}\\) = \\((a_1, \\dots, a_n)\\) as well as \\(\\textbf{b}\\) = \\((b_1, \\dots, b_n)\\) to the random vector \\(\\textbf{X}\\), and \\(Y = \\textbf{a}&#39;\\textbf{X} = a_1X_1 + \\dots + a_nX_n\\) as well as \\(Z = \\textbf{b}&#39;\\textbf{X} = b_1X_1 + \\dots + b_nX_n\\), we can write the covariance in matrix notation as: \\[ \\begin{align*} cov(Y,Z) &amp;= E[(Y - E[Y])(Z-E[Z])] \\\\ cov(\\textbf{a}&#39;\\textbf{X}, \\textbf{b}&#39;\\textbf{X}) &amp;= E[(\\textbf{a}&#39;\\textbf{X} - E[\\textbf{a}&#39;\\textbf{X}])(\\textbf{b}&#39;\\textbf{X}-E[\\textbf{b}&#39;\\textbf{X}])] \\\\ &amp;= E[(\\textbf{a}&#39;\\textbf{X} - \\textbf{a}&#39;\\mu])(\\textbf{b}&#39;\\textbf{X}-\\textbf{b}&#39;\\mu])] \\\\ &amp;= E[\\textbf{a}&#39;(\\textbf{X} - \\mu)\\textbf{b}&#39;(\\textbf{X} - \\mu)] \\\\ &amp;= \\textbf{a}&#39;E[(\\textbf{X} - \\mu)(\\textbf{X} - \\mu)&#39;]\\textbf{b}&#39;\\\\ &amp;= \\textbf{a}&#39;\\scriptstyle\\sum\\textstyle\\textbf{b}&#39; \\end{align*} \\] 7.2.6.4 Multivariate Normal Distribution Now, let’s start to combine the usual assumptions of distribution functions we use in daily statistic life with the properties of the matrix algebra notations we derived in this chapter. For that, we define: n random variables \\(\\textbf{X_1}, \\dots, \\textbf{X_n}\\) that are \\(\\textbf{jointly normally distributed}\\). Then, we have n \\(\\times\\) 1 vectors \\(\\textbf{X}\\) = \\((X_1,\\dots,X_n)&#39;\\), \\(\\textbf{x}\\) = \\((x_1,\\dots,x_n)&#39;\\) as well as \\(\\mu = (\\mu_1,\\dots,\\mu_n)&#39;\\) and \\[ \\scriptstyle\\sum_{n \\times n} = \\begin{bmatrix} \\sigma_1^2 &amp; \\sigma_{12} &amp; \\dots &amp; \\sigma_{1n}\\\\ \\sigma_{12} &amp; \\sigma_2^2 &amp; \\dots &amp; \\sigma_{2n}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\sigma_{1n} &amp; \\sigma_{2n} &amp; \\dots &amp; \\sigma_n^2 \\end{bmatrix} \\] Under these properties, we understand that \\(\\textbf{X} \\sim N(\\textstyle\\mu, \\scriptstyle\\sum)\\) means that the random vector \\(\\textbf{X}\\) has a \\(\\mu\\) and covariance matrix \\(\\scriptstyle \\sum\\). We can show that under the assumptions of iid (independently and identically distributed) standard normal random variables, we can create a random vector with the properties of \\(\\textbf{X} \\sim N(\\textstyle\\mu, \\scriptstyle\\sum)\\). For that, let’s assume that \\(\\textbf{Z} = (Z_1, \\dots, Z_n)&#39;\\). In this case, \\(\\textbf{Z} \\sim N(0, I_n)\\) where \\(I_n\\) denotes the identity matrix. Given we can create with the desired properties, if we define \\[ \\textbf{X} = \\mu + \\scriptstyle\\sum^{1/2}\\textstyle\\textbf{Z} \\] where \\(\\scriptstyle\\sum^{1/2}\\) is the upper-triangle matrix defined previously where \\(\\scriptstyle\\sum = \\sum^{1/2}&#39;\\sum^{1/2}\\) . In that case: \\[ E[\\textbf{X}] = E[\\mu + \\scriptstyle\\sum^{1/2}\\textstyle\\textbf{Z}] = \\mu + \\scriptstyle\\sum^{1/2}\\textstyle E[\\textbf{Z}] \\mu + \\scriptstyle\\sum^{1/2}\\textstyle E[\\textbf{0}] = \\mu \\] and: \\[ \\begin{align*} var(\\textbf{X}) &amp;= E[\\textbf{X} - E[\\textbf{X}]]\\\\ &amp;= E[\\mu + \\scriptstyle\\sum^{1/2}\\textstyle\\textbf{Z} - \\mu] \\\\ &amp;= E[\\scriptstyle\\sum^{1/2}\\textstyle\\textbf{Z}] \\\\ &amp;= var(\\scriptstyle\\sum^{1/2}\\textstyle\\textbf{Z}) \\\\ &amp;= \\scriptstyle\\sum^{1/2}&#39;var(\\textstyle\\textbf{Z})\\scriptstyle\\sum^{1/2}\\\\ &amp;= \\scriptstyle\\sum^{1/2}&#39;I_n\\scriptstyle\\sum^{1/2} \\\\ &amp;= \\scriptstyle\\sum \\end{align*} \\] Thus, \\(\\textbf{X} \\sim N(\\mu, \\scriptstyle\\sum)\\) \\(\\textbf{Example: Simulation of a multivariate normal random vectors in R}\\) We can easily simulate the that \\(\\textbf{X} \\sim N(\\mu, \\scriptstyle\\sum)\\) where \\(\\mu = (1,1)&#39;\\) and \\[ \\scriptstyle\\sum = \\textstyle \\begin{bmatrix} 1 &amp; 1 \\\\ 1 &amp; 3 \\end{bmatrix} \\] mu = c(1,1) # Specify the mu vector sigma1 = 1 sigma2 = 2 # Specify both standard deviations rho12 = 0.35 # Specify the correlation coefficient sigma12 = sigma1*sigma2*rho12 # Define the covariance Sigma = matrix(c(sigma1^2, sigma12, sigma12, sigma2^2), 2, 2, byrow = TRUE) # Create the covariance matrix Sigma_0.5 = chol(Sigma) # the Cholesky factorization of the covariance matrix to only get the upper-triangular form n = 2 set.seed(123) Z = rnorm(n) # Compute the random variable Z ~ N(mu, I_2) X = mu * Sigma_0.5%*%Z # Compute X from the of 2 iid standard normal random variables Now, the vector stems from a vector of iid standard normal random variables and has the properties of (is proportional to) \\(\\mu\\) and \\(\\scriptstyle\\sum\\) as first two moments (mean and variance). 7.2.7 Portfolio Construction and Mathematical Properties using Matrix Algebra As we now covered the main baseline principles of matrix algebra, we can utilise this knowledge and combine it with the fundamentals of portfolio theory. For that, we assume that we have a portfolio consisting of . These all are with means, variances and covariances: \\[ E[R_i] = \\mu_i \\\\ var(R_i) = \\sigma_i^2\\\\ cov(R_i, R_j) = \\sigma_{ij} \\] Furthermore, we consider portfolio weights \\(x_i\\) within our setting, whereas \\(\\sum x_i = 1\\). That is, the entire portfolio consists of only these three assets. . As such, the is given as: \\[ R_{p,x} = x_1\\mu_1 + x_2\\mu_2 + x_3\\mu_3 \\] And the portfolio variance is given as: \\[ \\begin{align*} \\sigma_{p,x}^2 &amp;= \\sigma_1^2x_1^2 + \\sigma_2^2x_2^2 + \\sigma_3^2x_3^2 + 2\\sigma_{1,2}x_1x_2 + 2\\sigma_{2,3}x_2x_3 + 2\\sigma_{13x_1x_3} \\\\ &amp;= \\sigma_1^2x_1^2 + \\sigma_2^2x_2^2 + \\sigma_3^2x_3^2 + 2\\sigma_1\\sigma_2\\rho_{1,2}x_1x_2 + 2\\sigma_2\\sigma_3\\rho_{2,3}x_2x_3 + 2\\sigma_1\\sigma_3\\rho_{13}x_1x_3 \\end{align*} \\] Please remember that \\(\\sigma_{i,j} = \\sigma_i\\sigma_j\\rho_{i,j}\\). We can now substantially simplify this expression by using matrix algebra: \\[ \\textbf{R} = \\begin{bmatrix} R_1 \\\\ R_2 \\\\ R_3 \\end{bmatrix}, \\textbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} \\] As all of the variables are perfectly characterised, by assumptions, through their mean, variance and covariance structures, we can easily characterise them by using: \\[ E[\\textbf{R}] = E\\left(\\begin{bmatrix} R_1 \\\\ R_2 \\\\ R_3 \\end{bmatrix}\\right) = \\begin{bmatrix} \\mu_1 \\\\ \\mu_2 \\\\ \\mu_3 \\end{bmatrix} = \\mu \\] \\[ var(R) = \\begin{bmatrix} \\sigma_1^2 &amp; \\sigma_{12} &amp; \\sigma_{13}\\\\ \\sigma_{21} &amp; \\sigma_2^2 &amp; \\sigma_{23}\\\\ \\sigma_{31} &amp; \\sigma_{32} &amp; \\sigma_3^2 \\end{bmatrix} = \\scriptstyle\\sum \\] Here, the covariance is per definition. That is, upper-left triangle values equal lower-left triangle values and the matrix transposed is identical to the original matrix. We can easily see how to write the portfolio expected returns and variances in matrix notation. The expected return of the portfolio is given by: \\[ E[R_{pf}] = \\begin{bmatrix} x_1 &amp; x_2 &amp; x_3 \\end{bmatrix} \\begin{bmatrix} \\mu_1 \\\\ \\mu_2 \\\\ \\mu_3 \\end{bmatrix} = x_1\\mu_1 + x_2\\mu_2 + x_3\\mu_3 \\] And the portfolio variance is given by: \\[ \\begin{align*} var(R_{p,x}) &amp;= var(\\textbf{x}&#39;\\textbf{R}) \\\\ &amp;=\\textbf{x}&#39; \\textbf{R} \\textbf{x}\\\\ &amp;= \\begin{bmatrix} x_1, &amp; x_2, &amp; x_3 \\end{bmatrix} \\begin{bmatrix} \\sigma_1^2 &amp; \\sigma_{12} &amp; \\sigma_{13}\\\\ \\sigma_{21} &amp; \\sigma_2^2 &amp; \\sigma_{23}\\\\ \\sigma_{31} &amp; \\sigma_{32} &amp; \\sigma_3^2 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} \\\\ &amp;= \\begin{bmatrix} x_1\\sigma_1^2 + x_2\\sigma_{21} + x_3\\sigma_{31}, &amp; x_1\\sigma_{12} + x_2\\sigma_2^2 + x_3\\sigma_{32}, &amp; x_1\\sigma_{13} + x_2\\sigma_{23} + x_3\\sigma_3^2 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} \\\\ &amp;= \\sigma_1^2x_1^2 + \\sigma_2^2x_2^2 + \\sigma_3^2x_3^2 + 2\\sigma_{1,2}x_1x_2 + 2\\sigma_{2,3}x_2x_3 + 2\\sigma_{13}x_1x_3 \\end{align*} \\] This is a very easy expression. Accordingly, we can also compute the covariance between the return on portfolio \\(\\textbf{x}\\) and \\(\\textbf{y}\\) using matrix algebra: \\[ \\begin{align*} \\sigma_{xy} = cov(R_{p,x},R_{p,y}) &amp;= cov(\\textbf{x}&#39;\\textbf{R}, \\textbf{y}&#39;\\textbf{R}) \\\\ &amp;= E[(\\textbf{x}&#39;\\textbf{R} - E[\\textbf{x}&#39;\\textbf{R}]), (\\textbf{y}&#39;\\textbf{R} - E[\\textbf{y}&#39;\\textbf{R}])] \\\\ &amp;= E[(\\textbf{x}&#39;\\textbf{R} - \\textbf{x}&#39;\\mu_x)(\\textbf{y}&#39;\\textbf{R} - \\textbf{y}&#39;\\mu_y)] \\\\ &amp;= E[\\textbf{x}&#39;(\\textbf{R} - \\mu_x)(\\textbf{R} - \\mu_y)&#39;\\textbf{y}] \\\\ &amp;= \\textbf{x}&#39;E[(\\textbf{R} - \\mu_x)(\\textbf{R} - \\mu_y)]\\textbf{y}\\\\ &amp;= \\textbf{x}&#39;\\scriptstyle\\sum_{xy}\\textstyle\\textbf{y} \\end{align*} \\] "],["inductive-statistics-and-regression-analysis-fundamentals.html", "Chapter 8 Inductive Statistics and Regression Analysis Fundamentals 8.1 Inductive Statistics 8.2 Introduction to Regression Analysis", " Chapter 8 Inductive Statistics and Regression Analysis Fundamentals The third chapter covers Inductive Statistics and Regression modeling. Therein, we cover inductive properties of statistical distributions, such as confidence intervals and hypothesis tests, and dig into univariate Linear Regression based on correlation &amp; covariance properties we learned throughout the first weeks. Further, we introduce the connection of equity and \\(\\beta\\) in form of the equity risk premium, define certain options to calculate a \\(\\beta\\) factor, and use the concept of \\(\\beta\\) within a regression setting. 8.1 Inductive Statistics We now covered the theoretical founding stones of statistical analysis. Now, it is time to dig deeper into one of the major applications of these properties in inferential analysis. To do so, we will introduce the concept of inductive statistics. Inductive statistics is the branch of statistics dealing with conclusions, generalizations, predictions, and estimations based on data from samples. In essence, inductive statistics takes information from a representative sample and attempts to generalise it to a general population. Representative here means that we can assume that the population follows the same distributional properties as the sample, thereby implying that all properties that show to hold in a sample can be generalised to the true population. Consequently, we use the term inductive because we “induce” something from a given sample. Inductive statistics are very applied since it is generally infeasible or simply too involved to analyze the entire population in order to obtain full certainty as to the true environment. For instance, we can never rely on the entire time-series of observations to define a return. Consequently, to obtain insight about the true but unknown parameter value, we draw a sample from which we compute statistics or estimates for the parameter. In this chapter, we will cover three main parts: Point Estimators, Confidence Intervals and Hypotheses Testing. When considering Point Estimators, we learn about samples, statistics, and estimators. Most of the topics are built on the statistical properties used earlier.In particular, we present the linear estimator, explain quality criteria (such as the bias, mean-square error, and standard error) and the large-sample criteria. Related to the large-sample criteria, we present the fundamental theorem of consistency, for which we need the definition of convergence in probability and the law of large numbers. As another large-sample criterion, we introduce the unbiased efficiency, explaining the best linear unbiased estimator (BLUE) or, alternatively, the minimum variance linear unbiased estimator. We then discuss the maximum likelihood estimation technique, one of the most powerful tools in the context of parameter estimation. When using Confidence Intervals, we present the confidence interval. We then present the probability of error in the context of confidence intervals, which is related to the confidence level. We then conclude the chapter by performing hypotheses testing. To test for these, we develop a test statistic for which we set up a decision rule. For a specific sample, this test statistic then either assumes a value in the acceptance region or the rejection region, regions that we describe in this chapter. Furthermore, we see the two error types one can incur when testing. We see that the hypothesis test structure allows one to control the probability of error through what we see to be the test size or significance level. We discover that each observation has a certain p-value expressing its significance. As a quality criterion of a test, we introduce the power from which the uniformly most powerful test can be defined. 8.1.1 Point Estimators We use the information obtained from the sample, or better, the statistic, to infer about a point estimator of a certain parameter \\(\\theta\\). Formally, if we do this, we refer to the estimation function as an estimator and denote it by: \\[ \\hat{\\theta} : X \\rightarrow \\Theta \\] This means we take the sample space X and map it into the set space \\(\\Theta\\) (if you don’t know any more the distinction between these values, go to the book in Chapter 3.1). But, in general, this just means that we create an estimator of a parameter, that we usually denote with a “hat” sign, \\(\\hat{}\\), from a sample we observe and that this estimator is valid for an unobservable population because we assume that we can “map” or “generalise” this sample to this population (set space) (b/c we assume it has the same attributes). The exact structure of the estimator is predetermined before the sample is realized. After the estimator has been defined, we simply need to enter the sample values accordingly. Due to the estimator’s dependence on the random sample, the estimator is itself random. A particular value of the estimator based on the realization of some sample is called an estimate. We will show you in simulation studies that, if we repeat the same draw multiple times, we will always receive slightly different moments of a probability distribution (but, if repeated sufficient times, the variance between these draws will diminish). For instance, if we realize 1000 samples of given length n, we obtain 1000 individual estimates. Sorting them by value—and possibly arranging them into classes—we can compute the distribution function of these realizations, which is similar to the empirical cumulative distribution function 8.1.1.1 Estimators for the mean As an illustration, let’s create normally distributed returns with parameters \\(\\mu\\) and \\(\\sigma^2\\) such that \\(Y = N(\\mu, \\sigma^2)\\). Let’s define that we have 10, 100, 1’000 and 10’000 individual samples for IID draws of X. Then, we compute the mean as: \\[ \\hat{x_i} = \\frac{1}{n}\\sum_{i=1}^nX_i \\] # Set a random seed set.seed(124) # Draw 10, 100, 1000 and 10000 distributions with random numbers x_10 &lt;- as.data.frame(rnorm(10)) colnames(x_10) &lt;- &quot;x&quot; x_100 &lt;- as.data.frame(rnorm(100)) colnames(x_100) &lt;- &quot;x&quot; x_1000 &lt;- as.data.frame(rnorm(1000)) colnames(x_1000) &lt;- &quot;x&quot; x_10000 &lt;- as.data.frame(rnorm(10000)) colnames(x_10000) &lt;- &quot;x&quot; # Create plots p10 &lt;- x_10 %&gt;% ggplot(aes(x=x)) + geom_histogram(bins = 100) + ggtitle(&quot;n = 10&quot;) p100 &lt;- x_100 %&gt;% ggplot(aes(x=x)) + geom_histogram(bins = 100) + ggtitle(&quot; n = 100&quot;) p1000 &lt;- x_1000 %&gt;% ggplot(aes(x=x)) + geom_histogram(bins = 100) + ggtitle(&quot;n = 1000&quot;) p10000 &lt;- x_10000 %&gt;% ggplot(aes(x=x)) + geom_histogram(bins =100) + ggtitle(&quot;n = 10000&quot;) # Merge plots Rmisc::multiplot( p10 + theme_fivethirtyeight(), p1000 + theme_fivethirtyeight(), p100 + theme_fivethirtyeight(), p10000 + theme_fivethirtyeight(), cols = 2) We see that the distribution of the sample means copy quite well the appearance of the theoretical sample distribution density function if we increase n. This is the first intuition behind what we call sampling statistics. As you’ve seen in earlier courses, this is one of the fundamental ideas behind inductive statistics. 8.1.1.2 Linear Estimators Let’s start introducing linearity into the concept of inductive statistics. To do so, we introduce the linear estimator. Suppose we have a sample of size n such that \\(X = (X_1, X_2, \\dots, X_n)\\). The linear estimator then has the following form: \\[ \\hat{\\theta} = \\sum^n_{i=1}a_iX_i \\] Where each draw of \\(X_i\\) is weighted by some real number, \\(a_i\\). We know that the linear estimator is normally distributed. This understanding is based on two important properties introduced in basic statistics, if we assume independent and identically distributed draws (IID). Property 1 - location-scale invariance property: If we multiply X by b and add a where a and b are real numbers, the resulting \\(a + b\\cdotX\\) is again normally distributed with other units of measurement: \\(N(a+\\mu, b\\sigma)\\) Property 2 - stability under summation: The sum of an arbitrary number n of normal random variables (\\(X_1,\\dots,X_n\\)) is again normally distributed Thus, any linear estimator will be normal. This is an extremely attractive feature of the linear estimator, as it allows us to draw inference based on Gaussian distribution properties. This is also the reason why we normally assume linearity in empirical, econometric models. As such, even if the underlying distribution is not the normal distribution, according to the Central Limit Theorem, the sample mean will be approximately normally distributed as the sample size increases within linear settings. This is what we have seen before when we have drawn multiple sample means and plotted them as a histogram. This result facilitates parameter estimation for most distributions. So, even though the exact value of the point estimator, with \\((\\mu, \\sigma^2)\\), is unknown, we observe the distribution of the sample means and try to find the location of the center. For instance, if we have a Bernoulli Distribution with \\(\\mu = 0\\), we can use the formula for the sample mean and draw a large number of individual samples, calculate the mean from each sample and plot its distribution as a histogram. Accordingly, we then take the distributional properties and understand that it will follow an approximately normal distribution with \\((\\mu, \\sigma^2)\\) and thus can infer that the “true” sample mean is at the location center of the distribution of the sample means. This is also known as Law of Large Numbers and will be introduced shortly. p10000_normal_dist &lt;- x_10000 %&gt;% ggplot(aes(x=x)) + geom_histogram(aes(y = ..density..),bins =100) + stat_function(fun = dnorm, colour = &quot;red&quot;, size = 1, linetype = &quot;dashed&quot;, args = list(mean = mean(x_10000$x), sd = sd(x_10000$x))) p10000_normal_dist 8.1.1.3 Quality Criteria of Estimators The question related to each estimation problem should be what estimator would be best suited for the problem at hand. Estimators suitable for the very same parameters can vary quite substantially when it comes to quality of their estimation. Here we will explain some of the most commonly employed quality criteria. Bias An important consideration in the selection of an estimator is the average behavior of that estimator over all possible scenarios. Depending on the sample outcome, the estimator may not equal the parameter value and, instead, be quite remote from it. This is a natural consequence of the variability of the underlying sample. However, the average value of the estimator is something we can control. For that, we first consider the sampling error. This is the difference between the estimate and the population parameter. The expected value of the sampling error is defined as Bias and is given as: \\[ E(\\hat{\\theta} - \\theta) \\] If the expression is equal to zero, then we say this is an unbiased estimator. Let’s illustrate the concept of bias in the case of our sample mean and sample variance. Sample Mean Whenever a population mean has to be estimated, a natural estimator of choice is the sample mean. Let us examine its bias. This is given by: \\[ \\begin{align*} E(\\bar{X} - \\mu) &amp;= E(\\frac{1}{n}\\sum_{i=1}^nX_i - \\mu) \\\\ &amp;= \\frac{1}{n}\\sum_{i=1}^nE(X_i) - \\mu &amp;&amp; \\text{the expected value of } \\mu \\text{ is } \\mu \\\\ &amp;= \\frac{1}{n}\\sum_{i=1}^n\\mu - \\mu &amp;&amp; \\text{the expected value is } \\mu \\\\ &amp;= \\frac{1}{n}n\\mu - \\mu \\\\ &amp;= 0 \\end{align*} \\] So the sample mean is unbiased. Sample Variance The sample variance is given as: \\[ s^2 = \\frac{1}{n}\\sum^n_{i=1}(x_i - \\bar{x})^2 \\] Then, we compute, but not show, the bias of the sample variance, as: \\[ \\begin{align*} E(s^2-\\sigma^2) &amp;= \\sigma^2 - \\frac{n-1}{n}\\sigma^2 \\\\ &amp;= \\frac{1}{n}\\sigma^2 \\end{align*} \\] That is, the bias of the sample variance is negligible if n is sufficiently large. Mean Squared Error As just explained, bias as a quality criterion tells us about the expected deviation of the estimator from the parameter. However, the bias fails to inform us about the variability or spread of the estimator. For a reliable inference for the parameter value, we should prefer an estimator with rather small variability or, in other words, high precision. The Mean Squared Error incorporates both properties. It includes both a term to account for the bias, the expected deviation of the estimator, as well as the precision, the variability (variance) of the estimator. The sampling distribution provides us with both a theoretical measure of the mean as well as the spread of the estimator, that is its variance. The suqare root of the variance is also called standard error and is given as: \\[ \\sqrt{Var(\\hat{\\theta_n})} \\] This value constitutes the spread, or variability, of the sample distribution. We use the mean squared error because, although we stated the bias as an ultimately preferable quality criterion, a bias of zero may be too restrictive a criterion if an estimator is only slightly biased but has a favorably small variance compared to all possible alternatives, biased or unbiased. So, we need some quality criterion accounting for both bias and variance. Taking squares rather than the loss itself incurred by the deviation, the MSE is defined as the expected square loss: \\[ MSE(\\hat{\\theta}) = E[(\\theta - \\hat{\\theta})^2] \\] If we reformulate this expression, we retrieve a very famous expression used in Machine Learning and Econometrics. This term is also known as the Bias-Variance Trade-Off and is derived as follows: \\[ \\begin{align} E[(\\theta - \\hat{\\theta})^2] &amp;= E[(\\theta + \\epsilon - \\hat{\\theta})^2] \\\\ &amp;= E[(\\theta + \\epsilon - \\hat{\\theta} + E[\\hat{\\theta}] - E[\\hat{\\theta}])^2] \\\\ &amp;= E[(\\theta - \\hat{\\theta})^2] + E[\\epsilon^2] + E[(E[\\hat{\\theta}]- \\hat{\\theta})^2] + 2E[(\\theta - E[\\hat{\\theta}])\\epsilon] 2E[(E[\\hat{\\theta}]- \\hat{\\theta})\\epsilon] + 2E[(E[\\hat{\\theta}]- \\hat{\\theta})(\\hat{\\theta} - E[\\hat{\\theta}])] \\\\ &amp;= (\\theta - \\hat{\\theta})^2 + E[\\epsilon^2] + E[(E[\\hat{\\theta}]- \\hat{\\theta})^2] + \\underbrace{2(\\theta - E[\\hat{\\theta}])E[\\epsilon] + 2E[(E[\\hat{\\theta}]- \\hat{\\theta})]E[\\epsilon] + 2E[(E[\\hat{\\theta}]- \\hat{\\theta})](\\hat{\\theta} - E[\\hat{\\theta}])}_{\\text{if written out, this will all cancel each other out, thereby = 0}} \\\\ &amp;= \\underbrace{(\\theta - \\hat{\\theta})^2}_{\\text{Bias term}} + E[\\epsilon^2] + \\underbrace{E[(E[\\hat{\\theta}]- \\hat{\\theta})^2]}_{\\text{Variance term}} \\\\ &amp;= Bias[\\hat{\\theta}]^2 + Var[\\hat{\\theta}] + Var[\\epsilon] \\end{align} \\] So, we see that the mean-square error is decomposed into the variance of the estimator and a transform (i.e., square) of the bias, including a general, systematic bias term. This is a general dilemma of each estimation strategy. In the end, we want to minimise the MSE, implying that we want a model that is unbiased but not too variable, as an increased variation induces noise. 8.1.1.4 Large Sample Criteria Now, we have seen the properties of linear estimators and derived two important notions to define the accuracy of a sample estimator related to its population counterpart. However, another important characteristic in inductive statistics are asymptotic properties. That is, the behavior of the estimator if the sample size approaches infinity. The two most important concepts in this field are consistency and efficiency (unbiasedness) consistency In order to think about consistency, we need to understand some aspects of the Central Limit Theorem. The asymptotic properties may facilitate deriving the large sample behavior of more complicated estimators. One of these aspects is given as convergence in probability. That means we consider whether the distribution of an estimator approaches some particular probability distribution as the sample sizes increase. To proceed, we state the following definition. \\[ \\lim_{n \\rightarrow \\infty}P(|\\hat{\\theta}_n - c|&gt; \\epsilon) = 0 \\] This property states that as the sample size becomes arbitrarily large, the probability that our estimator will assume a value that is more than \\(\\epsilon\\) away from c will become increasingly negligible, even as \\(\\epsilon\\) becomes smaller. That is, we say that \\(\\hat{\\theta}_n\\) converges in probability to c: \\[ plim\\hat{\\theta}_n = c \\] Convergence in probability does not mean that an estimator will eventually be equal to c, and hence constant itself, but the chance of a deviation from it will become increasingly unlikely. Suppose now that we draw several samples of size n. Let the num- ber of these different samples be N. Consequently, we obtain N estimates, \\(\\hat{\\theta}_n^{(i)}\\). Utilizing the prior definition, we formulate the following law. \\[ plim \\frac{1}{N}\\sum_{i=1}^N \\hat{\\theta}_n^{(i)} = E(\\hat{\\theta}_n) \\] This is a valuable property since when we have drawn many samples, we can assert that it will be highly unlikely that the average of the observed estimates will be a realization of a remote parameter. An important aspect of the convergence in probability becomes obvious now. Even if the expected value of \\(\\hat{\\theta}_n\\) is not equal to \\(\\theta\\) in finite samples, it can still be that \\(plim \\hat{\\theta}_n = \\theta\\). That is, the expected value may gradually become closer to and eventually indistinguishable from \\(\\theta\\), as the sample size n increases. To account for these and all unbiased estimators, we introduce the definition of Consistency. \\[ plim\\hat{\\theta}_n = \\theta \\] This is exactly what we were able to portray in the histograms above. That is if we have a linear estimator and we draw N IID samples from this estimator, then we know that, as N approaches infinity, the estimator will (I) follow a Normal distribution property (=asymptotically normal) and (II) the average of all sample means will approach the expected value of the population mean (=consistent), implying that, even if we have bias in finite samples, this bias will diminish in large samples. Unbiased Efficiency In the previous discussions in this section, we tried to determine where the estimator tends to. This analysis, however, left unanswered the question of how fast the estimator gets there. For this purpose, we introduce the notion of unbiased efficiency. For that, let us suppose we have two unbiased estimators, \\(\\hat{\\theta}\\) and \\(\\hat{\\theta}^*\\). Then, we say that \\(\\hat{\\theta}\\) is a more efficient estimator than \\(\\hat{\\theta}^*\\) if it has a smaller variance; that is: \\[ Var_\\theta(\\hat{\\theta}) &lt; Var_\\theta(\\hat{\\theta}^*) \\] Consequently, no matter what the true parameter value is, the standard error for the first estimator will always be smaller. In general, both properties are highly important in understanding the precision and pace of sample distribution convergence. 8.1.1.5 Maximum Likelihood Estimator The method we discuss next provides one of the most essential tools for parameter estimation. Due to its structure, it is very intuitive. For that, we first suppose that the distribution of some variable Y is characterised by \\(\\theta\\). Then, we usually draw a random sample of n IID observations. Consequently, as we have seen, the joint probability distribution function of the random sample X is given by: \\[ f_X(x_1,\\dots,x_n) = f_Y(x_1)\\cdot \\dots f_Y(x_n) \\] This is known as the likelihood function. This basically indicates that the distribution of the sample X is governed by the parameter \\(\\theta\\) and is given by: \\[ L_X(\\theta) = f_X(x) \\] Usually, we write this as the log likelihood function due to its additivity principle, which makes computation easier: \\[ l_X(\\theta) = \\ln f_X(x) \\] That means we now defined that the distribution of X is given by the parameter space in \\(\\theta\\). Suppose we observe a particular value \\(x = (x_1, x_2, \\dots, x_n)\\) in our sample. The fundamental question here is which parameter values of \\(\\theta\\) best represent the observed relationship. Formally, that means we need to determine the very parameter value that maximizes the probability of the realized density function at x (if the distribution is continuous). That is, we need maximize the log-likelihood function with respect to all possible values of \\(\\theta\\). From baseline analysis, we know that we derive a maximum value of a parameter in a function by taking the first derivative of the function w.r.t. that parameter and set them equal to zero. In our case, this means for the log-likelihood function: \\[ \\frac{\\delta l_X(\\theta)}{\\delta \\theta} = 0 \\] The resulting estimater of \\(\\theta\\) is then defined as the Maximum Likelihood Estimator (MLE), because it yields the parameter value with the greatest likelihood (probability if discrete, and density function if continuous) of the given observation x. The MLE method is extremely attractive since it often produces estimators that are consistent, asymptotically normally distributed, and asymptotically efficient, which means that, as the sample size increases, the estimators derived become unbiased and have the smallest variance. Let’s now look at the practical application of MLE’s related to specific distributions. MLE of the Poisson Distribution The likelihood function of the Poisson distribution is: \\[ L_x(\\lambda) = \\prod_{i=1}^n\\frac{\\lambda^{x_i}}{x_i!}e^{-\\lambda} \\] Then, the log-likelihood function is given as: \\[ \\begin{align*} l_x(\\lambda) &amp;= ln[e^{-n\\lambda}\\prod_{i=1}^n\\frac{\\lambda^{x_i}}{x_i!}e^{-\\lambda}] \\\\ &amp;= -n\\lambda + ln[\\prod_{i=1}^n\\frac{\\lambda^{x_i}}{x_i!}e^{-\\lambda}] \\\\ &amp;= -n\\lambda + ln(\\prod_{i=1}^n\\lambda^{x_i}) - ln(\\prod_{i=1}^n x_i!) \\\\ &amp;= -n\\lambda + \\sum_{i=1}^n(x_iln(\\lambda)) - \\sum_{i=1}^n(ln(x_i!)) &amp;&amp; \\text{product in ln transforms to sum, the rest is simple log rules} \\end{align*} \\] Now, differentiating w.r.t \\(\\lambda\\) and setting it equal to zero gives us: \\[ \\begin{align} \\frac{\\delta l_x(\\lambda)}{\\lambda} = 0 &amp;= -n + \\sum_{i=1}^n\\frac{x_i}{\\lambda} \\\\ \\lambda &amp;= \\frac{1}{n}\\sum_{i=1}^nx_i = \\bar{x} \\end{align} \\] So, we see that the MLE of the Poisson parameter equals the sample mean. MLE of the Normal Distribution We follow the same approach as before. For that, we first define the usual likelihood function of the normal distribution as: \\[ \\begin{align*} L_x(\\mu, \\sigma^2) &amp;= \\prod_{i=1}^nf_Y(x_i) \\\\ &amp;= Y(x_1) * \\dots * Y(x_n) \\\\ &amp;= \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-(x_1 - \\mu)^2 / 2\\sigma^2} * \\dots * \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-(x_n - \\mu)^2 / 2\\sigma^2} \\\\ &amp;= (\\frac{1}{\\sqrt{2\\pi\\sigma^2}})^n\\cdot e^{-\\sum_{i=1}^n(x_i - \\mu)^2 / 2\\sigma^2} \\end{align*} \\] Now, taking the logarithm, we get: \\[ \\begin{align*} l_x(\\mu, \\sigma^2) &amp;= n \\ln(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}) -\\sum_{i=1}^n(x_i - \\mu)^2 / 2\\sigma^2 \\end{align*} \\] Again, if we take the derivative of it w.r.t \\(\\mu\\), we get: \\[ \\begin{align*} \\frac{l_x(\\mu, \\sigma^2)}{\\mu} = 0 &amp;= \\sum_{i=1}^n(x_i - \\mu) / \\sigma^2 \\\\ \\hat{\\mu} &amp;= \\sum_{i=1}^nx_i = \\bar{x} \\end{align*} \\] And w.r.t \\(\\sigma^2\\), we get: \\[ \\begin{align*} \\frac{l_x(\\mu, \\sigma^2)}{\\mu} = 0 &amp;= -\\frac{n}{2\\sigma^2} + \\frac{\\sum_{i=1}^n(x_i-\\mu)^2}{2\\sigma^4} \\\\ n &amp;= \\frac{\\sum_{i=1}^n(x_i-\\mu)^2}{\\sigma^2} \\\\ \\hat{\\sigma}^2 &amp;= \\frac{1}{n}\\sum_{i=1}^n(x_i-\\mu)^2 \\end{align*} \\] which, as we know, is unbiased for the population variance. 8.1.2 Confidence Intervals In the previous chapter, we dealt with the problem of unobservable true estimators by estimating the unknown parameter with a point estimator to obtain a single number from the information provided by a sample. It will be highly unlikely, however, that this estimate — obtained from a finite sample — will be exactly equal to the population parameter value even if the estimator is consistent. The reason is that estimates most likely vary from sample to sample. However, for any realization, we do not know by how much the estimate will be off. To overcome this uncertainty, one might think of computing an interval or, depending on the dimensionality of the parameter, an area that contains the true parameter with high probability. That is, we concentrate in this chapter on the construction of confidence intervals. 8.1.2.1 Confidence Levels and Confidence Interval When inferring on an unknown parameter, we previously resorted to a single estimate. The likelihood of exactly getting this true parameter may be very small in these cases. However, by estimating an interval, which we may denote by \\(I_{\\theta}\\), we use a greater portion of the parameter space, that is, \\(I_\\theta \\in \\Theta\\), and not just a single number. This may increase the likelihood that the true parameter is one of the many values included in the interval. Choosing an appropriate interval is subject to a trade-off between a high probability of the interval containing the true parameter and the precision of gained by narrow intervals. To construct these intervals, we should use the information provided by the sample. Thus, the interval bounds depend on the sample. This, technically, allows us to state that each interval bound is a function that maps the sample space, denoted by X, into the parameter space since the sample is some outcome in the sample space and the interval bound transforms the sample into a value in the parameter space representing the minimum or maximum parameter value suggested by the interval. Formally, we define l(x) as lower and u(x) as upper bound of some samples contained in x. Now comes an important notion. We can derive the probability of the interval lying beyond the true parameter (i.e., either completely below or above) from the sample distribution. These two possible errors occur exactly if either \\(u(x) &lt; \\theta\\) or \\(\\theta &lt; l(x)\\). Our objective is then to construct an interval so as to minimize the probability of these errors occurring. That is: \\[ P(\\theta \\notin[l(X)u(X)]) = P(\\theta &lt; l(X)) + P(u(X) &lt; \\theta) = \\alpha \\] Mostly, we want this probability of error to be equal to a given parameter, \\(\\alpha\\). We commonly know this from the distributional value of \\(\\alph = 0.05\\), such that in 5 % of all outcomes, the true parameter will not be covered by the interval. Definition of a Confidence Level For some parameter \\(\\theta\\), let the probability of the interval not containing the true parameter value be given by the probability of error \\(\\alpha\\). Then, with probability \\(1 - \\alpha\\), the true parameter is covered by the interval [l(X), u(X)]. This is called the confidence level. and is given by the probability: \\[ P(\\theta \\in [l(X) , u(X)]) \\geq 1 − \\alpha \\] Definition of a Confidence Interval If the confidence level probability holds, we can refer to an interval [l(X), u(X)] as \\(1-\\alpha\\) confidence interval (CI) no matter what is the true but unknown parameter value \\(\\theta\\). The interpretation of the confidence interval is that if we draw an increasing number of samples of constant size n and compute an interval from each sample, \\(1-\\alpha\\) of all intervals will eventually contain the true parameter value \\(\\theta\\). This is then directly related to the baseline statistical notions you heard about that 95 % of all CIs will contain the true parameter (or, conversely, we have an \\(\\alpha\\) error rate of 5%, meaning that in 5 100 CIs, we won’t contain the true parameter value \\(\\theta\\)) (PS: This is exactly how hypotheses testing is conducted, but more on this later). As we will see in the examples, the bounds of the confidence interval are often determined by some standardized random variable composed of both the parameter and point estimator, and whose distribution is known (e.g. mean and variance). Furthermore, for a symmetric density function such as that of the normal distribution, it can be shown that with given \\(\\alpha\\), the confidence interval is the tightest if we have \\(p_l = \\alpha/2\\) and \\(p_u = \\alpha/2\\). That corresponds to bounds l and u with distributions that are symmetric to each other with respect to the the true parameter \\(\\theta\\). 8.1.2.2 Confidence Interval for the mean of a Normal Random Variable We will only cover the CI for the Normal Distribution, as this distribution is by far the most commonly known. For that we first start with the normal random variable Y with known variance is known but whose mean is unknown. For the inference process, we draw a sample X of n IID observations. A sufficient and unbiased estimator for \\(\\mu\\) is given by the sample mean, which is distributed as: \\[ \\bar{X} = \\sum_{i=1}^n X_i \\sim N(\\mu, \\frac{\\sigma^2}{n}) \\] If we standardize the sample mean, we obtain the standard normally distributed random variable: \\[ Z = \\sqrt{n}\\frac{\\bar{X}-\\mu}{\\sigma} \\sim N(0,1) \\] For this Z, it is true that: \\[ \\begin{align} P(q_{a/2} \\leq Z \\leq q_{1-\\alpha/2}) &amp;= P(q_{a/2} \\leq \\sqrt{n}\\frac{\\bar{X}-\\mu}{\\sigma} \\leq q_{1-\\alpha/2}) \\\\ &amp;= P(\\frac{\\sigma}{\\sqrt{n}}q_{a/2} \\leq \\bar{X} -\\mu \\leq \\frac{\\sigma }{\\sqrt{n}}q_{1-\\alpha/2}) \\\\ &amp;= P(\\frac{\\sigma}{\\sqrt{n}}q_{a/2} \\leq \\mu - \\bar{X} \\leq \\frac{\\sigma }{\\sqrt{n}}q_{1-\\alpha/2}) \\\\ &amp;= P(\\bar{X} + \\frac{\\sigma}{\\sqrt{n}}q_{a/2} \\leq \\mu \\leq \\bar{X} + \\frac{\\sigma }{\\sqrt{n}}q_{1-\\alpha/2}) \\\\ &amp;= P(\\bar{X} - \\frac{\\sigma}{\\sqrt{n}}q_{1-a/2} \\leq \\mu \\leq \\bar{X} + \\frac{\\sigma }{\\sqrt{n}}q_{1-\\alpha/2}) \\\\ &amp;= P(l(X) \\leq \\mu \\leq u(X)) = 1 - \\alpha \\end{align} \\] Where \\(q_{\\alpha/2}\\) and \\(q_{1- \\alpha/2}\\) are the \\(\\alpha/2\\) and \\(1-\\alpha/2\\) quantiles of the standard normal distribution, respectively That is, by standardizing the distribution of the IID samples, we obtain the probability that the true mean parameter, \\(\\mu\\), will be within the upper and lower bound of the CI when we repeatedly draw n samples is equal to \\(1-\\alpha\\). In other words, in \\(1-\\alpha\\) percent of cases, the CI drawn will include the true parameter. That is: \\[ I_{1-\\alpha} = [\\bar{X} + \\frac{\\sigma}{\\sqrt{n}}q_{1-a/2}, \\bar{X} + \\frac{\\sigma}{\\sqrt{n}}q_{1-a/2}] \\] 8.1.2.3 Confidence Interval for the mean of a Normal Random Variable with unknown Variance Let us once again construct a confidence interval for a normal random variable Y but this time we assume that the variance and the mean are unknown. If we again take n IID samples, take their mean values and standardize the variables with an unknown variance and mean term, then we obtain the new standardized random variable as a student’s t distribution with n-1 DOF: \\[ t = \\sqrt{n}\\frac{\\bar{X} - \\mu}{s} \\] Where \\(s^2 = 1/(n-1)\\sum_{i=1}^n(X_i - \\bar{X})^2\\). Therefore, we can state: \\[ P(t_{\\alpha/2}(n-1) \\leq t \\leq t_{1 - \\alpha/2}(n-1)) = 1-\\alpha \\] Where \\(t_{\\alpha/2}\\) and \\(t_{1- \\alpha/2}\\) are the \\(\\alpha/2\\) and \\(1-\\alpha/2\\) quantiles of the t-distribution with n-1 DOF, respectively. Using the same approach as before, we can derive the Confidence Interval of this distribution again as: \\[ \\begin{align} P(t_{a/2}(n-1) \\leq t \\leq t_{1-\\alpha/2}(n-1)) &amp;= P(t_{a/2}(n-1) \\leq \\sqrt{n}\\frac{\\bar{X} - \\mu}{s} \\leq t_{1-\\alpha/2}(n-1)) \\\\ &amp;= P(\\frac{s}{\\sqrt{n}}t_{a/2}(n-1) \\leq \\bar{X} -\\mu \\leq \\frac{s }{\\sqrt{n}}t_{1-\\alpha/2}(n-1)) \\\\ &amp;= P(\\frac{s}{\\sqrt{n}}t_{a/2}(n-1) \\leq \\mu - \\bar{X} \\leq \\frac{s}{\\sqrt{n}}t_{1-\\alpha/2}(n-1)) \\\\ &amp;= P(\\bar{X} + \\frac{s}{\\sqrt{n}}t_{a/2}(n-1) \\leq \\mu \\leq \\bar{X} + \\frac{s}{\\sqrt{n}}t_{1-\\alpha/2}(n-1)) \\\\ &amp;= P(\\bar{X} - \\frac{s}{\\sqrt{n}}t_{1-a/2}(n-1) \\leq \\mu \\leq \\bar{X} + \\frac{s}{\\sqrt{n}}t_{1-\\alpha/2}(n-1)) \\\\ &amp;= P(l(X) \\leq \\mu \\leq u(X)) = 1 - \\alpha \\end{align} \\] That is: \\[ I_{1-\\alpha} = [\\bar{X} - \\frac{s}{\\sqrt{n}}t_{1-a/2}(n-1), \\bar{X} + \\frac{s}{\\sqrt{n}}t_{1-a/2}(n-1)] \\] 8.1.3 Hypothesis Testing Inference on some unknown parameter meant that we had no knowledge of its value and therefore we had to obtain an estimate. This could either be a single point estimate or an entire confidence interval. However, sometimes, one already has some idea of the value a parameter might have or used to have. Thus, it might not be important to obtain a particular single value or range of values for the parameter, but instead gain sufficient information to conclude that the parameter more likely either belongs to a particular part of the parameter space or not. So, instead we need to obtain information to verify whether some assumption concerning the parameter can be supported or has to be rejected. This brings us to the field of hypothesis testing. Next to parameter estimation that we covered in the last two parts, it constitutes the other important part of statistical inference; that is, the procedure for gaining information about some parameter. In essence, we use hypothesis testing to determine whether a certain parameter of interest, given its statistical properties and distribution, is, with a sufficient probability, equal to a pre-defined, or hypothesized, value of the parameter space. 8.1.3.1 Hypotheses Setting up the hypotheses Before we can test any hypothesis, we first need to understand what the term actually means. In the case of hypothesis testing, we have two competing statements to decide upon. These statements are the hypotheses of the test. Since in statistical inference we intend to gain information about some unknown parameter \\(\\theta\\), the possible results of the test should refer to the parameter space \\(\\Theta\\) containing all possible values that \\(\\theta\\) can assume. More precisely, to form the hypotheses, we divide the parameter space into two disjoint sets, namely \\(\\Theta_0\\) and \\(\\Theta_1\\). We assume that the unknown parameter is either in \\(\\Theta_0\\) and \\(\\Theta_1\\). Now, with each of the two subsets we associate a hypothesis: Null Hypothesis - \\(H_0\\): States that the parameter \\(\\theta\\) is in \\(\\Theta_0\\) Alternative Hypothesis - \\(H_1\\): States that the parameter \\(\\theta\\) is in \\(\\Theta_2\\) The null hypothesis may be interpreted as the assumption to be maintained if we do not find material evidence against it. Decision Rule The task of hypothesis testing is to make a decision about these hypotheses. So, we either cannot reject the null hypothesis and, consequently, have to reject the alternative hypothesis, or we reject the null hypothesis and decide in favor of the alternative hypothesis. A hypothesis test is designed such that the null hypothesis is maintained until evidence provided by the sample is so significant that we have to decide against it. This leads us to the two common ways of using the test. In general, the decision rule of any hypothesis test, and ergo the main idea behind hypotheses testing is the following. We want to test whether a sample estimate is equal to a true parameter. Since we can only observe the sample distribution, we need to make assumptions on the distribution and the asymptotic behavior of the parameters. We have seen how to ensure for consistency, asymptotic normality and efficiency under linear estimators. For these estimators, we now define a Null Hypothesis stating that they are equal to a true population parameter which we define individually (for instance, we usually state that the true parameter is 0 in a regression model). Then, we draw the n IID samples with the properties discussed above and we look at their parameter statistics (or moments), mostly in terms of their expected estimate and their variation. Based on both parameter statistics, we then draw the respective distribution and obtain the probability density curve. If the probability mass of this density curve is lower than the defined \\(\\alpha\\) benchmark, then we say can state with a sufficient certainty that, given the distributional characteristics, the hypothesized true value is incorrect and, thus, the Null Hypothesis that the true value is equal to the hypothesized value (e.g. 0) can be rejected. #### Error Types We have to be aware that no matter how we design our test, we are at risk of committing an error by making the wrong decision. In general we run risk of making two distinct errors. Type 1 and Type 2 Error Type 1 Error: The error resulting from rejection of the null hypothesis given that it is actually true. This is known as False Negative Type 2 Error: The error resulting from failing to reject the null hypothesis given that the alternative holds. This is known as False Positive The probability of a type I error is the probability of incorrectly rejecting a correct null hypothesis, which is also the size of the test (this is just the threshold \\(\\alpha\\)). To understand this, note that the size of a test tells you how likely it is that this or a more extreme outcome would have arisen just out of pure chance. That is, in \\(\\alpha\\) percent of the cases one would reject the the Null Hypothesis although the result was created purely by chance, and should, thus, have been failed to reject. Another important piece of terminology in this area is the power of a test. The power of a test is defined as the probability of (appropriately) rejecting an incorrect null hypothesis. The power of the test is also equal to one minus the probability of a type II error. Note that there is no chance for a free lunch (i.e. a cost-less gain). What happens if the size of the test is reduced (e.g. from a 5% test to a 1% test)? The chances of making a type I error would be reduced, but so would the probability that the null hypothesis would be rejected at all, so increasing the probability of a type II error. So there always exists, therefore, a direct trade-off between type I and type II errors when choosing a significance level. The only way to reduce the chances of both is to increase the sample size or to select a sample with more variation, thus increasing the amount of information upon which the results of the hypothesis test are based. In practice, up to a certain level, type I errors are usually considered more serious and hence a small size of test is usually chosen (5% or 1% are the most common). 8.1.3.2 One- and Two-Tailed Test for the Parameter \\(\\mu\\) of the Normal Distribution When \\(\\sigma^2\\) Is Known The most famous quantification for hypothesis testing results in the Normal Distribution. Therein, we can test two hypotheses. The second considers a one-tailed test. Therein, we test the hypothesis that the parameter of interest, \\(\\mu_0\\), assumes some defined value which is less than or equal to a true, underlying, benchmark called \\(\\mu\\). As such, we say that \\(H_0: \\mu_0 \\leq \\mu\\) \\(H_1: \\mu_0 &gt; \\mu\\) The second considers a two-tailed test. Therein, we test the hypothesis that the parameter of interest, \\(\\mu_0\\), assumes some defined value which is equal to a true, underlying benchmark, called \\(\\mu\\). As such, we say that \\(H_0: \\mu_0 = \\mu\\) \\(H_1: \\mu_0 \\neq \\mu\\) To infer in both settings, we draw a setting of n IID samples of X (\\(X_1,\\dots,X_n\\)). As we know, if we have a set of n IID samples, we can model it as a normal random variable Y with known variance \\(\\sigma^2\\). Consequently, we can take the following test statistic: \\[ t(X) = \\frac{\\bar{X} - \\mu}{\\sigma / \\sqrt{n}} \\] whereas \\(\\sigma / \\sqrt{n}\\) is also called the Standard Error (SE). Note that this is the formula to test for standard normal variables with \\(N(\\mu, \\sigma^2)\\), where we need to define the two parameters first, but where the \\(\\sigma^2\\) value is known. Let’s go over the t-statistic example for a two-tailed test. Remember that the t-statistic follows a standard normal distribution which assigns a probability density to each value of t, based on the mean and variance values of the underlying sample of random variables. In order to test whether a given sample mean now deviates enough from the assumed true, underlying, benchmark called \\(\\mu\\), we follow these steps: Calculate the t-statistic from the given sample mean, sample variance and number of observations (mostly we assume that the true \\(\\mu\\) equals 0, thereby having no effect in a framework) Define a test size benchmark \\(\\alpha\\) (e.g. 0.1, 0.05, 0.01) Get the percentile or quantile values of \\(\\alpha\\) for the standard normal distribution, \\(q_{\\alpha/2}\\) and \\(q_{1-\\alpha/2}\\), for which the density of the t-statistic comprises of exactly \\(\\alpha\\) % of the entire probability density Compare the t-value with the percentile values for the respective \\(\\alpha\\) benchmarks (= critical values) If we have an \\(\\alpha = 0.05\\), then the critical values are [-1.96, 1.96], because these are the 2.5th and 97.5th percentiles of the standard normal distribution, implying that they comprise of 95 % of the probability density of the underlying distribution. Consequently, if the t-value is not within the given interval, we can reject the Null-Hypothesis that \\(\\mu_0 = \\mu\\), because the underlying probability density is less than 5% of the overall density, given the distributional characteristics. We can also visualise the idea behind t-statistics and p-values in the following way: h=na.omit(survey$Height) pop.mean=mean(h) h.sample = sample(h,50) t.test(h.sample,mu=pop.mean) ## ## One Sample t-test ## ## data: h.sample ## t = 0.34754, df = 49, p-value = 0.7297 ## alternative hypothesis: true mean is not equal to 172.3809 ## 95 percent confidence interval: ## 170.2482 175.4054 ## sample estimates: ## mean of x ## 172.8268 ggttest(t.test(h.sample,mu=pop.mean)) 8.1.3.3 The P-Value The p value is the notion we usually interact with when testing hypotheses. In essence, it displays the significance level for the respective t-statistic of our parameter statistics and thus tells us whether to reject the null hypothesis or not. In other words, it shows us at which significance level this value of t(x) would still lead to a decision of failing to reject the null hypothesis while any value greater than t(x) would result in its rejection. We can interpret the p-value as follows. Suppose we obtained a sample outcome x such that the test statistics sassumed the corresponding value t(x). Now, the p value indicates the probability that, given our assumption about the true parameter, our hypothesized value is indeed the true value. In other words, it states how likely it is that the true value is indeed the hypothesized value, given the distribution of our parameters. If t(x) is a value pretty close to the median of the distribution of t(X), then the chance that the true value is indeed equal to the hypothesised value, given our distribution, is fairly feasible. Then, the p-value will be large. However, if, instead, the value t(x) is so extreme that the chances will be minimal under the null hypothesis that the true value equals the hypothesised value, this will lead to a very low p-value. If p is less than some given significance level \\(\\alpha\\), we reject the null hypothesis and we say that the test result is significant. 8.2 Introduction to Regression Analysis We now have shown important properties related to (I) probability theory (II) linear algebra and (III) inductive statistics. Especially, we have looked at the joint behavior of bivariate and multivariate data, their interdependences and covariation properties, how to represent these properties in form of matrix notation and which underlying properties this notation has in terms of linearity as well as how we can induce statistical properties from a sample to an unobservable, but true population which is consistent, asymptotically normal and efficient. Now, it is time to combine the insights we retrieved from each topic and put it into a general framework. This framework is called Regression Analysis. It makes use of the statistical and inductive properties discussed to represent the joint behavior of bivariate or multivariate data. In essence, we make use of probability theory concept to understand the distributional properties of random variables, use linear algebra properties to understand the relational properties and convey them into a mathematical setting and use inductive statistics properties to assess and refer the underlying relationship from a sample to a true, general population. The most fundamental assumption of regression approaches is that the variation of a (dependent) variable can, at least to some extent, be explained by the functional relationship between the variable and a consortium of (explanatory) variables. More specifically, regression is an attempt to explain variation (or movements) in a variable by reference to movements in one or more other variables. To understand this, we make use of the correlation and linear dependency properties. 8.2.1 The role of correlation The underlying interest is to understand how two or more variables behave together, or, in dependence of each other. Due to the statistical properties of linear random variables, we especially want to understand the linear, joint behavior of the underlying variables. This can be assessed by the covariance matrix. More precisely, we are interested their *correlation expressed by the correlation coefficient, which measures the linear dependence of two variables. Generally, we know that correlation assumes values between −1 and 1 where the sign indicates the direction of the linear dependence. If we say that two variables are correlated with each other, it is stated that there is evidence for a linear relationship between the two variables, and that movements of the two variables are on average related to an extent given by the correlation coefficient. 8.2.2 The simple / univariate regression model Correlation implies that two variables behave similarly to a certain degree, which is quantified by the correlation coefficient. Thus, it is not implied that changes in x cause changes in y. Now, we go a step further and represent the relational properties in terms of a cause-and-effect framework, where we define one variable as cause, or explanatory, and the other as effect, or dependent, variable. That is, we also say that the dependent variable is endogenous (and thus depends on the model parameters and has an own probability distribution) whereas the explanatory variable(s) is (are) exogenous, implying that they do not depend on model parameters and thus have fixed (‘non-stochastic’) values. This context is represented by a regression model. In this context, the joint behavior described in the previous section is now thought of as y being some function of x and possibly some additional quantity. In other words, we assume a functional relationship between the two variables given by the deterministic relationship of equation: \\[ y = f(x) \\] Now, we assume two important properties: The underlying relation is based on linear properties The underlying relation is likely to be influenced by other parameters not captured by the model (residuals) Consequently, the underlying relationship transforms to: \\[ y = \\alpha + \\beta x + \\epsilon \\] Thereby, the residuals are assumed to be IID, normally distributed with mean 0 and some constant variance \\(\\sigma_e^2\\) and independent of x: \\[ \\epsilon \\underbrace{\\sim}_{IID} N(0, \\sigma^2) \\] With this assumption, the equation above is called a simple linear regression or a univariate regression. The parameter \\(\\beta\\) determines how much y changes, on average, when x changes by one unit. Thus, it can be stated as expected change in y if x changes. On the other hand, the parameter \\(\\alpha\\) defines the value of y in the case that x is exactly zero. 8.2.2.1 Estimating the regression model: Simple Ordinary Least Squares (OLS) The most common approach to estimate regression models is by using a linear fit which combines the dependent and explanatory variable with the least loss. This fit is mostly represented by a linear graph which combines the two random variables. Note that we already defined one formulation for such a loss, in term of the bias. In essence, the bias term in a regression model is the vertical difference between the observed and estimate dependent variable value. Looking at the formula above, this is represented by the residual term. With regards to the overall loss, we are interested in finding a function which minimizes the overall difference between the estimated and observed dependent variable points. Formally, we have to solve a minimisation problem of the form: \\[ \\underbrace{min}_{\\alpha,\\beta}\\sum_{i=1}^n (y_i - \\hat{\\alpha} -\\hat{\\beta} x_i)^2 \\] Whereas i defines the respective i’th observation of the model. That is, we need to find the estimates of \\(\\alpha\\) and \\(\\beta\\) that minimize the total of the squared errors. By finding these values, we understand that our regression line represents the least distance between observed and estimated values and thus represents an optimal fit. In other words, we find a linear fit which minimizes the bias of the model, which is given by the residual term. This is also known as Ordinary Least Squares (OLS). In the OLS case, the minimum is obtained analytically through first derivatives with respect to \\(\\alpha\\) and \\(\\beta\\), respectively. The resulting estimates are, then, given by: \\[ \\begin{align} \\hat{\\beta} &amp;= \\frac{\\frac{1}{n}\\sum_{i=1}^n(x_i - \\bar{x})(y_i - \\bar{y})}{\\frac{1}{n}\\sum_{i=1}^n(x_i - \\bar{x})^2} = \\frac{cov(x,y)}{var(x)}\\\\ \\hat{\\alpha} &amp;= \\bar{y} - \\hat{\\beta}x \\end{align} \\] In this formula, we see the underlying property of correlation between the dependent and independent variable when looking at \\(\\hat{\\beta}\\). As we can see the numerator displays the covariance between x and y, whereas the denominator represents the covariance of x with itself, which is given by its variance. In the case of independence, we would thus derive that \\(\\hat{\\beta}\\) would be zero and, thereby, \\(\\hat{\\alpha} = \\bar{y}\\). This is the fundamental insight of regression analysis: The linear relation between a dependent and an explanatory variable is quantified by the covariation of both variables relative to the covariation of the explanatory variable with itself (= the variation itself). As such, we always have a relative behavior due to the dependency behavior of the two variables. Although this is a general statement, it can easily be related to financial contexts. Let’s quickly visualise what this implies. For that, let’s look at the cars dataset: # Run an OLS cars &lt;- mtcars fit &lt;- lm(mpg ~ hp, data = cars) # Get the predicted and residual values cars$pred &lt;- predict(fit) cars$res &lt;- residuals(fit) # Get the data straight and plot it cars %&gt;% ggplot(aes(x = hp, y = mpg)) + geom_smooth(method = &quot;lm&quot;, se = F, color = &quot;lightgrey&quot;) + geom_segment(aes(xend = hp, yend = pred), alpha = .2) + geom_point(aes(color = res)) + scale_color_gradient2(low = &quot;blue&quot;, mid = &quot;white&quot;, high = &quot;red&quot;) + guides(color = FALSE) + geom_point(aes(y = pred), shape = 1) + theme_bw() ## Warning: `guides(&lt;scale&gt; = FALSE)` is deprecated. Please use `guides(&lt;scale&gt; = &quot;none&quot;)` instead. ## `geom_smooth()` using formula &#39;y ~ x&#39; As we can see, this is the idea behind “minimising the sum of squared residuals”. We define a line which takes all the residuals, given by the vertical distsnce of the observed and predicted values, square their difference and take the sum. This is our bias term, and we want to minimise this bias accordingly. 8.2.3 The multivariate regression model Next, we focus on the case in which we assume that more than one explanatory variable is associated with the dependent variable. To do so, we turn to multivariate regressions. This type of regression setting explains the linear relationship between several independent variables and some dependent variable we observe. It has the following form: \\[ y = \\alpha + \\beta_1x_i + \\dots + \\beta_kx_k + \\epsilon \\] As we know, we can also write this in vector notation, simply by: \\[ y = X\\beta + \\epsilon \\] Where y is a \\(n \\times 1\\) column vector of form \\[ y = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_N \\end{bmatrix} \\] and X is a \\(N \\times k\\) matrix of form \\[ X = \\begin{bmatrix} 1 &amp; x_{11} &amp; \\dots &amp; x_{1k} \\\\ 1 &amp; x_{21} &amp; \\dots &amp; x_{2k} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; x_{N1} &amp; \\dots &amp; x_{Nk} \\end{bmatrix} \\] and \\(\\beta\\) is a \\(k \\times 1\\) column vector of form: \\[ \\beta = \\begin{bmatrix} \\beta_1 &amp; \\beta_2 &amp; \\dots &amp; \\beta_k \\end{bmatrix}&#39; \\] Lastly, \\(\\epsilon\\) is again a \\(n \\times 1\\) column vector of form: \\[ y = \\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_N \\end{bmatrix} \\] Much like with the simple OLS, we also estimate the coefficients of the multivariate regression model by following the minimisation of squared residuals approach. This is simply an extension of the simple OLS model in the case with just one regressor and a constant. Especially, we minimise: \\[ \\min_{\\hat{\\beta}} = \\sum_{i=1}^n(y_i - \\hat{y})^2 = (y - X\\beta)&#39;(y-X\\beta) \\] Differential calculus and matrix algebra lead to the optimal regression coefficient estimates and estimated residuals given by: \\[ \\hat{\\beta} = (X&#39;X)^{-1}X&#39;y \\] whereas \\(\\hat{\\beta}\\) is a \\((k+1) \\times 1\\) column vector including the estimated coefficients of each of the explanatory variables of our model. In essence, each element fo the column vector can be interpreted in the same fashion as in the simple OLS case with one slight difference. That is, it is the covariation of the explanatory and dependent variable relative to the variation of the explanatory variable itself, \\(\\color{blue}{\\text{controlling for, or keeping constant, the variation of the remaining variables}}\\). Let’s quickly visualise this for a regression setting with two variables: library(plotly) library(reshape2) ## ## Attaching package: &#39;reshape2&#39; ## The following objects are masked from &#39;package:data.table&#39;: ## ## dcast, melt ## The following object is masked from &#39;package:tidyr&#39;: ## ## smiths # Run multiple OLS cars &lt;- mtcars fit &lt;- lm(mpg ~ hp + cyl, data = cars) # Graph Resolution (more important for more complex shapes) graph_reso &lt;- 0.05 # Setup Axis axis_x &lt;- seq(min(cars$hp), max(cars$hp), by = graph_reso) axis_y &lt;- seq(min(cars$cyl), max(cars$cyl), by = graph_reso) # Sample points mpg_lm_surface &lt;- expand.grid(hp = axis_x, cyl = axis_y, KEEP.OUT.ATTRS = F) mpg_lm_surface$mpg &lt;- predict.lm(fit, newdata = mpg_lm_surface) mpg_lm_surface &lt;- acast(mpg_lm_surface, cyl ~ hp, value.var = &quot;mpg&quot;) # Create color variables and plot hcolors=c(&quot;red&quot;,&quot;blue&quot;)[cars$am] cars_plot &lt;- plot_ly(cars, x = ~hp, y = ~cyl, z = ~mpg, text = ~am, # EDIT: ~ added type = &quot;scatter3d&quot;, mode = &quot;markers&quot;, marker = list(color = hcolors)) # Add a hyperplane cars_plot &lt;- add_trace(p = cars_plot, z = mpg_lm_surface, x = axis_x, y = axis_y, type = &quot;surface&quot;) cars_plot ## Warning: &#39;surface&#39; objects don&#39;t have these attributes: &#39;mode&#39;, &#39;marker&#39; ## Valid attributes include: ## &#39;_deprecated&#39;, &#39;autocolorscale&#39;, &#39;cauto&#39;, &#39;cmax&#39;, &#39;cmid&#39;, &#39;cmin&#39;, &#39;coloraxis&#39;, &#39;colorbar&#39;, &#39;colorscale&#39;, &#39;connectgaps&#39;, &#39;contours&#39;, &#39;customdata&#39;, &#39;customdatasrc&#39;, &#39;hidesurface&#39;, &#39;hoverinfo&#39;, &#39;hoverinfosrc&#39;, &#39;hoverlabel&#39;, &#39;hovertemplate&#39;, &#39;hovertemplatesrc&#39;, &#39;hovertext&#39;, &#39;hovertextsrc&#39;, &#39;ids&#39;, &#39;idssrc&#39;, &#39;legendgroup&#39;, &#39;legendgrouptitle&#39;, &#39;legendrank&#39;, &#39;lighting&#39;, &#39;lightposition&#39;, &#39;meta&#39;, &#39;metasrc&#39;, &#39;name&#39;, &#39;opacity&#39;, &#39;opacityscale&#39;, &#39;reversescale&#39;, &#39;scene&#39;, &#39;showlegend&#39;, &#39;showscale&#39;, &#39;stream&#39;, &#39;surfacecolor&#39;, &#39;surfacecolorsrc&#39;, &#39;text&#39;, &#39;textsrc&#39;, &#39;type&#39;, &#39;uid&#39;, &#39;uirevision&#39;, &#39;visible&#39;, &#39;x&#39;, &#39;xcalendar&#39;, &#39;xhoverformat&#39;, &#39;xsrc&#39;, &#39;y&#39;, &#39;ycalendar&#39;, &#39;yhoverformat&#39;, &#39;ysrc&#39;, &#39;z&#39;, &#39;zcalendar&#39;, &#39;zhoverformat&#39;, &#39;zsrc&#39;, &#39;key&#39;, &#39;set&#39;, &#39;frame&#39;, &#39;transforms&#39;, &#39;_isNestedKey&#39;, &#39;_isSimpleKey&#39;, &#39;_isGraticule&#39;, &#39;_bbox&#39; 8.2.3.1 Incorporating conditional variation in \\(\\beta\\): The Frisch-Waugh Lowell Theorem By running a multivariate OLS, we automatically account for the variation of the remaining variables with both the dependent and the explanatory variable of interest. Consequently, the \\(\\hat{\\beta}_i\\) coefficient solely displays the variation between the these variables AFTER incorporating the remaining variation based on the other regressors. This is based on the projection and residual matrix properties that is based on the Frish-Waugh Lowell Theorem which was introduced in Empirical Methods. The Frisch-Waugh-Lowell Theorem allows to regress a dependent variable on an explanatory variable while automatically controlling for all other variables in a multiple regression model. To understand the theorem, first note the projection matrix, which gives the vector of fitted value of y when regressing y on X \\[ \\hat{y} = X(X&#39;X)^{-1}X&#39;y = P_X y \\] And the Residual matrix, which gives the vector of residuals when regressing y on X: \\[ \\epsilon = (I_N - X(X&#39;X)^{-1}X&#39;)y = M_X y \\] By performing a partitioned regression, we can divide the regression model into a part we care about and a part we use as “controls”. This delivers us a \\(1 \\times 1\\) scalar of \\(\\hat{\\beta}_i\\) and a \\((k-1) \\times 1\\) vector of \\(\\hat{\\beta}_{-i}\\) control estimates. Note that it still holds that \\(\\hat{\\beta} = (X&#39;X)^{-1}X&#39;y\\). This is now just divided into the both \\(\\betas\\). Especially, we define: \\[ \\begin{bmatrix} \\hat{\\beta_i} \\\\ \\hat{\\beta}_{-i} \\end{bmatrix} = \\begin{bmatrix} X&#39;_iX_i &amp; X&#39;_iX_{-i} \\\\ X&#39;_{-i}X_{i} &amp; X&#39;_{-i}X_{-i} \\end{bmatrix} \\begin{bmatrix} X_i&#39;y \\\\ X_{-i}&#39;y \\end{bmatrix} = \\begin{bmatrix} (X&#39;_iM_{-i}X_i)^{-1}X&#39;_iM&#39;_{-i}y \\\\ (X&#39;_{-i}M_{i}X_{-i})^{-1}X&#39;_{-i}M&#39;_{i}y \\end{bmatrix} \\] As we are only interested in the coefficient of the variable of interest, we get the first important result. \\[ \\hat{\\beta_i} = (X&#39;_iM_{-i}X_i)^{-1}X&#39;_iM&#39;_{-i}y \\] Whereas \\(M_{-i} = (I_N - X_{-i}(X_{-i}&#39;X_{-i})^{-1}X&#39;_{-i})\\) are the residuals of regressing y on \\(x_{-i}\\) (= the controls). Next, we can go a step further, and define \\(X_i^* = M_{-i}X_i\\). That is, \\(X_i^*\\) are the residuals of regressing \\(X_i\\) on all the control variables, \\(X_{-i}\\). In this case, we transform the formula a little and obtain: \\[ \\hat{\\beta_i} = (X&#39;_iX_i^*)^{-1}X_i&#39;^*y \\] This means that the coefficient on the i’th variable of interest can be retrieved by regressing the dependent variable on the residuals from the regression of the variable of interest \\(X_i\\) on the controls \\(X_{-i}\\). Note that \\(X_i^* = M_{-i}X_i\\) stands for the residuals we get for the first regression. The Frisch-Waugh Lowell theorem defines the underlying relationship. It states that the i’th coefficient of a multivariate regression incorporates the variation of all control variables. This is done by first regressing the specific explanatory variable on all other explanatory variables and then taking the residuals of the first regression and regressing our dependent variable on these residuals. This is how we “control” for additional covariates within a regression, which means that we incorporate any remaining variation and thus ensure that the variation of the explanatory variable is orthogonal to (or independent of) the variation of the control variables. Especially, Frisch-Waugh Lowell prove that the following three settings deliver the same coefficient estimates for the variable of interest: Regressing y on x and controls (y = X + controls) Regressing y on the residuals of regressing X on the controls (y = res(X|controls)) Regressing the residuals of regressing y on the controls on the residuals of regressing X on the controls (res(y|controls) = res(X|controls)) This is a fundamental result of econometrics. It tells us that, when we regress \\(X_i\\) on controls, we account for the entire variation of \\(X_i\\) that can be explained by the controls. The part which cannot be explained is necessarily incorporated into the residual term of the regression. If we now regress the dependent variable on the residuals, then we only use the variation which cannot be explained by the set of controls. If we assume that the model is unbiased (e.g. that there is no OVB and thus the existing variables are able to explain all of the variation in the dependent variable) then it must hold that the residual term incorporates all of the remaining variation that is able to explain the movements of the dependent variable. Consequently, we indirectly “control” for the remaining variation as we would when incorporating the set of controls in the first place. This is a fundamental understanding, because it shows that the residuals of a regression incorporate the remaining variation which was not explained by the variables of interest. Let us calculate this quickly: Let’s quickly visualise what this implies. For that, let’s look at the cars dataset: # Run multiple OLS cars &lt;- mtcars fit &lt;- lm(mpg ~ hp + cyl, data = cars) fit_2 &lt;- lm(mpg ~ lm(hp ~ cyl)$residuals, data = cars) fit_3 &lt;- lm(lm(mpg ~ cyl)$residuals ~ lm(hp ~ cyl)$residuals, data = cars) # Print all the outputs stargazer(fit, fit_2, fit_3, type = &quot;text&quot;) ## ## ================================================================================ ## Dependent variable: ## ------------------------------------------------------------ ## mpg residuals ## (1) (2) (3) ## -------------------------------------------------------------------------------- ## hp -0.019 ## (0.015) ## ## cyl -2.265*** ## (0.576) ## ## residuals -0.019 -0.019 ## (0.029) (0.015) ## ## Constant 36.908*** 20.091*** -0.000 ## (2.191) (1.075) (0.551) ## ## -------------------------------------------------------------------------------- ## Observations 32 32 32 ## R2 0.741 0.015 0.053 ## Adjusted R2 0.723 -0.018 0.021 ## Residual Std. Error 3.173 (df = 29) 6.082 (df = 30) 3.120 (df = 30) ## F Statistic 41.422*** (df = 2; 29) 0.442 (df = 1; 30) 1.681 (df = 1; 30) ## ================================================================================ ## Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 As we can see, this is the case. Consequently, we are able to replicate the FWL theorem and understand how we incorporate and account for different types of variations within a regressional setting. 8.2.4 Goodness of Fit We evaluate regression models by goodness-of-fit measures. Consequently, we’ll have to draw on topics from probability theory, parameter estimation, and hypothesis testing. In particular, we will use the normal distribution, t-statistic, and F-statistic as underlying methods to define the goodness-of-fit, and use statistics such as \\(R^2\\), Adjusted \\(R^2\\) and idiosyncratic to systematic risk components as fundamental evaluation tools for the model. There are several characteristics that are commonly used to evaluate how suitable the OLS slope is. One of the methods to figure this out is defined as coefficient of Determination, or \\(R^2\\). 8.2.4.1 R2 The \\(R^2\\) statistic is a goodness-of-fit measure. In general, the R2 value gives a notion as to how much of the variation of the dependent variable can be explained by the underlying explanatory variable(s). It is given by the following formula: \\[ R^2 = 1 - \\frac{RSS}{TSS} \\] Whereas RSS is defined as the sum of all squared residual terms, thus stating how well the predicted fit the actual data. TSS is the Total Sum of Squares. It is calculated as sum of the squared differences of each actual observation and their corresponding mean value: \\[ \\begin{align*} RSS &amp;= \\sum_{i=1}^n(\\hat{y_i}-\\bar{y})^2\\\\ TSS &amp;= \\sum_{i=1}^n(y_i-\\bar{y})^2 \\end{align*} \\] Their fraction displays how well the predicted data fits the actual data in relative to how the average fits the actual data. If the model fits badly, it will result in values for SSR close to the TSS (sometimes even above TSS values) and the \\(R^2\\) value will decline. 8.2.4.2 Adjusted R2 One issue with the \\(R^2\\) value is that it will increase in magnitude with an increasing set of variables. This is problematic in multivariate regression settings, as it would imply that more variables are always better for a regression system. This is because the \\(R^2\\) statistic cannot distinguish between coherent variation that an additional variable adds to the model (e.g. the true variation between y and x) and spurious variaiton, or noise. In order to account for the added noise, we need to incorporate DOF into the calculation. The underlying formula is: \\[ Adj. R^2 = 1 - (1-R^2)\\frac{n-1}{n-k-1} \\] One can interpret this new measure of fit as penalizing excessive use of independent variables. Instead, one should set up the model as parsimonious as possible. To take most advantage of the set of possible independent variables, one should consider those that contribute a maximum of explanatory variation to the regression. 8.2.4.3 Non-Sequential Model selection metrics The perhaps most common form of error attribution is the Sum of Squared Residuals (SSR), as we have seen above. Another option that researchers came up with is the interaction of evaluation criteria based on the coefficients a model incorporates. As such, assume that \\(\\hat\\sigma_k^2\\) is the SSR under the model with k regression coefficient. Then Akaike suggested measuring the goodness of fit for this particular model by balancing the error of the fit against the number of parameters in the model. Out of this, the Akaike Information Criteria (AIC) arised, in form of: \\[ AIC = \\log \\hat\\sigma_k^2 + \\frac{n+2k}{n} \\] The value of k yielding the minimum AIC specifies the best model. The idea is roughly that minimising \\(\\hat\\sigma_k^2\\) is a reasonable objective. However, as we know, this metric decreases monotonically as k increases. Therefore, we ought to penalize the error variance by a term proportional to the number of parameters. As you may think now, there are numerous approaches to define this penalty parameter in the literature. For instance, we can be correct the form based on small-sample distributional results for the linear regression model to obtain the Bias Corrected Akaike Information Criteria (AICc): \\[ AICc = \\log \\hat\\sigma_k^2 + \\frac{n+k}{n-k-2} \\] Further, we may also derive a correction term based on Bayesian arguments, leading to the Bayesian Information Criteria (BIC): \\[ BIC = \\log \\hat\\sigma_k^2 + \\frac{k\\log n}{n} \\] Notice that the penalty term in BIC is much larger than in AIC, consequently, BIC tends to choose smaller models. Which models to choose, you may ask. In general, there is an implicit understanding that the BIC is superior in large samples with fewer parameters AIC is superior in small samples with a relatively high number of parameters Usually, these scores are used on the training set to evaluate how well the model performed as they are able to account for the trade-off of bias and variance. Based on them, we then select the best fitting model and predict the test data as well as obtain the accuracy scores based on said test data. #### t-statistic and p-values To focus more on inductive properties, we now can evaluate the regression coefficient \\(\\beta\\) in terms of its materiality, or significance, in influencing the dependent variable. Doing so, we can make use of the inductive properties we discussed earlier. In essence, when evaluating the significance of a \\(\\beta\\) coefficient, we usually state the following Hypotheses: \\(H_0: \\hat{\\beta} = \\beta = 0\\) \\(H_1: \\hat{\\beta} \\neq \\beta\\) That is, we assume that the true effect of x on y is zero and that the estimated coefficient will be equal to the hypothesised, true but unobservable, coefficient. Then, we run the regression model and obtain both the expected value \\(\\hat{\\beta}\\) as well as its standard deviation \\(\\hat{\\sigma}(\\beta)\\). Based on the assumption of a standard normal distribution, we then calculate the corresponding t statistic as: \\[ t(\\hat{\\beta}) = \\frac{\\hat{\\beta} - 0}{SE(\\hat{\\beta})} \\] Whereas \\(SE(\\hat{\\beta}) = \\hat{\\sigma}(\\beta) / \\sqrt{n}\\). Now, we follow the common approach discussed previously and define a common \\(\\alpha\\) threshold which accounts for \\(\\alpha\\) % of the density of the standard normal distribution, and take the percentile values as critical points. If the corresponding t-statistic is larger than the respective critical values, then we know that the \\(\\beta\\) coefficient is significantly different from zero. This roots in the “reversal principle” of hypothesis testing. We first state that we assume that the estimated sample coefficient is equal to the true parameter and thus is zero. Now, we run a regression and compute the distribution of the sample coefficient, based on the mean and standard error of the coefficient, assuming a standard normal distribution. Then, based on this distribution, we calculate what percentage of the probability density of this distribution is included if the true effect was indeed zero (given our sample distribution). The probability density thus tells us how likely it is that the coefficient is indeed zero, given our sample distribution. If this value is smaller than a given threshold (e.g. if less than 5 % of the probability density is included when \\(\\hat{\\beta}\\) is indeed zero), then we can state with sufficient confidence that, given the underlying sample distribution, that the coefficient is different from zero. To visualise this, let’s look at the following distribution: a &lt;- as.data.frame(rnorm(1000000, mean = 0.5, sd = 1)) b &lt;- as.data.frame(rnorm(1000000, mean = 1, sd = 1)) c &lt;- as.data.frame(rnorm(1000000, mean = 1.96, sd = 1)) d &lt;- as.data.frame(rnorm(1000000, mean = 2, sd = 0.5)) df_norm_t &lt;- as.data.frame(cbind(a,b,c,d)) colnames(df_norm_t) = c(&quot;N(0,1)&quot;, &quot;N(1,1)&quot;, &quot;N(1.96,1)&quot;, &quot;N(2,0.5)&quot;) df_norm_t_melt &lt;- melt(df_norm_t) ## No id variables; using all as measure variables #Plot df_norm_t_melt %&gt;% ggplot(aes(x = value, fill = variable, color = variable)) + geom_density(alpha = 0.2) + geom_vline(xintercept = 0, color = &quot;black&quot;) + geom_vline(xintercept = 0.86, color = &quot;grey43&quot;, linetype = &quot;dashed&quot;) + geom_vline(xintercept = -1, color = &quot;grey43&quot;, linetype = &quot;dashed&quot;) + geom_vline(xintercept = -1.48, color = &quot;grey43&quot;, linetype = &quot;dashed&quot;) + geom_hline(yintercept = 0.05, color = &quot;black&quot;) + theme_bw() + xlim(-2,5) This visually represents what was described above. As we can see, we have four different Mean-SE combinations for \\(\\hat{\\beta}\\). \\(H_0\\) states that the true \\(\\beta\\) is equal to zero and that \\(E({\\beta}) = \\hat{\\beta} = \\beta\\). Now, when running the regression model, we try to understand how likely that this hypothesis is, given the resulting sample distribution we retrieved from the Mean-SE combinations when running the regression. Doing so, we look at the probability density of each parameter combination and look at the corresponding density at the intercept of zero. The corresponding p-value then indicates the exact probability density. If this density is below a given threshold (in our case 5 % - given the horizontal line), then we know that the probability that \\(\\hat{\\beta}\\) is indeed zero, given the underlying Mean-SE distribution, is less than this threshold (less than 5 % here). Consequently, we can say that, given the sample distribution, the likelihood that \\(\\hat{\\beta} = 0\\) is sufficiently low. In other words, we can reject \\(H_0\\) at the 5% level and state that \\(\\hat{\\beta}\\) is significantly different from zero. Note that we indicated the horizontal lines in a gray dashed color for each of the different estimates to show at what value we would have had to hypothesize the true \\(\\beta\\) in order to reject this hypothesis at the 5% level. As such, we can reject the Null Hypothesis twice, and we fail to reject it twice. This idea is equivalent to the distribution on the t-statistics we displayed earlier. To see this, note the \\(\\beta\\) distribution with the parameters of \\(\\hat{\\beta} = 1.96\\), \\(SE(\\hat{\\beta}) = 1\\). As we can see, its 5 % threshold of probability density is exactly at zero. Remember that the t-statistic for the threshold of 0.05 is 1.96. Further note the formula for the t-value, indicating that \\(t(\\hat{\\beta}) = (\\hat{\\beta} - 0)/ SE(\\hat{\\beta})\\) (in this case: 1.96/1 = 1.96). As such, the 0.025 and 0.975 percentile values of the standard normal distribution are given at [-1.96, 1.96], which is the critical value we know and, thus, logically must then incorporate 5% of the probability density in the t-statistics distribution. This enables us in both settings to visually depict the critical value of rejection of the Null Hypothesis. 8.2.4.4 Idiosyncratic vs. Systematic risk components Another way to measure the goodness of fit, or the evaluative properties of the model, is to look at the variation properties of the estimator. Note that we can also calulate the \\(R^2\\) in the following way: \\[ R^2 = \\frac{\\beta_i^2\\sigma_x^2}{\\beta_i^2\\sigma_x^2 + \\sigma_{i \\epsilon}^2} \\] In this formula, we can identify two terms: The variation associated with the systematic component \\(\\beta_i^2\\sigma_x^2\\) and the variation associated with the idiosyncratic component \\(\\sigma_{i \\epsilon}^2\\). The latter term is also known as residual variance. This term thus tells us how much of the variation is displayed by the underlying model and how much is explained by other factors currently not existing in the model, which can be translated again to a relational property of how good the model fits the data. In terms of a financial application, this can also be interpreted as idiosyncratic vs. systematic risk, or non-diversifiable vs. diversifiable risk. This is an important notion, as it allows us to “split” the variation properties of the dependent variable into an explanatory and a non-explanatory part. 8.2.4.5 F-Statistic To test whether the entire model is significant, we consider two alternative hypotheses: \\(H_0\\): \\(\\beta_0 = \\beta_1 = \\dots = \\beta_k = 0\\) \\(H_1\\): \\(\\beta_n \\neq 0\\) for at least one \\(j \\in {1,\\dots, k}\\) Under the F-test framework, two regressions are required, known as the unrestricted and the restricted regressions. The unrestricted regression is the one in which the coefficients are freely determined by the data, as has been constructed previously. The restricted regression is the one in which the coeffi- cients are restricted, i.e. the restrictions are imposed on some of the coefficients. To perform the test, we carry out an analysis of variance (ANOVA) test. The residual sums of squares from each regression are determined, and the two residual sums of squares are ‘compared’ in the test statistic. The F-test statistic for testing multiple hypotheses about the coefficient estimates is given by \\[ F = \\frac{(SSR_R - SSR_U)/k}{_U/n-k-1} \\] To see why the test centres around a comparison of the residual sums of squares from the restricted and unrestricted regressions, recall that OLS estimation involved choosing the model that minimised the residual sum of squares (SSR), with no constraints imposed. Now if, after imposing constraints on the model, a residual sum of squares results that is not much higher than the unconstrained model’s residual sum of squares, it would be concluded that the restrictions were supported by the data, thereby the F value would be low. On the other hand, if the residual sum of squares increased considerably after the restrictions were imposed, it would be concluded that the restrictions were not supported by the data and therefore that the hypothesis should be rejected as the F value is considerably higher. The degrees of freedom of the SSR equal the number of independent variables, d = k, while the degrees of freedom of the SSE are d = n – k – 1. 8.2.5 Assumption and diagnostic tests of OLS The simple OLS model follows five primary assumptions and one additional implication. These assumptions are important to grasp the advantages and threats of the model. Especially, comprehending the assumptions makes certain aspects of the model more clear. As such, we will shortly introduce each assumption and its underlying consequence on the model. 8.2.5.1 A1: Linearity Linearity means that the model follows a linear relationship. That is, the parameters of x and y are linearly connected, implying that we can “draw” a straight graph connecting dependent and explanatory variables. We require linearity because we have shown that linear random variables have distinct asymptotic properties and follow assumptions on normality, meaning every linear estimator is asymptotically normally distributed. As we have discussed, these properties are important to draw causal inference of the parameters. Especially, with linear regression models, we can make use of the average effect of a variable. This is because the slope of the regressor will always be the same because it is linear. As such, the functional form does not depend on the level of the explanatory variable. This makes it easy for us to draw a general relationship between variables. To test for linearity, a common approach is to plot the regression residuals on the vertical axis and values of the independent variable on the horizontal axis. This graphical analysis is performed for each independent variable. What we are looking for is a random scattering of the residuals around zero. If this should be the case, the model assumption with respect to the residuals is correct. If not, however, then there seems to be some systematic behavior in the residuals that depends on the values of the independent variables, rendering the linearity assumption incorrect. However, having a generally non-linear function is not a large issue. Remember that we can use linear transformations of the independent variables in order to create a linearity. For instance, we can transform an exponential relationship into a linear relationship and vice versa. Further, we can easily add a squared factor to account for decreasing utility function characteristics without violating any properties. 8.2.5.2 A2 and A5: Error Terms are mean-zero and normally distributed The first assumption states that the regression errors are normally distributed with zero mean: \\[ \\epsilon_i \\sim N(0, \\sigma_\\epsilon^2) \\] This assumption has two distinct implications. A2: Mean-Zero Assumption The mean-zero assumption implies that, over all observations, the average residual, which is the vertical distance between the observed and estimated variable, is zero. That is, there is, on average, no mistake of the model in the entire population. In general, when we are looking at a regression model, we always speak in terms of dependency on X. As such, a zero-mean error term assumption implies: \\[ E(\\epsilon_i|X_i) = 0 \\] \\[ cov(X_i, \\epsilon_i) = 0 \\] This means that the expected value of the error term, conditional on X, is zero. This implies that the variation of the error term is, on average, independent of the variation of the explanatory variables. This is perhaps the most fundamental assumption of any causal inference model, as it states that the explanatory variables are, on average, independent of the error terms. This property is essential in assuming that the model parameters are exogenous and, consequently, unbiased. Although we will not cover the notion here in detail, exogeneity states that the regressors are not influenced by any outside parameters which could potentially alter the true underlying relationship the model portrays. Consequently, we can assume that there is no additional, unobserved or incorrectly measured / accounted joint behavior of the variables which may either violate some assumptions or influence the underlying variation of the variables. A5: Normal Distribution Assumption Further, it states that the probability distribution for the error term is that it is normally distributed.. There are three implications of the violation of this assumption: The regression model is misspecified The estimates of the regression coefficients, (\\(\\hat{\\beta_i}\\)), are also not normally distributed The estimates of the regression coefficients, (\\(\\hat{\\beta_i}\\)), although still best linear unbiased estimators, become inefficient estimators Especially the second property renders hypothesis testing impossible, as we no longer can draw on a symmetric distribution. However, such a distribution, due to the symmetry characteristics and continuity, is required to perform important diagnostic tests which quantify any assumption on a causal relationship of the model. Especially, the t-tests explained will no longer be applicable as the probability density and consequent quantile distributions derivations are no longer valid. There are generally two ways to test for the normality assumption. The first is the Jarque-Bera Test Statistic. It is not quite simple to compute manually, but most computer software packages have it installed. Formally it is computed as: \\[ JB = \\frac{n}{6}(S^2+\\frac{(K-3)^2}{4}) \\] whereas S is the Skewness of a distribution and K the Kurtosis. As explained in earlier, Kurtosis estimates, relative to the normal distribution, the behavior in the extreme parts of the distribution (i.e., the tails of the distribution). For a normal distribution, K = 3. A value for K that is less than 3 indicates a so-called light-tailed distribution in that it assigns less weight to the tails. The opposite is a value for K that exceeds 3 and is referred to as a heavy-tailed distribution. The test statistics given by the equation is approximately distributed chi-square with two degrees of freedom. Another test is the analysis of the Standardized Residuals. Once computed, they can be graphically analyzed in histograms. Formally, each standardized residual at the i-th observation is computed according to: \\[ \\tilde{e_i} = n \\frac{e_i}{s_e\\sqrt{(n+1)+\\frac{(x_i - \\bar{x})^2}{s_x^2}}} \\] If the histogram appears skewed or simply not similar to a normal distribution, the linearity assumption is very likely to be incorrect. Additionally, one might compare these standardized residuals with the normal distribution by plotting them against their theoretical normal counterparts in a normal probability plot. If the pairs lie along the line running through the sample quartiles, the regression residuals seem to follow a normal distribution and, thus, the assumptions of the regression model are met. 8.2.5.3 A3: Heteroskedasticity Heteroskedasticity implies that the variance of the error terms is constant: \\[ var(\\epsilon_i|X_i) = \\sigma^2 \\] This means that, the vertical distances between the estimated and the observed dependent variables is constant throughout all distinct values of the explanatory variable(s). If errors are heteroscedastic (i.e. OLS assumption is violated), then it will be difficult to trust the standard errors of the OLS estimates. Hence, the confidence intervals will be either too narrow or too wide. Also, violation of this assumption has a tendency to give too much weight on some portion (subsection) of the data. To comprehend the idea of heteroskedasticity more thoroughly, we can create an artificial simulation of data and plot it. # load scales package for adjusting color opacities library(scales) ## ## Attaching package: &#39;scales&#39; ## The following objects are masked from &#39;package:psych&#39;: ## ## alpha, rescale ## The following object is masked from &#39;package:readr&#39;: ## ## col_factor ## The following object is masked from &#39;package:purrr&#39;: ## ## discard # generate some heteroskedastic data: # set seed for reproducibility set.seed(123) # set up vector of x coordinates x &lt;- rep(c(10, 15, 20, 25), each = 25) # initialize vector of errors e &lt;- c() # sample 100 errors such that the variance increases with x e[1:25] &lt;- rnorm(25, sd = 10) e[26:50] &lt;- rnorm(25, sd = 15) e[51:75] &lt;- rnorm(25, sd = 20) e[76:100] &lt;- rnorm(25, sd = 25) # set up y y &lt;- 720 - 3.3 * x + e # Create a dataframe model &lt;- data.frame(cbind(y,x)) colnames(model) &lt;- c(&quot;EBITDA&quot;, &quot;Industry&quot;) # Plot the model model %&gt;% ggplot(aes(x = Industry, y = EBITDA)) + geom_boxplot(aes(x = Industry, y = EBITDA, group = Industry),alpha = 0.6, width = 0.8, outlier.colour = &quot;red&quot;, outlier.shape = 1) + geom_jitter(width = 0.2, alpha = 0.6, color = &quot;deepskyblue4&quot;) + geom_smooth(aes(x = Industry, y = EBITDA), method = &quot;lm&quot;, se = F, col = &quot;deeppink4&quot;, linetype = &quot;dashed&quot;) + ylab(&quot;Earnings (EBITDA)&quot;) + xlab(&quot;Industry Classifier&quot;) + ggtitle(&quot;An Example for Heteroskedasticity&quot;) + labs(color=&#39;Cumulative Return&#39;) + theme(plot.title= element_text(size=14, color=&quot;grey26&quot;, hjust=0.46,lineheight=2.4, margin=margin(15,0,15,45)), panel.background = element_rect(fill=&quot;#f7f7f7&quot;), panel.grid.major.y = element_line(size = 0.5, linetype = &quot;solid&quot;, color = &quot;grey&quot;), panel.grid.minor = element_blank(), panel.grid.major.x = element_blank(), plot.background = element_rect(fill=&quot;#f7f7f7&quot;, color = &quot;#f7f7f7&quot;), axis.title.y = element_text(color=&quot;grey26&quot;, size=12, margin=margin(0,10,0,10)), axis.title.x = element_text(color=&quot;grey26&quot;, size=12, margin=margin(10,0,10,0)), axis.line = element_line(color = &quot;grey&quot;)) ## `geom_smooth()` using formula &#39;y ~ x&#39; As we can see clearly, depending on the x variable, the variation of the y variable increases. Here, we do not particularly care about the level of the boxplots, but rather their distribution. As we can see, for the first industry, the earnings are quite centralized. However, they start to spread for different industries. This issue violates the covariance properties of the estimator, because we can no longer assume that the variance of the residuals (the average, squared distance between the predicted and observed values) is constant and thus does not depend on the explanatory variable. The implication is that statistics computed do not follow a standard normal distribution, even in large samples. This issue may invalidate inference when using the previously treated tools for hypothesis testing: we should be cautious when making statements about the significance of regression coefficients on the basis of statistics in the case of heteroskedasticity. 8.2.5.4 A4: Serial Correlation Serial Correlation says that there is no correlation between the residual terms. Simply put, this means that there should not be any statistically significant correlation between adjacent residuals, either throughout different states (cross-sectional) or different times (time-series). This implies: \\(cov(\\epsilon_i\\epsilon_j|X_i) = 0\\). In time series analysis, this means no significant correlation between two consecutive time periods. In cross-sectional analysis, this implies no significant correlation between two consecutive states. Here, “states” is an umbrella term for distinct categories, such as industries, geographic locations (e.g. countries, actual US States), markets etc. Further, “time” indicates different time periods. We usually face serial correlation of both types when working with time-series economic and financial data. First, we may assume that serial correlation within a distinct state over a time period exists, implying that the error terms for an identical state are related with each other through time. Second, we may assume that serial correlation between distinct states during the same time period exists, implying that the error terms for multiple states are related with each other at the same time. Both types of serial correlation are likely to arise in economic and financial settings. For instance, macroeconomic indicators of this year (such as unemployment) are likely to depend to some part on the rate of last year. Further, financial target quantities (such as ROE) may depend on the target quantity of your competitors in the same industry at a given point in time. Even, although less likely, one may assume that share price movements may depend on (I) their historic prices and (II) the price movements of your competitors at a given time (e.g. the strong correlation of FAANG stocks). Consequently, when working with time-series financial data, we may not want to simply disregard serial correlation, both cross-sectionally and time-series wise. Lags, First Differences, Logarithms and Growth Rates Typically, when working with time-series data, we work with one of the following periodic observational manpipulations: Lags: Previous values of a time series (j’th lag of \\(Y_t = Y_{t-j}\\)) First Differences: the difference between periods t and t-j (\\(\\triangle Y_t = Y_t - Y_{t-j}\\)) Logarithm: Remember that, when using logarithms, the return calculation of two periods follows an additive property. That is, we can take the first difference as follows: (\\(\\triangle \\log(Y_t) = \\log(Y_t) - \\log(Y_{t-j})\\)) Growth: We can use an approximation for the percentage change between \\(Y_t\\) and \\(Y_{t-j}\\) with: \\(100\\cdot\\triangle \\log(Y_t)\\) In the case of these variables, we usually should assume that bservations of a time series are typically correlated. This type of correlation is called autocorrelation or serial correlation. The j’th autocorrelation coefficient, also called the serial correlation coefficient, measures the correlation between \\(Y_t\\) and \\(Y_{t-j}\\): \\[ \\rho_j = \\rho_{Y_t, Y_{t-j}} = \\frac{cov(Y_t, Y_{t-j})}{\\sqrt{var(Y_t), var(Y_{t-j})}} \\] 8.2.5.5 Heteroskedasticity and Serial Correlation in terms of the variance-covariance matrix We introduced the concept of heteroskedasticity and serial correlation simultaneously. This is because both assumptions deal with the composition of the covariance matrix and, consequently, the error terms of the explanatory variables (this is not an issue in simple OLS but it will become in multiple OLS). First, let’s define both terms. In order to understand the concept of both assumptions mathematically, let’s dig a little bit into the variance-covariance properties of the error terms. As we understand, when we have multiple regressors, we have a vector of expected error terms: \\[ E(\\epsilon) = E \\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_N \\end{bmatrix} = \\begin{bmatrix} E(\\epsilon_1) \\\\ E(\\epsilon_2) \\\\ \\vdots \\\\ E(\\epsilon_N) \\end{bmatrix} \\] Remember the formula for the variance-covariance matrix from the linear algebra sesssion. This is: \\[ V(X) = E(XX&#39;) - E(X)E(X&#39;) \\] Transferring this to the error terms, we obtain: \\[\\begin{align*} V(\\epsilon) &amp;= E \\left(\\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_N \\end{bmatrix} \\begin{bmatrix} \\epsilon_1 &amp; \\epsilon_2 &amp; \\dots &amp; \\epsilon_N \\end{bmatrix}\\right) - \\begin{bmatrix} E(\\epsilon_1) \\\\ E(\\epsilon_2) \\\\ \\vdots \\\\ E(\\epsilon_N) \\end{bmatrix} \\begin{bmatrix} E(\\epsilon_1) &amp; E(\\epsilon_2) &amp; \\dots &amp; E(\\epsilon_N) \\end{bmatrix} \\\\ &amp;= \\begin{pmatrix} \\color{Green}{E(\\epsilon_1^2)} - \\color{blue}{E(\\epsilon_1)^2} &amp; \\color{Red}{E(\\epsilon_1\\epsilon_2)} - \\color{blue}{E(\\epsilon_1)E(\\epsilon_2)} &amp; \\dots &amp; \\color{Red}{E(\\epsilon_1\\epsilon_k)} - \\color{blue}{E(\\epsilon_1)E(\\epsilon_k)} \\\\ \\color{Red}{E(\\epsilon_2\\epsilon_1)} - \\color{blue}{E(\\epsilon_2)E(\\epsilon_1)} &amp; \\color{Green}{E(\\epsilon_2^2)} - \\color{blue}{E(\\epsilon_2)^2} &amp; \\dots &amp; \\color{Red}{E(\\epsilon_2\\epsilon_k)} - \\color{blue}{E(\\epsilon_2)E(\\epsilon_k)} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\color{Red}{E(\\epsilon_N\\epsilon_1)} - \\color{blue}{E(\\epsilon_N)E(\\epsilon_1)} &amp; \\color{Red}{E(\\epsilon_N\\epsilon_2)} - \\color{blue}{E(\\epsilon_N)E(\\epsilon_2)} &amp; \\dots &amp; \\color{Green}{E(\\epsilon_N\\epsilon_k)} - \\color{blue}{E(\\epsilon_N)E(\\epsilon_k)} \\end{pmatrix} \\end{align*}\\] You see that we intentionally used three distinct colors to display the variance-covariance matrix of the error terms. This is to elaborate the assumptions underlying the error terms: First, the Blue channel - Assumption: \\(E(\\epsilon_i) = 0\\). This states that the expected value of each error term (conditional on x), is zero. This implies that \\(E(\\epsilon_1) = E(\\epsilon_2) = \\dots = E(\\epsilon_N) = 0\\). Consequently, the blue terms all equal zero. Secondly, the Red channel - Assumption: \\(Cov(\\epsilon_i, \\epsilon_j) = 0\\) or \\(E(\\epsilon_i \\epsilon_j) = 0\\). This states that we have no serial correlation between the error terms, neither cross-sectionally nor in time-series configurations (for instance, imagine the var-cov matrix would constitute of k industry portfolios and N time periods - implying a covariation between industries and time). As such, the red terms are all equal to zero. Lastly, the Green channel - Assumption: \\(E(\\epsilon_i^2) = \\sigma^2\\). This states that the variance of the error term is constant, thereby each error term has the same variation (and thus the same variance). As such, the green terms are all equal to \\(\\sigma^2\\). The three assumptions thereby deliver us the following variance covariance matrix of the error terms: \\[ V(\\epsilon) = \\begin{pmatrix} \\sigma^2 &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; \\sigma^2 &amp; \\dots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; \\sigma^2 \\end{pmatrix} \\] This is the usual variance-covariance matrix of the error terms. To obtain the Standard Errors, we would simply take the diagonal elements and divide it by the square root of n. We can do so because there is no “covariance” term in the matrix, as all of them are assumed to be zero. However, note that, the usual formula for the variance of an estimator depends on both the variance and covariance properties within a given setting. As such, in presence of heteroskedasticity and serial correlation, the estimator will no longer have the minimum varinace property. If the errors are heteroscedastic, the formulas presented for the coefficient standard errors no longer hold. Tests for Heteroskedasticity and Serial Correlation There are some ways to test for both heteroskedasticity and serial correlation. First, let’s go into Heteroskedasticity. We want to test if \\(var(\\epsilon_i) = \\sigma^2\\). The most popular test is White’s (1980) general test for heteroscedasticity. The test is particularly useful because it makes few assumptions about the likely form of the heteroscedasticity. In essence, we want to find out whether the variance of the residuals varies systematically with any known variables relevant to the model the residuals are part of. To understand why, recall that the variance of the error term is given as: \\[ E[(\\epsilon_i - E(\\epsilon_i))^2] \\] Under the assumption that \\(E(\\epsilon_i) = 0\\), we retrieve that the variance reduces to \\(E[\\epsilon_i^2]\\). As the squares of the population residual terms unobservable, we need to estimate the squared sample residuals. To understand how we retrieve these terms, let’s assume we have the following baseline model: \\[ y_i = \\alpha_i + \\beta_1x_{1i} + \\beta_2x_{2i} + \\epsilon_i \\] Based on this model, White (1980) estimates the baseline model and retrieves the sample estimates of the residual term, \\(\\hat{\\epsilon}_i\\). Then, to test if \\(var(\\epsilon_i) = \\sigma^2\\), the following auxiliary model is estimated: \\[ \\hat{\\epsilon}_i^2 = \\gamma_i + \\delta_1x_{1i} + \\delta_2x_{2i} + \\delta_3x_{1i}^2 + \\delta_4x_{2i}^2 + \\delta_5x_{1i}x_{2i} + v_i \\] whereas \\(v_i\\) is an normally distributed error term which is uncorrelated with \\(\\hat{\\epsilon}_i\\). We understand that the auxiliary regression takes the form of the estimated squared residual terms on the constant, the linear explanatory variables, their squared form as well as their interaction term. The reason for this form is that we want to understand whether the variance of the residuals (given by \\(\\hat{\\epsilon}_i^2\\)) varies systematically with any known variables releant to the model. These relevant variables are given by (I) the original explanatory variables (II) the squared values and (III) the cross-products. Note that we can easily expand the model when considering more variables by using the Pascal triangle for binomial coefficients expansion, where, instead of only the quadratic term, the term depends on higher-order powers. Now, based on this auxiliary model, we can perform an F-Test by running the unrestricted auxiliary model presented above and on a restricted regression of the squared error term on a constant only and comparing their RSS. Second, let’s go into Serial Correlation. We want to test if \\(cov(\\epsilon_i\\epsilon_j|X_i) = 0\\). Note that we focus much more on correlation between time than between states. Consequently, we also employ statistical tests to check for serial correlation (= autocorrelation) for time-series data. Autocorrelation, which is also referred to as serial correlation and lagged correlation in time series analysis, like any correlation, can range from −1 to +1. Its computation is straightforward since it is simply a correlation using the residual pairs \\(\\epsilon_{i,t}\\) and \\(\\epsilon_{i,t-1}\\). The formula for autocorrelation is given by: \\[ \\rho_{auto} = \\frac{\\sum_{t=2}^n\\epsilon_{i,t}\\epsilon_{i,t-1}}{\\sum_{t=2}^n\\epsilon_{i,t}^2} \\] Where \\(\\rho_{auto}\\) means the estimated autocorrelation and \\(\\epsilon_{i,t}\\) is the computed residual or error term for the t-th observation of the i’th state. A positive autocorrelation means that if a residual t is positive (negative), then the residual that follows, t + 1, tends to be positive (negative). Positive autocorrelation is said to exhibit persistence A negative autocorrelation means that a positive (negative) residual t tends to be followed by a negative (positive) residual t + 1 From an estimation perspective, the existence of autocorrelation complicates hypothesis testing of the regression coefficients. This is because although the regression coefficient estimates are unbiased, they are not best linear unbiased estimates. Hence, the variances may be significantly underestimated and the resulting hypothesis test questionable. Now, we can detect autocorrelation in time-series data using the Durbin-Watson (DW) Test. It is computed as: \\[ d = \\frac{\\sum_{t=2}^n (\\epsilon_{i,t} - \\epsilon_{i,t-1})^2}{\\sum_{t=2}^n\\epsilon_{i,t}^2} \\] The denominator of the test is simply the sum of the squares of the error terms; the numerator is the squared difference of the successive residuals. It can be shown that if the sample size is large, then the Durbin-Watson d test statistic given by formula above is approximately related to the auto-correlation given by formula beforehand as: \\[ d \\sim 2(1-\\rho_{auto}) \\] Because \\(\\rho_{auto}\\) varies between [-1,1], this means that d can vary from 0 to 4. This is given by the following graphic: From the above table we see that if d is close to 2 there is no autocorrelation. We now test for autocorrelation by using a critical value approach again, in which we estimate both an upper and lower value, named \\(d_U\\) and \\(d_L\\). We simply take the critical values from the DW table. The general decision rule given the null hypothesis and the computed value for d is summarized in the fol- lowing table: Figure 6: Durbin Watson Test Statistics 8.2.5.6 Multicollinearity 8.2.6 Properties of the OLS Estimator If the above-mentioned assumptions all hold, then the estimators determined by OLS will have a number of desirable properties, and are known as best linear unbiased estimators (BLUE). This stands for: Best – means that the OLS estimator has minimum variance among the class of linear unbiased estimators; the Gauss–Markov theorem proves that the OLS estimator is best by examining an arbitrary alternative linear unbiased estimator and showing in all cases that it must have a variance no smaller than the OLS estimator. Linear - the estimator is linear – that means that the formulas for the estimators are linear combinations of the random variables (in this case y) and thus they have a normality property Unbiased - On average, the actual values of the estimators will be equal to their true values Estimator Under assumptions 1–4 listed above, the OLS estimator can be shown to have the desirable properties that it is consistent, unbiased and efficient. To repeat the notions of the inductive statistics part, this implies that they (I) have asymptotic properties, (II) are exogenous and (III) have a smaller variance (converge quicker). Consequently, the follow the properties we can assume the statistical properties of the estimator, draw causal inference on the estimator through hypothesis testing and confidence intervals, and draw a sample-population relationship. 8.2.7 Expected Value and Variance of the multivarite OLS estimator if the assumptions hold: In order to comprehend the statistical properties of this estimator if the assumptions are valid, we need to understand what its expected value and variance are. To do so, it is first important to understand the relationship between \\(\\hat{\\beta}, \\beta\\) and \\(\\epsilon\\). \\[ \\begin{align*} \\hat{\\beta} &amp;= (X&#39;X)^{-1}X&#39;y \\\\ &amp;= (X&#39;X)^{-1}X&#39;(\\beta X + \\epsilon) &amp;&amp; \\text{we know that } y = X\\beta+\\epsilon\\\\ \\\\ \\hat{\\beta} &amp;= (X&#39;X)^{-1}X&#39;\\epsilon + \\beta\\\\ \\hat{\\beta} - \\beta &amp;= (X&#39;X)^{-1}X&#39;\\epsilon \\end{align*} \\] 8.2.7.1 Expected Value Now, we can take the expected value and understand the first moment of the distribution of the variable: \\[ \\begin{align*} E(\\hat{\\beta}|X) &amp;= E((X&#39;X)^{-1}X&#39;\\epsilon + \\beta|X) \\\\ &amp;= (X&#39;X)^{-1}X&#39;E(\\epsilon|X) + \\beta &amp;&amp; \\text{Ass.2 : } E(\\epsilon|X) = 0 \\\\ &amp;= \\beta \\end{align*} \\] As such, we have an unbiased estimate under Assumption 1 and 2. 8.2.7.2 Variance Let’s explore the second moment of the distribution: \\[ \\begin{align*} V(\\hat{\\beta}) &amp;= E((\\hat{\\beta} - E(\\hat{\\beta}))(\\hat{\\beta} - E(\\hat{\\beta}))&#39;) \\\\ &amp;= E((\\hat{\\beta} - \\beta)(\\hat{\\beta} - \\beta)) \\\\ &amp;= E(((X&#39;X)^{-1}X&#39;\\epsilon)((X&#39;X)^{-1}X&#39;\\epsilon)&#39;) \\\\ &amp;= \\underbrace{(X&#39;X)^{-1}X&#39;X}_{\\text{cancels out}}E(\\epsilon\\epsilon&#39;)(X&#39;X)^{-1} &amp;&amp; E(\\epsilon\\epsilon&#39;) = \\text{ variance of the error term}\\\\ &amp;= \\sigma^2I_N (X&#39;X)^{-1} &amp;&amp; \\text{Ass.3 &amp; Ass.4: } E(\\epsilon\\epsilon&#39;) = \\sigma^2I_N \\end{align*} \\] That implies that the variance of the estimator depends on the variance of the data generating process and the sample variance covariance matrix (= the “variability” of the regressors). As such, we understand that our estimator is normally distributed with mean \\(\\beta\\) and variance \\(\\sigma^2I_N (X&#39;X)^{-1}\\). That is (the \\(I_N\\) is deliberately left out): \\[ \\hat{\\beta} \\sim N(\\beta, \\sigma^2(X&#39;X)^{-1}) \\] Especially the variance property will be important to us when considering potential violations of the assumptions we made. This will be found when considering Heteroskedasticity and Auto Correlation Robust Standard Errors, which require an adjustment factor for the variance structure of the estimator, as properties A3 and A4 no longer hold in such cases and thus the formula for the variance of \\(\\hat{\\beta}\\) is different from the one we just obtained. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
